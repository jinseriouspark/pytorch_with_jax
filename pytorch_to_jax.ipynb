{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinseriouspark/pytorch_with_jax/blob/main/pytorch_to_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform pytorch to jax\n",
        "\n",
        "- 활용자료 : https://github.com/hunkim/PyTorchZeroToAll"
      ],
      "metadata": {
        "id": "ynp9olWoHBz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CosN6ukkgYbf",
        "outputId": "adc41fbb-b108-43da-ffd4-fb32fe5bd7ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform pytorch to jax"
      ],
      "metadata": {
        "id": "WPcCYoxuG22g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/sample_data/california_housing_train.csv', nrows = 100)\n",
        "feature_col = 'median_income'\n",
        "target_col = 'median_house_value'"
      ],
      "metadata": {
        "id": "FLUkG_f6h6Zs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01_basic.py"
      ],
      "metadata": {
        "id": "tzo4-YL3qfAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_basic.py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#x_data = data[feature_col].values\n",
        "#y_data = data[target_col].values\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) **2\n",
        "\n",
        "# list of weights/mean square Error (MSE) for each input\n",
        "w_list = []\n",
        "mse_list = []\n",
        "\n",
        "for w in np.arange(0.0, 4.0, 1.0):\n",
        "  l_sum = 0\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    y_pred_val = forward(x_val)\n",
        "    l = loss(x_val, y_val)\n",
        "    l_sum += l\n",
        "\n",
        "    print('\\t', x_val, y_val, y_pred_val, l)\n",
        "  print('MSE=', l_sum/ len(x_data)) # 직접 평균 계산\n",
        "  w_list.append(w)\n",
        "  mse_list.append(l_sum / len(x_data))\n",
        "# Plot it all\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wokSxfHZHOlU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "outputId": "f47169e3-8948-4101-861b-a6aef3022766"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABMvUlEQVR4nO3dd3wUBf7G8c+mU1JoaRASahJAiihNERCkRYreWTgLeurdeepPj8MTxAOs2CsclhOxne1OQelFEkBApEQpSUhCSAESCJBO2u78/ojmLkowIWW2PO/Xa/7I7szus+O6+7D73RmLYRgGIiIiIi7EzewAIiIiIs1NBUhERERcjgqQiIiIuBwVIBEREXE5KkAiIiLiclSARERExOWoAImIiIjL8TA7gD2y2WwcO3YMX19fLBaL2XFERESkDgzDoLCwkNDQUNzczv8ZjwrQORw7doywsDCzY4iIiMgFyMzMpFOnTuddRwXoHHx9fYGqHejn52dyGhEREamLgoICwsLCqt/Hz0cF6Bx++trLz89PBUhERMTB1GV8RUPQIiIi4nJUgERERMTlqACJiIiIy1EBEhEREZejAiQiIiIuRwVIREREXI4KkIiIiLgcFSARERFxOSpAIiIi4nJUgERERMTlqACJiIiIy1EBEhEREZejAtTMtqeeoris0uwYIiIiLk0FqBktWJXAtLd28OrXyWZHERERcWkqQM3o0oi2ACzZmkbKiSKT04iIiLguFaBmNKZXEFdGBVJhNXj0qwMYhmF2JBEREZekAtTM5k3qhZe7G1uSc1mzP9vsOCIiIi5JBaiZhbdrxR9HdAXg8RUHOVtuNTmRiIiI61EBMsGfR3anY0ALjuWXsmhTitlxREREXI4KkAlaeLnz96t7AfDm5sOk5RabnEhERMS1qACZZFzvIIb3aE+51aaBaBERkWamAmQSi8XCo5N74+luITbpJBsSTpgdSURExGWoAJmoa4fW3Dm8aiD60a8OUFqhgWgREZHmoAJksntHdSfE34esM2dZHJtqdhwRERGXoAJkslbeHsyJiQZgcVwqGadKTE4kIiLi/FSA7EDMRSEM69aO8kobj604aHYcERERp6cCZAd+Goj2cLOwISGHTYkaiBYREWlKKkB2okeQL7dfFgHAfA1Ei4iINCkVIDty/5ieBPp6k36qhH9uOWx2HBEREaelAmRHWv/PQPTCTSlkndFAtIiISFNQAbIzk/uFMqhLW0orbDy5MsHsOCIiIk7J1AK0efNmJk2aRGhoKBaLhWXLltW43mKxnHN57rnnar3N+fPn/2L9qKioJn4kjcdisfDYlN64u1lYvT+bLcknzY4kIiLidEwtQMXFxfTr149Fixad8/rjx4/XWJYsWYLFYuE3v/nNeW+3d+/eNbbbunVrU8RvMlHBftw6NByAeV8eoLzSZnIiERER5+Jh5p1PmDCBCRMm1Hp9cHBwjb+XL1/OqFGj6Nq163lv18PD4xfbOpoHxvTkq++PcfhkMW9vTePukd3MjiQiIuI0HGYGKCcnh5UrV3LHHXf86rrJycmEhobStWtXbrrpJjIyMs67fllZGQUFBTUWs/m38GTWhKqB6Ne+TuZ4/lmTE4mIiDgPhylA7777Lr6+vlx77bXnXW/w4MEsXbqUNWvWsHjxYtLS0hg+fDiFhYW1brNgwQL8/f2rl7CwsMaOf0GuHdCRgeFtKCm3aiBaRESkETlMAVqyZAk33XQTPj4+511vwoQJXHfddfTt25dx48axatUq8vLy+PTTT2vdZvbs2eTn51cvmZmZjR3/gri5VR0h2s0CK344zrbUXLMjiYiIOAWHKEBbtmwhKSmJO++8s97bBgQE0LNnT1JSUmpdx9vbGz8/vxqLvejT0Z+bBv84EL38ABVWDUSLiIg0lEMUoLfffpuBAwfSr1+/em9bVFREamoqISEhTZCsecwcG0nbVl4knyji3W1HzI4jIiLi8EwtQEVFRcTHxxMfHw9AWloa8fHxNYaWCwoK+Oyzz2r99Gf06NEsXLiw+u+ZM2cSFxfHkSNH2LZtG9dccw3u7u5MmzatSR9LU/Jv6clD4yMBeHlDMicKSk1OJCIi4thMLUC7du1iwIABDBgwAIAZM2YwYMAA5s6dW73Oxx9/jGEYtRaY1NRUcnP/OxuTlZXFtGnTiIyM5Prrr6ddu3bs2LGDDh06NO2DaWLXDQyjX1gARWWVLFidaHYcERERh2YxDMMwO4S9KSgowN/fn/z8fLuaB/ohK48pi77BMODTPw5lUJe2ZkcSERGxG/V5/3aIGSCp0rdTADde2hmAucv3U6mBaBERkQuiAuRgHhwXSUBLTxKzC/lgR7rZcURERBySCpCDadvKi5ljqwaiX1h/iJOFZSYnEhERcTwqQA5o2qDO9OnoR2FpJc+s0UC0iIhIfakAOSB3NwuPTu4DwL93Z7E7/YzJiURERByLCpCDGhjehusGdgJg3pf7sdr0Yz4REZG6UgFyYA9NiMLXx4P9Rwv4187zn/FeRERE/ksFyIG1b+3NX6/qCcDza5M4XVxuciIRERHHoALk4G4eEk5UsC/5Zyt4bq0GokVEROpCBcjBebi78fjUqoHoj7/L5PvMPHMDiYiIOAAVICdwaURbrhnQEcOoOkK0TQPRIiIi56UC5CRmT4iitbcH32fl8+muTLPjiIiI2DUVICcR6OfDA2N6APDMmkTySjQQLSIiUhsVICcyfVgEPYNac6akgufXJZkdR0RExG6pADkRT3e36iNEf/htBvuP5pucSERExD6pADmZod3aMalfqAaiRUREzkMFyAnNmRhNSy939mTk8Z89WWbHERERsTsqQE4o2N+H/xv934Ho/LMVJicSERGxLypATur3l3WhW4dW5BaV89L6Q2bHERERsSsqQE7Ky8ON+ZN7A/De9iMkHC8wOZGIiIj9UAFyYsN7dGBCn2BsPw5EG4YGokVEREAFyOk9cnUvWni6892RMyyPP2Z2HBEREbugAuTkOga04N4ruwPw5KoECks1EC0iIqIC5ALuHN6FiHYtOVlYxisbks2OIyIiYjoVIBfg7eHOvB8Hot/ZdoRDOYUmJxIRETGXCpCLGBUZyFW9grDaDOYtP6CBaBERcWkqQC5k7tW98PZwY/vhU6z44bjZcUREREyjAuRCwtq25O6R3QB4cmUCxWWVJicSERExhwqQi/nTiG6EtW1BdkEpr32dYnYcERERU6gAuRgfT3fmXV01EP321sOkniwyOZGIiEjzUwFyQaOjAxkV2YEKq8H8LzUQLSIirkcFyAVZLBbmTeqNl7sbW5JzWXsg2+xIIiIizUoFyEVFtG/FH0d0BeDxFQmcLbeanEhERKT5qAC5sD+P7E7HgBYczTvLP2I1EC0iIq7D1AK0efNmJk2aRGhoKBaLhWXLltW4/rbbbsNisdRYxo8f/6u3u2jRIiIiIvDx8WHw4MHs3LmziR6BY2vh5c7fr44G4I24wxzJLTY5kYiISPMwtQAVFxfTr18/Fi1aVOs648eP5/jx49XLRx99dN7b/OSTT5gxYwbz5s1jz5499OvXj3HjxnHixInGju8UxvUOZniP9pRbbTz6lQaiRUTENZhagCZMmMATTzzBNddcU+s63t7eBAcHVy9t2rQ5722++OKL3HXXXdx+++306tWL119/nZYtW7JkyZLGju8ULBYL8yf3xtPdwqakk2xIUFEUERHnZ/czQLGxsQQGBhIZGcndd9/NqVOnal23vLyc3bt3M2bMmOrL3NzcGDNmDNu3b691u7KyMgoKCmosrqRbh9bccXnVQPRjKw5QWqGBaBERcW52XYDGjx/Pe++9x8aNG3nmmWeIi4tjwoQJWK3nfoPOzc3FarUSFBRU4/KgoCCys2v/qfeCBQvw9/evXsLCwhr1cTiC+67sTrCfD5mnz/J6XKrZcURERJqUXRegG2+8kcmTJ3PRRRcxdepUVqxYwXfffUdsbGyj3s/s2bPJz8+vXjIzMxv19h1BK28P5sRUDUQvjk0l83SJyYlERESajl0XoJ/r2rUr7du3JyXl3D/Zbt++Pe7u7uTk5NS4PCcnh+Dg4Fpv19vbGz8/vxqLK7q6bwjDurWjrNLGYysOmh1HRESkyThUAcrKyuLUqVOEhISc83ovLy8GDhzIxo0bqy+z2Wxs3LiRoUOHNldMh2WxWHh0cm883CysP5jDpiQNRIuIiHMytQAVFRURHx9PfHw8AGlpacTHx5ORkUFRUREPPvggO3bs4MiRI2zcuJEpU6bQvXt3xo0bV30bo0ePZuHChdV/z5gxg7feeot3332XhIQE7r77boqLi7n99tub++E5pB5Bvtw2LAKAR788QFmlBqJFRMT5eJh557t27WLUqFHVf8+YMQOA6dOns3jxYn744Qfeffdd8vLyCA0NZezYsTz++ON4e3tXb5Oamkpubm713zfccAMnT55k7ty5ZGdn079/f9asWfOLwWip3f1jerD8+2McOVXCP7ekcc+o7mZHEhERaVQWQ0e++4WCggL8/f3Jz8932XmgZXuP8sAn8fh4urHxryPpGNDC7EgiIiLnVZ/3b4eaAZLmM6V/KIMi2lJaYeMJDUSLiIiTUQGSc7JYLDw6pTfubhZW789mS/JJsyOJiIg0GhUgqVV0iB+3DAkHYN6XByivtJmcSEREpHGoAMl5/eWqnrRv7cXhk8Us+SbN7DgiIiKNQgVIzsu/hScPjY8C4NWNyRzPP2tyIhERkYZTAZJf9ZuLO3Fx5wBKyq08tSrR7DgiIiINpgIkv8rNzcJjU/pgscBX3x9jW2rur28kIiJix1SApE76dPTnpsGdAZi3/AAVVg1Ei4iI41IBkjqbOTaSNi09ST5RxLvbjpgdR0RE5IKpAEmdBbT0qh6IfnlDMicKSk1OJCIicmFUgKRerr8kjH6d/Ckqq2TBag1Ei4iIY1IBknr534HoL/YeZWfaabMjiYiI1JsKkNRbv7AAbrw0DIC5y/dTqYFoERFxMCpAckEeHBeFfwtPErML+WBHutlxRERE6kUFSC5I21ZezBwXCcAL6w+RW1RmciIREZG6UwGSC/a7QZ3p09GPwtJKntFAtIiIOBAVILlg7m4WHp3cB4DPdmexO/2MyYlERETqRgVIGmRgeBt+O7ATAPO+3I/VZpicSERE5NepAEmDzZoQha+PB/uPFvDRzgyz44iIiPwqFSBpsPatvfnrVT0BeG5tEqeLy01OJCIicn4qQNIobh4STlSwL/lnK3hurQaiRUTEvqkASaPwcHfjsSlVA9Eff5fJ95l55gYSERE5DxUgaTSDurTlmgEdMQyY++UBbBqIFhERO6UCJI1q9oQoWnt78H1mHp/uyjQ7joiIyDmpAEmjCvTz4YExPQB4Zk0ieSUaiBYREfujAiSNbvqwCHoEtuZMSQUvrDtkdhwREZFfUAGSRufp7sajU3oD8OG36ew/mm9yIhERkZpUgKRJDOvWnqv7hmAzYO7y/RqIFhERu6ICJE1mTkw0Lb3c2ZORx+d7j5odR0REpJoKkDSZEP8W3Hdl1UD006sTyD9bYXIiERGRKipA0qTuuLwLXTu0IreonJc3aCBaRETsgwqQNCkvDzcenVw1EP3e9nQSswtMTiQiIqICJM1geI8OTOgTjNVmMHfZAQxDA9EiImIuFSBpFo9c3QsfTzd2HjnN8vhjZscREREXZ2oB2rx5M5MmTSI0NBSLxcKyZcuqr6uoqOChhx7ioosuolWrVoSGhnLrrbdy7Nj53zznz5+PxWKpsURFRTXxI5Ff0zGgBfeO6g7Ak6sSKCzVQLSIiJjH1AJUXFxMv379WLRo0S+uKykpYc+ePfz9739nz549fP755yQlJTF58uRfvd3evXtz/Pjx6mXr1q1NEV/q6a4ruhLRriUnC8t4dWOy2XFERMSFeZh55xMmTGDChAnnvM7f35/169fXuGzhwoUMGjSIjIwMOnfuXOvtenh4EBwc3KhZpeG8PdyZN6k3ty/9jne+OcL1l4TRI8jX7FgiIuKCHGoGKD8/H4vFQkBAwHnXS05OJjQ0lK5du3LTTTeRkZFx3vXLysooKCiosUjTGBUVyJjoICptBvO+1EC0iIiYw2EKUGlpKQ899BDTpk3Dz8+v1vUGDx7M0qVLWbNmDYsXLyYtLY3hw4dTWFhY6zYLFizA39+/egkLC2uKhyA/mjepF14ebmxLPcXKfcfNjiMiIi7IYtjJP8EtFgtffPEFU6dO/cV1FRUV/OY3vyErK4vY2NjzFqCfy8vLIzw8nBdffJE77rjjnOuUlZVRVlZW/XdBQQFhYWHk5+fX676k7l5af4hXNiYT7OfDxr+OoJW3qd/GioiIEygoKMDf379O7992/wlQRUUF119/Penp6axfv77ehSQgIICePXuSkpJS6zre3t74+fnVWKRp3T2yG2FtW5BdUMrCTbX/txEREWkKdl2Afio/ycnJbNiwgXbt2tX7NoqKikhNTSUkJKQJEsqF8vF0Z+7VVUeI/ueWw6SeLDI5kYiIuBJTC1BRURHx8fHEx8cDkJaWRnx8PBkZGVRUVPDb3/6WXbt28eGHH2K1WsnOziY7O5vy8vLq2xg9ejQLFy6s/nvmzJnExcVx5MgRtm3bxjXXXIO7uzvTpk1r7ocnv2JMdCAjIztQYTWYr4FoERFpRqYWoF27djFgwAAGDBgAwIwZMxgwYABz587l6NGjfPnll2RlZdG/f39CQkKql23btlXfRmpqKrm5udV/Z2VlMW3aNCIjI7n++utp164dO3bsoEOHDs3++OT8LBYL8yf1xsvdjS3Juaw9kGN2JBERcRF2MwRtT+ozRCUN9/zaJBZuSqFjQAs2zBhBCy93syOJiIgDcqohaHF+94zqTseAFhzNO8s/YjUQLSIiTU8FSEzXwsudR2KiAXgj7jBHcotNTiQiIs5OBUjswvg+wQzv0Z5yq43HVhw0O46IiDg5FSCxCxaLhfmTe+PpbuHrxBNsOKiBaBERaToqQGI3unVoze8v7wLAoysOUFphNTmRiIg4KxUgsSv/d2UPgv18yDx9ljfiDpsdR0REnJQKkNiVVt4ezPlxIPofsSlkni4xOZGIiDgjFSCxO1f3DWFo13aUVWogWkREmoYKkNgdi8XCo1N64+FmYf3BHDYlnTA7koiIOBkVILFLPYN8uW1YBACPfnmAskoNRIuISONRARK7df+YHnTw9ebIqRL+uSXN7DgiIuJEVIDEbvn6ePLwxCgAXvs6maN5Z01OJCIizkIFSOza1P4duTSiDaUVNp5cqYFoERFpHCpAYtcsFguPTu6DmwVW7ctma3Ku2ZFERMQJqACJ3esV6setQyMAmPflfsorbeYGEhERh6cCJA7hL1f1pH1rL1JPFvPONxqIFhGRhlEBEofg38KTh8ZXDUS/sjGZ7PxSkxOJiIgjUwESh/GbizsxoHMAJeVWnlyVYHYcERFxYCpA4jDc3Cw8PqUPFgt89f0xtqeeMjuSiIg4KBUgcSh9Ovpz0+DOQNVAdIVVA9EiIlJ/KkDicGaOjaRNS08O5RTx7rYjZscREREHpAIkDiegpRd/+3Eg+uUNyZwo1EC0iIjUjwqQOKQbLgmjXyd/isoqeXpVotlxRETEwagAiUNyc7Pw6I8D0Z/vPcp3R06bHUlERByICpA4rP5hAdxwSRgAf1+2n0oNRIuISB2pAIlD+9v4KPxbeJKYXciH32aYHUdERByECpA4tLatvJg5ticAz69LIreozOREIiLiCFSAxOH9bnA4vUP9KCyt5Nk1GogWEZFfpwIkDs/dzcJjU3oD8OmuLPZknDE5kYiI2DsVIHEKA8Pb8puLOwEwd/l+rDbD5EQiImLPVIDEacyaEIWvjwf7jxbw8XcaiBYRkdqpAInT6ODrzYyrqgain1ubxJnicpMTiYiIvVIBEqdyy5BwooJ9ySup4Nm1SWbHERERO6UCJE7Fw92NRydXDUR//F0GP2TlmRtIRETskqkFaPPmzUyaNInQ0FAsFgvLli2rcb1hGMydO5eQkBBatGjBmDFjSE5O/tXbXbRoEREREfj4+DB48GB27tzZRI9A7NHgru2Y2j8Uw4C/Lz+ATQPRIiLyM6YWoOLiYvr168eiRYvOef2zzz7Lq6++yuuvv863335Lq1atGDduHKWltZ/9+5NPPmHGjBnMmzePPXv20K9fP8aNG8eJEyea6mGIHXp4YjStvNz5PjOPz3Znmh1HRETsjMUwDLv457HFYuGLL75g6tSpQNWnP6Ghofz1r39l5syZAOTn5xMUFMTSpUu58cYbz3k7gwcP5tJLL2XhwoUA2Gw2wsLCuO+++5g1a1adshQUFODv709+fj5+fn4Nf3Biirc2H+bJVQm0beXF138dQUBLL7MjiYhIE6rP+7fdzgClpaWRnZ3NmDFjqi/z9/dn8ODBbN++/ZzblJeXs3v37hrbuLm5MWbMmFq3ASgrK6OgoKDGIo7vtssi6BHYmtPF5by4/pDZcURExI7YbQHKzs4GICgoqMblQUFB1df9XG5uLlartV7bACxYsAB/f//qJSwsrIHpxR54/s9A9Ac70jlwLN/kRCIiYi8uqABlZmaSlZVV/ffOnTt54IEHePPNNxstWHOaPXs2+fn51UtmpmZGnMWw7u2J6RuCzYC5GogWEZEfXVAB+t3vfsemTZuAqk9qrrrqKnbu3MmcOXN47LHHGiVYcHAwADk5OTUuz8nJqb7u59q3b4+7u3u9tgHw9vbGz8+vxiLO45GYaFp6ubM7/Qxf7D1qdhwREbEDF1SA9u/fz6BBgwD49NNP6dOnD9u2bePDDz9k6dKljRKsS5cuBAcHs3HjxurLCgoK+Pbbbxk6dOg5t/Hy8mLgwIE1trHZbGzcuLHWbcT5hfi34L4rewCwYHUiBaUVJicSERGzXVABqqiowNvbG4ANGzYwefJkAKKiojh+/Hidb6eoqIj4+Hji4+OBqsHn+Ph4MjIysFgsPPDAAzzxxBN8+eWX7Nu3j1tvvZXQ0NDqX4oBjB49uvoXXwAzZszgrbfe4t133yUhIYG7776b4uJibr/99gt5qOIk7ri8C13btyK3qIyXNBAtIuLyPC5ko969e/P6668TExPD+vXrefzxxwE4duwY7dq1q/Pt7Nq1i1GjRlX/PWPGDACmT5/O0qVL+dvf/kZxcTF/+MMfyMvL4/LLL2fNmjX4+PhUb5Oamkpubm713zfccAMnT55k7ty5ZGdn079/f9asWfOLwWhxLV4ebsyf3Jtbl+zkve3p3HBpGFHB+qpTRMRVXdBxgGJjY7nmmmsoKChg+vTpLFmyBICHH36YxMREPv/880YP2px0HCDn9af3d7PmQDaDurTlkz8MwWKxmB1JREQaSX3evy/4QIhWq5WCggLatGlTfdmRI0do2bIlgYGBF3KTdkMFyHllnSlhzItxlFbYeOXG/kzp39HsSCIi0kia/ECIZ8+epaysrLr8pKen8/LLL5OUlOTw5UecW6c2LblnZHcAnlyZQKEGokVEXNIFFaApU6bw3nvvAZCXl8fgwYN54YUXmDp1KosXL27UgCKN7a4ruhLeriUnCst47esUs+OIiIgJLqgA7dmzh+HDhwPw73//m6CgINLT03nvvfd49dVXGzWgSGPz8XRn/qSqI0Qv2ZpGck6hyYlERKS5XVABKikpwdfXF4B169Zx7bXX4ubmxpAhQ0hPT2/UgCJNYVRUIGOiA6m0Gcz78gB2ck5gERFpJhdUgLp3786yZcvIzMxk7dq1jB07FoATJ05oaFgcxtyre+Pl4ca21FOs2lf7ueJERMT5XFABmjt3LjNnziQiIoJBgwZVH2V53bp1DBgwoFEDijSVzu1acveIbgA8sfIgxWWVJicSEZHmcsE/g8/Ozub48eP069cPN7eqHrVz5078/PyIiopq1JDNTT+Ddx2lFVbGvBhH1pmz3D2yGw+Nd+znroiIK2vyn8FD1clKBwwYwLFjx6rPDD9o0CCHLz/iWnw83Zl7dS8A/rnlMIdPFpmcSEREmsMFFSCbzcZjjz2Gv78/4eHhhIeHExAQwOOPP47NZmvsjCJN6qpeQYyM7ECF1WD+Vwc1EC0i4gIuqADNmTOHhQsX8vTTT7N371727t3LU089xWuvvcbf//73xs4o0qQsFgvzJvXGy92NzYdOsvZAjtmRRESkiV3QDFBoaCivv/569Vngf7J8+XL+/Oc/c/To0UYLaAbNALmm59YmsmhTKh0DWrBhxghaeLmbHUlEROqhyWeATp8+fc5Zn6ioKE6fPn0hNyliuntGdSfU34ejeWdZHKsjRIuIOLMLKkD9+vVj4cKFv7h84cKF9O3bt8GhRMzQ0suDR34ciH5982HSTxWbnEhERJqKx4Vs9OyzzxITE8OGDRuqjwG0fft2MjMzWbVqVaMGFGlOE/oEc3n39mxNyeWxrw7y9m2Xmh1JRESawAV9AjRixAgOHTrENddcQ15eHnl5eVx77bUcOHCA999/v7EzijQbi8XC/Mm98XCzsDHxBBsTNBAtIuKMLvhAiOfy/fffc/HFF2O1WhvrJk2hIWhZsCqBNzYfpnPblqz7yxX4eGogWkTE3jXLgRBFnNl9o3sQ5OdNxukS3og7bHYcERFpZCpAIufQ2tuDOTFVA9H/iE0h83SJyYlERKQxqQCJ1GJS3xCGdG1LWaWNx1ccNDuOiIg0onr9Cuzaa6897/V5eXkNySJiVywWC49O7sPEV7ew7mAOsUknGBkZaHYsERFpBPUqQP7+/r96/a233tqgQCL2JDLYl9uGRfD21jQe/eogQ7u1w9tDA9EiIo6uUX8F5iz0KzD5X4WlFYx6Po7cojIeHBfJPaO6mx1JRETOQb8CE2lEvj6ePDyx6tQvC79O4WjeWZMTiYhIQ6kAidTBNQM6cmlEG85WWHlqZYLZcUREpIFUgETq4KeBaDcLrNx3nK3JuWZHEhGRBlABEqmjXqF+3DIkHIB5X+6nvNJmciIREblQKkAi9TBjbCTtWnmRerKYpdvSzI4jIiIXSAVIpB78W3jy0ISqgehXNiSTU1BqciIREbkQKkAi9fTbizvRPyyA4nIrT2ogWkTEIakAidSTm5uFx6f0wWKBL78/xvbUU2ZHEhGRelIBErkAF3Xy53eDOgMw/8sDVFg1EC0i4khUgEQu0MyxkQS09CQpp5D3tqebHUdEROpBBUjkArVp5cXfxlUNRL+8/hAnCjUQLSLiKOy+AEVERGCxWH6x3HPPPedcf+nSpb9Y18fHp5lTi6u44dIw+nbyp7CskqdXJ5odR0RE6sjuC9B3333H8ePHq5f169cDcN1119W6jZ+fX41t0tP19YQ0DXc3C49N6QPA53uOsuvIaZMTiYhIXdh9AerQoQPBwcHVy4oVK+jWrRsjRoyodRuLxVJjm6CgoGZMLK6mf1gAN1wSBsDflx+gUgPRIiJ2z+4L0P8qLy/ngw8+4Pe//z0Wi6XW9YqKiggPDycsLIwpU6Zw4MCB895uWVkZBQUFNRaR+vjb+Ej8fDxIOF7Av3ZmmB1HRER+hUMVoGXLlpGXl8dtt91W6zqRkZEsWbKE5cuX88EHH2Cz2Rg2bBhZWVm1brNgwQL8/f2rl7CwsCZIL86sXWtvHhwXCcDza5M4VVRmciIRETkfi2EYhtkh6mrcuHF4eXnx1Vdf1XmbiooKoqOjmTZtGo8//vg51ykrK6Os7L9vWAUFBYSFhZGfn4+fn1+Dc4trsNoMJr22lYPHC7j+kk48+9t+ZkcSEXEpBQUF+Pv71+n922E+AUpPT2fDhg3ceeed9drO09OTAQMGkJKSUus63t7e+Pn51VhE6svdzcLjU3sD8OmuLPZmnDE5kYiI1MZhCtA777xDYGAgMTEx9drOarWyb98+QkJCmiiZyH8NDG/Lby7uBMDc5Qew2hzmA1YREZfiEAXIZrPxzjvvMH36dDw8PGpcd+uttzJ79uzqvx977DHWrVvH4cOH2bNnDzfffDPp6en1/uRI5ELNmhCFr7cH+47m8/F3GogWEbFHDlGANmzYQEZGBr///e9/cV1GRgbHjx+v/vvMmTPcddddREdHM3HiRAoKCti2bRu9evVqzsjiwjr4evOXq3oC8NzaJM4Ul5ucSEREfs6hhqCbS32GqETOpdJqI+bVrSTlFPK7wZ156pqLzI4kIuL0nHIIWsSReLi78diUqoHoj3Zm8ENWnrmBRESkBhUgkSYyuGs7pvQPxTCqBqJtGogWEbEbKkAiTejhidG08nInPjOPf++u/WCcIiLSvFSARJpQkJ8PD4ypGoh+ek0i+SUVJicSERFQARJpcrddFkH3wNacLi7nhfVJZscRERFUgESanKe7G49NrhqI/mBHOgeO5ZucSEREVIBEmsGw7u2J6RuCzYB5yw+go0+IiJhLBUikmcyZGE0LT3d2pZ/h8z1HzY4jIuLSVIBEmkloQAvuG90dgAWrEyko1UC0iIhZVIBEmtEdl3eha/tW5BaV8fL6ZLPjiIi4LBUgkWbk7eHOvB8Hot/dfoTE7AKTE4mIuCYVIJFmNqJnB8b1DsJqMzQQLSJiEhUgERP8/epeeHu48W3aab78/pjZcUREXI4KkIgJOrVpyT2jqgain1qVQFFZpcmJRERciwqQiEn+cEVXwtu1JKegjFc3aiBaRKQ5qQCJmMTH0515k3oBsGRrGiknCk1OJCLiOlSAREx0ZVQQo6MCqbQZzPtSA9EiIs1FBUjEZPMm9cbLw41vUk6xal+22XFERFyCCpCIyTq3a8mfRnQD4ImVBykp10C0iEhTUwESsQN/HtmNTm1acDy/lIVfp5gdR0TE6akAidgBH093/n511UD0W1sOc/hkkcmJREScmwqQiJ0Y2yuIET07UGE1mP/VQQ1Ei4g0IRUgETthsViYP7k3Xu5ubD50knUHc8yOJCLitFSAROxIl/atuHN4FwAe++ogZ8utJicSEXFOKkAidubeK7sT6u/D0byzLI5LNTuOiIhTUgESsTMtvTx45MeB6NfjUkk/VWxyIhER56MCJGKHJvQJ5rLu7SivtPHYVwfNjiMi4nRUgETskMVi4dHJvfFws7Ax8QQbEzQQLSLSmFSAROxU90Bf7ri8aiD60a8OUlqhgWgRkcaiAiRix+4b3YMgP28yTpfw5ubDZscREXEaKkAidqy1twcPT4wGYNGmFDJPl5icSETEOagAidi5yf1CGdylLWWVNp5YqYFoEZHGoAIkYucsFguPTemDu5uFtQdyiDt00uxIIiIOTwVIxAFEBvsyfWgEAPO/PEBZpQaiRUQawq4L0Pz587FYLDWWqKio827z2WefERUVhY+PDxdddBGrVq1qprQiTeuBq3rQvrU3abnFvL01zew4IiIOza4LEEDv3r05fvx49bJ169Za1922bRvTpk3jjjvuYO/evUydOpWpU6eyf//+Zkws0jT8fDx5eGLVPwBe25jCsbyzJicSEXFcdl+APDw8CA4Orl7at29f67qvvPIK48eP58EHHyQ6OprHH3+ciy++mIULFzZjYpGmc82AjlwS3oazFVaeXJlgdhwREYdl9wUoOTmZ0NBQunbtyk033URGRkat627fvp0xY8bUuGzcuHFs3779vPdRVlZGQUFBjUXEHv00EO1mgZX7jvNNSq7ZkUREHJJdF6DBgwezdOlS1qxZw+LFi0lLS2P48OEUFhaec/3s7GyCgoJqXBYUFER2dvZ572fBggX4+/tXL2FhYY32GEQaW69QP24ZEg7AvC8PUF5pMzmRiIjjsesCNGHCBK677jr69u3LuHHjWLVqFXl5eXz66aeNej+zZ88mPz+/esnMzGzU2xdpbDOuiqRdKy9SThSxdJsGokVE6suuC9DPBQQE0LNnT1JSUs55fXBwMDk5NU8amZOTQ3Bw8Hlv19vbGz8/vxqLiD3zb+nJQ+OrBqJf2ZBMTkGpyYlERByLQxWgoqIiUlNTCQkJOef1Q4cOZePGjTUuW79+PUOHDm2OeCLN6rcDO9E/LIDicitPrdJAtIhIfdh1AZo5cyZxcXEcOXKEbdu2cc011+Du7s60adMAuPXWW5k9e3b1+vfffz9r1qzhhRdeIDExkfnz57Nr1y7uvfdesx6CSJNxc7Pw2JTeWCywPP4YOw6fMjuSiMivMgyDLcknTZ9ftOsClJWVxbRp04iMjOT666+nXbt27Nixgw4dOgCQkZHB8ePHq9cfNmwY//rXv3jzzTfp168f//73v1m2bBl9+vQx6yGINKm+nQKYNqgzAPd9tJf/7M7CZjNMTiUicm4Jxwu45e2d3PL2Tt7fkW5qFothGHq1/JmCggL8/f3Jz8/XPJDYvTPF5fxm8TYO5xYD0KejH3Mm9mJot3YmJxMRqXKioJQX1h3i092ZGAZ4ubtx75Xd+b/RPRr1furz/q0CdA4qQOJoSiusvPPNEf6xKYXCskoAxkQHMXtiFN06tDY5nYi4qpLySt7anMYbm1MpKa86h2HMRSE8ND6Kzu1aNvr9qQA1kAqQOKpTRWW8vCGZf+3MwGoz8HCzcNPgztw/pidtW3mZHU9EXITNZvCfPVk8vy6JnIIyAAZ0DuCRmGgGhrdtsvtVAWogFSBxdCknClmwKpGNiScA8PXx4N5R3bntsgi8PdxNTicizmxbSi5PrEzg4PGqsyp0atOCh8ZHcXXfECwWS5PetwpQA6kAibP45scXooT/eSGaNSGKmIua/oVIRFxLyokiFqxK+O8/vLw9uPfK7kwfFoGPZ/P8w0sFqIFUgMSZWH/6KHptEicKqz6KvrhzAHNiejEwvI3J6UTE0f38q3f3n756H92Ddq29mzWLClADqQCJMyopr+TNzYd5I+4wZyt+HEbsG8Ks8VGEtW38YUQRcW6lFVaWbjvCoq//98cXgcyaEE33QHN+fKEC1EAqQOLMcgpKeWFdEp/tzqr+Oeptl0Vwz6ju+LfwNDueiNg5wzD46ofjPLM6kaN5ZwHoHerHnJhohnVrb2o2FaAGUgESV3DwWAFPrUpga0ouAG1aenL/6B7cNCQcT3e7PkaqiJhkd/ppHl+RQHxmHgDBfj48OC6SawZ0xM3N/LlCFaAGUgESV2EYBrFJJ3lyVQIpJ4oA6Nq+FbMmRHFVryANSosIAOmninlmTSKr9mUD0NLLnT+N6MZdw7vSwst+flmqAtRAKkDiaiqtNj7+LpOX1h/iVHE5AEO6tuWRmF706ehvcjoRMUt+SQWvfZ3Mu9uPUGE1cLPA9ZeEMeOqngT6+Zgd7xdUgBpIBUhcVWFpBf+ITeXtrWmUV9qwWOCaAR15cFwkIf4tzI4nIs2kvNLGBzvSefXrZPJKKgAY3qM9c2KiiQq23/dFFaAGUgESV5d1poTn1iaxPP4YAD6ebtw1vCt/HNGN1t4eJqcTkaZiGAbrDubw9OpE0n48v2DPoNY8PDGakZGBJqf7dSpADaQCJFIlPjOPJ1ce5LsjZwBo39qbv47tyfWXhOFuBwOPItJ4fsjK44mVCexMOw1A+9ZezLgqkusv6YSHg/wwQgWogVSARP7LMAzWHshmwepE0k+VABAZ5MvDMdGM6NnB5HQi0lDH8s7y3Nokvth7FABvj6pPfP800vE+8VUBaiAVIJFfKq+08f6OdF7dmEz+2aqZgCt6dmDOxGgig31NTici9VVUVsni2BT+uSWNskob8N+Zv9AAx5z5UwFqIBUgkdrllZTz2tcpvPc/vwq54dIw/nJVTwJ97e9XISJSU6XVxie7qn71mVtU9avPQV3a8khMNH07BZgbroFUgBpIBUjk1x3JrTouyOr9VccFafXjcUHutLPjgohIFcMwiD10kqdWJpD843G/uvx43K+xTnLcLxWgBlIBEqm7746c5omVCXz/45FhQ/x9mDnWfo4MKyKQcLzqyO9bkquO/B7w05HfB4fj5eEYA851oQLUQCpAIvVjsxl89cMxnl2TVH1uoD4d/ZgzsRdDu7UzOZ2I6zpRUMoL6w7x2e5MbAZ4ulu4bVgE947qgX9L5zv3nwpQA6kAiVyY0gorS75J4x+bUin68ezQV/UKYvaEKLp2MOfs0CKuqKS8krc2p/HG5lRKyq0AxFwUwkPjo+jcrqXJ6ZqOClADqQCJNExuURkvbzjERzszsdoMPNws3DwknP8b3YO2rbzMjifitGw2g8/3HuW5tYnkFJQB0D8sgEdiorkkoq3J6ZqeClADqQCJNI6UE4U8tSqRrxNPAODr48F9V3Zn+rAIvD00KC3SmLal5vLkygQOHCsAoGNACx6aEMWkviFOMeBcFypADaQCJNK4vknJ5YmVCSQcr3phDmvbgofGRxFzkeu8MIs0lZQTRTy9OoENCT/+Q8Pbg3uu7M5twyLw8XStf2ioADWQCpBI47PaDP6zJ4vn1yZxorDqo/mLOwcwJ6YXA8PbmJxOxPGcKirjlY3JfPhtBlabgbubhZsGd+b+0T1o19rb7HimUAFqIBUgkaZTUl7Jm5sP80bcYc5W/Dic2TeEWeOjCGvrvMOZIo2ltMLK0m1HWPR1CoU//thgTHQgsyZE0z3QtX9soALUQCpAIk0vp6CU59cm8e89WRgGeLm7cftlEfx5VHf8Wzjfz3NFGsowDL764TjPrkkk60zV4SZ6h/oxJyaaYd3am5zOPqgANZAKkEjzOXAsn6dWJfBNyikA2rT05IExPfnd4M54OsgZqEWa2u70qgOO7s3IAyDYz4eZ4yK5VgccrUEFqIFUgESal2EYbEo6wVOrEkn58RD9XTu0YvaEaMZEB2pQWlxWxqkSnlmTyMp9xwFo+eMpZ+7SKWfOSQWogVSARMxRabXx0XeZvLz+EKeKq07SOKRrWx6J6UWfjv4mpxNpPvklFSzclMy729Ipt9pws8D1l4Qx46qeBPrppMO1UQFqIBUgEXMVllbwj9hU3t6aRnmlDYsFrhnQkQfHRRLi38LseCJNpsJq44Md6byyMZm8kgoAhvdoz8MTo4kO0fvRr1EBaiAVIBH7kHWmhOfWJrE8/hgAPp5u3DW8K38a0Y1W3h4mpxNpPIZhsO5gDk+vTiQttxiAHoGteTgmmpE9O+hr4DpSAWogFSAR+xKfmceTKw/y3ZEzALRv7c3MsT257pIw3DUAKg5uX1Y+T6w8yLdppwFo39qLv1zVkxsuCcNDPwSoFxWgBlIBErE/hmGw9kA2C1Ynkn6qBICoYF8enhjNFT07mJxOpP6O5Z3l+bVJfL73KADeHm7cObwLfxrRDV8fHQriQqgANZAKkIj9Kq+08f6OdF7dmEz+2aoZiRE9OzAnJpqeQb4mpxP5dUVllSyOTeGfW9Ioq7QBVTNuM8dF0jFAM24NUZ/3b7v+bG3BggVceuml+Pr6EhgYyNSpU0lKSjrvNkuXLsVisdRYfHw0MS/iLLw83Ljj8i7EPTiSOy7vgqe7hbhDJxn/8mZmf76Pkz+eZkPE3lRabXz4bTojn9vEok2plFXaGNSlLV/eexkv3dBf5aeZ2fUUYVxcHPfccw+XXnoplZWVPPzww4wdO5aDBw/SqlWrWrfz8/OrUZQ0PCbifAJaevH3q3txy5Bwnl6dyJoD2Xy0M4Mv449y98hu3HG5jpMi9iM26QRPrUrgUE7Vca4i2rVk9sRoxvYK0nuUSRzqK7CTJ08SGBhIXFwcV1xxxTnXWbp0KQ888AB5eXkXfD/6CkzE8exMO82TKw/yfVY+ACH+Pjw4LpKp/XWkXDFPYnYBT65MYEtyLgABLT35vyt7cPOQcLw87PpLGIdUn/dvu/4E6Ofy86te2Nq2bXve9YqKiggPD8dms3HxxRfz1FNP0bt371rXLysro6zsvx+bFxQUNE5gEWk2g7q05Ys/X8ZXPxzj2TVJHM07y4xPv+edb44wJyaaIV3bmR1RXMiJwlJeXHeIT3dlYjPA093C9KER3HdlD/xbasDZHjjMJ0A2m43JkyeTl5fH1q1ba11v+/btJCcn07dvX/Lz83n++efZvHkzBw4coFOnTufcZv78+Tz66KO/uFyfAIk4ptIKK0u+SeMfm1Ip+vFs2Vf1CmL2hCi6dnDts2VL0zpbbuWtLYd5PS6VknIrABMvCuah8VGEt6t9dEMah1P+Cuzuu+9m9erVbN26tdYicy4VFRVER0czbdo0Hn/88XOuc65PgMLCwlSARBxcblEZL284xEc7M7HaDDzcLNw8JJz7R/egTSsvs+OJE7HZDD7fe5Tn1yaRXVAKQP+wAB6JieaSiPN/ayGNx+kK0L333svy5cvZvHkzXbp0qff21113HR4eHnz00Ud1Wl8zQCLOJTmnkAWrE/k68QQAvj4e3Hdld6YPi8DbQ4PS0jDbUnN5cmUCB45VjU90DGjBQxOimNQ3RAPOzcxpZoAMw+C+++7jiy++IDY29oLKj9VqZd++fUycOLEJEoqII+gR5MuS2y5la3IuT6w8SGJ2IU+tSuT9HenMGh/NxIuC9UYl9ZZ6sogFqxLYkPBjsfb24J4ru3PbsAh8PFWs7Z1dfwL05z//mX/9618sX76cyMjI6sv9/f1p0aLqeAm33norHTt2ZMGCBQA89thjDBkyhO7du5OXl8dzzz3HsmXL2L17N7169arT/eoTIBHnZbUZ/GdPFs+vTeLEj8cMGhjehjkx0VzcuY3J6cQRnC4u55UNh/jw2wwqbQbubhZuGtyZ+0f3oF1rb7PjuTSn+QRo8eLFAIwcObLG5e+88w633XYbABkZGbi5/fenhGfOnOGuu+4iOzubNm3aMHDgQLZt21bn8iMizs3dzcL1l4Rxdd8Q3tx8mDfiDrM7/QzX/mMbV/cN4aHxUYS1bWl2TLFDpRVW3t12hIWbUigsrRquHxMdyKwJ0XQP1HC9o7HrT4DMok+ARFxHTkEpz69N4t97sjAM8HJ34/bLIvjzqO74t9DPlaVqHGPFD8d5Zk0iWWfOAtArxI9HYqIZ1r29yenkfzndEHRzUwEScT0HjuXz1KoEvkk5BUCblp48MKYnvxvcGU+dkdtl7U4/wxMrD7I3Iw+AID9vZo6N5NqLO+GuA2zaHRWgBlIBEnFNhmGwKekET61KJOVE1SkLunZoxewJ0YyJDtSgtAvJOFXCM2sSWbnvOAAtvdz54xXduOuKLrT0suvpEZemAtRAKkAirq3SauOj7zJ5ef0hThWXAzC0azvmxETTp6O/yemkKeWfrWDh18m8uy2dcqsNiwWuHxjGX8f2JNBPJ9a2dypADaQCJCIABaUVLI5N5e2taZRXVr0ZXjugEw+OiyTYX2+GzqTCauPDHem8sjGZMyUVAAzv0Z6HJ0YTHaL3AUehAtRAKkAi8r+yzpTw3NoklscfA8DH040/DO/KH0d0o5W3vg5xZIZhsP5gDk+vTuRwbjEAPQJb83BMNCN7dtDXng5GBaiBVIBE5FziM/N4YsVBdqWfAaCDrzd/vaon110SpoFYB7QvK58nVh7k27TTALRv7cVfrurJDZeE4aHBd4ekAtRAKkAiUhvDMFizP5un1ySSfqoEgKhgXx6eGM0VPTuYnE7q4ljeWZ5fm8Tne48C4O3hxh2Xd+Hukd3w9dGhDxyZClADqQCJyK8pr7Tx3vYjvPZ1Cvlnq2ZGRvTswJyYaHoG+ZqcTs6lqKyS12NTeWvLYcoqbQBM7R/Kg+Oj6BjQwuR00hhUgBpIBUhE6iqvpJxXN6bw/o4jVFgN3Cxw46DO/GVMTzr46rQI9qDSauPTXVm8uP4QuUVVpz8ZFNGWOTHR9AsLMDecNCoVoAZSARKR+jqSW8zTqxNZcyAbgFZe7vx5VHfuuLyLToxpotikEzy1KoFDOVXHdYpo15LZE6MZ2ytIA85OSAWogVSARORC7Uw7zZMrD/J9Vj4AIf4+PDgukqn9O+KmQelmk5hdwJMrE9iSnAtAQEtP/u/KHtw8JBwvDw04OysVoAZSARKRhrDZDL764RjPrkniaF7VuaMu6ujPnJhohnRtZ3I653aisJQX1x3i012Z2AzwdLcwfWgE913ZA/+WGnB2dipADaQCJCKNobTCypJv0vjHplSKyqrOHj62VxCzJkTRtYPOHt6YzpZb+eeWwyyOS6Wk3ArAxIuCeWh8FOHtWpmcTpqLClADqQCJSGPKLSrj5Q2H+GhnJlabgYebhZuHhHP/6B60aeVldjyHZrMZfLH3KM+tTSK7oBSA/mEBPBITzSURbU1OJ81NBaiBVIBEpCkk5xSyYHUiXyeeAMDPx4P7ruzBrcPC8fbQoHR9bU89xZOrDrL/aAEAHQNa8NCEKCb1DdGAs4tSAWogFSARaUpbk3N5YuVBErMLAejctiUPjY9i4kXBeuOug9STRSxYlciGhBwAfL09uOfK7tw2LEK/uHNxKkANpAIkIk3NajP4z+4snl+XxInCqmPTDAxvw5yYaC7u3MbkdPbpdHE5r2w4xIffZlBpM3B3s/C7QZ15YEwP2rXWMZdEBajBVIBEpLkUl1Xy5ubDvLn5MGcrqoZ3r+4bwkPjowhr29LkdPahrNLK0m+OsHBTCoWlVcPko6MCmT0xiu6BOuq2/JcKUAOpAIlIc8vOL+WFdUn8e08WhgFeHm7cflkE94zqjp+Lnp/KMAxW7jvO06sTyTpTdTiBXiF+PBITzbDu7U1OJ/ZIBaiBVIBExCwHjuXz1KoEvkk5BUDbVl48MKYH0wZ1xtOFzlC+O/0MT648yJ6MPACC/LyZOTaSay/uhLsOKCm1UAFqIBUgETGTYRhsSjrBkysTSD1ZDEDXDq14eEI0o6MDnXpQOvN0CU+vSWTlD8cBaOHpzp9GdOOuK7rQ0svD5HRi71SAGkgFSETsQYXVxsc7M3hpQzKni8sBGNq1HXNiounT0d/kdI0r/2wFizalsPSbI5RbbVgscP3AMP46tieBfj5mxxMHoQLUQCpAImJPCkor+MemVJZ8k0Z5ZVU5uHZAJx4cF0mwv2OXgwqrjQ93pPPKxmTOlFQAcHn39jw8MZpeoXr9lfpRAWogFSARsUeZp0t4bm0SX35/DAAfTzf+MLwrfxzRjVbejvX1kGEYrD+Yw9OrEzmcW/U1X/fA1syZGM3IyA5O/TWfNB0VoAZSARIRe7Y34wxPrkxgV/oZADr4ejNzbE9+OzDMIQaE9x/N54mVB9lx+DQA7Vp58ZerenLjpWF4uNCgtzQ+FaAGUgESEXtnGAZr9mfz9JpE0k+VABAV7MucmGiG9+hgcrpzO55/lufWJvHF3qMYBnh7uHHH5V24e2Q3fF30p/7SuFSAGkgFSEQcRXmljfe2H+G1r1PIP1s1QzMysgMPT4ymZ5B9HCSwqKySN+JSeWvLYUorbABM7R/Kg+Oj6BjQwuR04kxUgBpIBUhEHE1eSTmvbkzh/R1HqLAauFngxkGd+cuYnnTwNec0EZVWG5/tzuKFdYfILao63cegiLbMiYmmX1iAKZnEuakANZAKkIg4qiO5xTy9OpE1B7IBaO3twd0ju3HH5V2a9UShcYdO8tTKBJJyqk74GtGuJbMmRDOud5AGnKXJqAA1kAqQiDi6nWmneXLlQb7Pygcg1N+HB8dHMqVfR9yacFA6KbuQJ1clsPnQSQD8W3hy/+ge3DwkHC8PDThL01IBaiAVIBFxBjabwZffH+PZNYkcyy8FoG8nf+ZMjGZw13aNel8nCkt5af0hPvkuE5sBnu4Wpg+N4L4re+DfUgPO0jxUgBpIBUhEnElphZW3t6axODaVorKqs6mP7RXE7InRdGnfqkG3fbbcyj+3HOb1uFSKy6vOZj+hTzCzJkQR3q5hty1SXypADaQCJCLOKLeojJfWH+KjnRnYDPBws3DzkHDuH92DNq286nVbNpvBF3uP8vy6JI7/+OlSv7AAHomJ5tKItk0RX+RXqQA1kAqQiDiz5JxCnlqVwKakqjkdPx8P/m90D24ZGo63x68PSm9PPcWTqw6y/2gBAB0DWvC38ZFM6hvapPNFIr+mPu/fDjGRtmjRIiIiIvDx8WHw4MHs3LnzvOt/9tlnREVF4ePjw0UXXcSqVauaKamIiP3rEeTLO7cP4oM7BhMV7EtBaSVPrEzgqhc3s2rfcWr7d/Hhk0Xc9d4upr21g/1HC/D19uCh8VFs/OsIpvRv2uFqkcZm9wXok08+YcaMGcybN489e/bQr18/xo0bx4kTJ865/rZt25g2bRp33HEHe/fuZerUqUydOpX9+/c3c3IREft2eY/2rPy/4Tz7m74E+nqTcbqEP3+4h+te387ejDPV650uLmf+lwcY+9Jm1h/Mwd3Nwi1Dwol9cCR3j+zWrD+vF2ksdv8V2ODBg7n00ktZuHAhADabjbCwMO677z5mzZr1i/VvuOEGiouLWbFiRfVlQ4YMoX///rz++ut1uk99BSYirqa4rJI3Nh/mzc2p1UdrntQvlOgQXxbHplJYWjU8PToqkNkTo+geaB9HmRb5X07zFVh5eTm7d+9mzJgx1Ze5ubkxZswYtm/ffs5ttm/fXmN9gHHjxtW6PkBZWRkFBQU1FhERV9LK24MZV/UkduYofjuwExYLfPX9MZ5dk0RhaSXRIX58eOdg3r7tUpUfcQp2XYByc3OxWq0EBQXVuDwoKIjs7OxzbpOdnV2v9QEWLFiAv79/9RIWFtbw8CIiDijY34fnr+vHivsuZ3iP9oS3a8mzv+3Livsu57Lu7c2OJ9JoPMwOYA9mz57NjBkzqv8uKChQCRIRl9Y71J/37xhsdgyRJmPXBah9+/a4u7uTk5NT4/KcnByCg4PPuU1wcHC91gfw9vbG29uckwWKiIhI87Prr8C8vLwYOHAgGzdurL7MZrOxceNGhg4des5thg4dWmN9gPXr19e6voiIiLgeu/4ECGDGjBlMnz6dSy65hEGDBvHyyy9TXFzM7bffDsCtt95Kx44dWbBgAQD3338/I0aM4IUXXiAmJoaPP/6YXbt28eabb5r5MERERMSO2H0BuuGGGzh58iRz584lOzub/v37s2bNmupB54yMDNzc/vtB1rBhw/jXv/7FI488wsMPP0yPHj1YtmwZffr0MeshiIiIiJ2x++MAmUHHARIREXE8TnMcIBEREZGmoAIkIiIiLkcFSERERFyOCpCIiIi4HBUgERERcTkqQCIiIuJyVIBERETE5agAiYiIiMtRARIRERGXY/enwjDDTwfHLigoMDmJiIiI1NVP79t1OcmFCtA5FBYWAhAWFmZyEhEREamvwsJC/P39z7uOzgV2DjabjWPHjuHr64vFYmnU2y4oKCAsLIzMzEydZ+xXaF/VnfZV3Wlf1Z32Vd1pX9VdU+4rwzAoLCwkNDS0xonSz0WfAJ2Dm5sbnTp1atL78PPz0/8kdaR9VXfaV3WnfVV32ld1p31Vd021r37tk5+faAhaREREXI4KkIiIiLgcFaBm5u3tzbx58/D29jY7it3Tvqo77au6076qO+2rutO+qjt72VcaghYRERGXo0+ARERExOWoAImIiIjLUQESERERl6MCJCIiIi5HBagJLFq0iIiICHx8fBg8eDA7d+487/qfffYZUVFR+Pj4cNFFF7Fq1apmSmq++uyrpUuXYrFYaiw+Pj7NmNYcmzdvZtKkSYSGhmKxWFi2bNmvbhMbG8vFF1+Mt7c33bt3Z+nSpU2e017Ud3/Fxsb+4nllsVjIzs5unsAmWbBgAZdeeim+vr4EBgYydepUkpKSfnU7V3y9upB95aqvVwCLFy+mb9++1Qc6HDp0KKtXrz7vNmY8r1SAGtknn3zCjBkzmDdvHnv27KFfv36MGzeOEydOnHP9bdu2MW3aNO644w727t3L1KlTmTp1Kvv372/m5M2vvvsKqo4cevz48eolPT29GRObo7i4mH79+rFo0aI6rZ+WlkZMTAyjRo0iPj6eBx54gDvvvJO1a9c2cVL7UN/99ZOkpKQaz63AwMAmSmgf4uLiuOeee9ixYwfr16+noqKCsWPHUlxcXOs2rvp6dSH7Clzz9QqgU6dOPP300+zevZtdu3Zx5ZVXMmXKFA4cOHDO9U17XhnSqAYNGmTcc8891X9brVYjNDTUWLBgwTnXv/76642YmJgalw0ePNj44x//2KQ57UF999U777xj+Pv7N1M6+wQYX3zxxXnX+dvf/mb07t27xmU33HCDMW7cuCZMZp/qsr82bdpkAMaZM2eaJZO9OnHihAEYcXFxta7jyq9X/6su+0qvVzW1adPG+Oc//3nO68x6XukToEZUXl7O7t27GTNmTPVlbm5ujBkzhu3bt59zm+3bt9dYH2DcuHG1ru8sLmRfARQVFREeHk5YWNh5/0Xhylz1OdVQ/fv3JyQkhKuuuopvvvnG7DjNLj8/H4C2bdvWuo6eW1Xqsq9Ar1cAVquVjz/+mOLiYoYOHXrOdcx6XqkANaLc3FysVitBQUE1Lg8KCqp1niA7O7te6zuLC9lXkZGRLFmyhOXLl/PBBx9gs9kYNmwYWVlZzRHZYdT2nCooKODs2bMmpbJfISEhvP766/znP//hP//5D2FhYYwcOZI9e/aYHa3Z2Gw2HnjgAS677DL69OlT63qu+nr1v+q6r1z99Wrfvn20bt0ab29v/vSnP/HFF1/Qq1evc65r1vNKZ4MXhzF06NAa/4IYNmwY0dHRvPHGGzz++OMmJhNHFhkZSWRkZPXfw4YNIzU1lZdeeon333/fxGTN55577mH//v1s3brV7Ch2r677ytVfryIjI4mPjyc/P59///vfTJ8+nbi4uFpLkBn0CVAjat++Pe7u7uTk5NS4PCcnh+Dg4HNuExwcXK/1ncWF7Kuf8/T0ZMCAAaSkpDRFRIdV23PKz8+PFi1amJTKsQwaNMhlnlf33nsvK1asYNOmTXTq1Om867rq69VP6rOvfs7VXq+8vLzo3r07AwcOZMGCBfTr149XXnnlnOua9bxSAWpEXl5eDBw4kI0bN1ZfZrPZ2LhxY63ffQ4dOrTG+gDr16+vdX1ncSH76uesViv79u0jJCSkqWI6JFd9TjWm+Ph4p39eGYbBvffeyxdffMHXX39Nly5dfnUbV31uXci++jlXf72y2WyUlZWd8zrTnldNOmLtgj7++GPD29vbWLp0qXHw4EHjD3/4gxEQEGBkZ2cbhmEYt9xyizFr1qzq9b/55hvDw8PDeP75542EhARj3rx5hqenp7Fv3z6zHkKzqe++evTRR421a9caqampxu7du40bb7zR8PHxMQ4cOGDWQ2gWhYWFxt69e429e/cagPHiiy8ae/fuNdLT0w3DMIxZs2YZt9xyS/X6hw8fNlq2bGk8+OCDRkJCgrFo0SLD3d3dWLNmjVkPoVnVd3+99NJLxrJly4zk5GRj3759xv3332+4ubkZGzZsMOshNIu7777b8Pf3N2JjY43jx49XLyUlJdXr6PWqyoXsK1d9vTKMqv/H4uLijLS0NOOHH34wZs2aZVgsFmPdunWGYdjP80oFqAm89tprRufOnQ0vLy9j0KBBxo4dO6qvGzFihDF9+vQa63/66adGz549DS8vL6N3797GypUrmzmxeeqzrx544IHqdYOCgoyJEycae/bsMSF18/rpZ9o/X37aN9OnTzdGjBjxi2369+9veHl5GV27djXeeeedZs9tlvrur2eeecbo1q2b4ePjY7Rt29YYOXKk8fXXX5sTvhmdax8BNZ4rer2qciH7ylVfrwzDMH7/+98b4eHhhpeXl9GhQwdj9OjR1eXHMOzneWUxDMNo2s+YREREROyLZoBERETE5agAiYiIiMtRARIRERGXowIkIiIiLkcFSERERFyOCpCIiIi4HBUgERERcTkqQCIiIuJyVIBERETE5agAiYiIiMtRARIRERGXowIkIk5txYoVBAQEYLVaAYiPj8disTBr1qzqde68805uvvlmsyKKiAlUgETEqQ0fPpzCwkL27t0LQFxcHO3btyc2NrZ6nbi4OEaOHGlOQBExhQqQiDg1f39/+vfvX114YmNj+ctf/sLevXspKiri6NGjpKSkMGLECHODikizUgESEac3YsQIYmNjMQyDLVu2cO211xIdHc3WrVuJi4sjNDSUHj16mB1TRJqRh9kBRESa2siRI1myZAnff/89np6eREVFMXLkSGJjYzlz5ow+/RFxQfoESESc3k9zQC+99FJ12fmpAMXGxmr+R8QFqQCJiNNr06YNffv25cMPP6wuO1dccQV79uzh0KFD+gRIxAWpAImISxgxYgRWq7W6ALVt25ZevXoRHBxMZGSkueFEpNlZDMMwzA4hIiIi0pz0CZCIiIi4HBUgERERcTkqQCIiIuJyVIBERETE5agAiYiIiMtRARIRERGXowIkIiIiLkcFSERERFyOCpCIiIi4HBUgERERcTkqQCIiIuJy/h9++pRT7BuVLQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01_basic with jax"
      ],
      "metadata": {
        "id": "O12vMT2Fqhur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_basic with jax\n",
        "## jax.numpy as jnp , from jax import grad, jnp.mean() 등을 사용\n",
        "\n",
        "import datetime\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "# 데이터 정의\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "# forward pass\n",
        "def forward(x, w):\n",
        "  return x * w\n",
        "# loss\n",
        "def loss(w, x, y):\n",
        "  y_pred = forward(x, w)\n",
        "  return jnp.mean((y_pred - y) **2)\n",
        "\n",
        "# grad를 계산하는 함수 생성\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "# 초기 w 값 설정\n",
        "w = 1.0\n",
        "\n",
        "# w업데이트하면서 손실감소\n",
        "num_epochs = 100\n",
        "losses = []\n",
        "lr = 0.01\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  grad_w = grad_loss(w, x_data, y_data)\n",
        "  w -= lr * grad_w\n",
        "  loss_val = loss(w, x_data, y_data)\n",
        "  losses.append(loss_val)\n",
        "  if epoch % 10 == 0:\n",
        "      print(f'Epoch {epoch + 1}, Loss {loss_val}')\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9lydl-tGHAY1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "32c46b21-d01c-4b75-ebf8-8fdd9932ddc0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 3.836207151412964\n",
            "Epoch 11, Loss 0.5405738353729248\n",
            "Epoch 21, Loss 0.0761740654706955\n",
            "Epoch 31, Loss 0.01073399931192398\n",
            "Epoch 41, Loss 0.0015125819481909275\n",
            "Epoch 51, Loss 0.00021314274636097252\n",
            "Epoch 61, Loss 3.003445999638643e-05\n",
            "Epoch 71, Loss 4.233250365359709e-06\n",
            "Epoch 81, Loss 5.96372842665005e-07\n",
            "Epoch 91, Loss 8.396483508477104e-08\n",
            "0:00:02.927140\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02_manual_gradient.py"
      ],
      "metadata": {
        "id": "6bateGiDivo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = 1.0\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y) # 오차제곱\n",
        "\n",
        "# compute gradient\n",
        "def gradient(x, y):\n",
        "  return 2 * x * (x * w - y)\n",
        "\n",
        "# before training\n",
        "print('prediction (before training)', 4, forward(4))\n",
        "\n",
        "# training loop\n",
        "for epoch in range(10):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad = gradient(x_val, y_val)\n",
        "    w = w - 0.01 * grad\n",
        "    print('\\tgrad: ', x_val, y_val, round(grad, 2))\n",
        "    l = loss(x_val, y_val) # 오차 제곱을 데이터셋 별로 더하여 계산\n",
        "  print('progress:', epoch, 'w=', round(w, 2), 'loss=', round(1, 2))\n",
        "\n",
        "#after training\n",
        "print('predicted score (after training)', 4, forward(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi2OAKCRlrco",
        "outputId": "55d128cd-425b-4df4-9ec2-41b8b5b65e6e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.84\n",
            "\tgrad:  3.0 6.0 -16.23\n",
            "progress: 0 w= 1.26 loss= 1\n",
            "\tgrad:  1.0 2.0 -1.48\n",
            "\tgrad:  2.0 4.0 -5.8\n",
            "\tgrad:  3.0 6.0 -12.0\n",
            "progress: 1 w= 1.45 loss= 1\n",
            "\tgrad:  1.0 2.0 -1.09\n",
            "\tgrad:  2.0 4.0 -4.29\n",
            "\tgrad:  3.0 6.0 -8.87\n",
            "progress: 2 w= 1.6 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.81\n",
            "\tgrad:  2.0 4.0 -3.17\n",
            "\tgrad:  3.0 6.0 -6.56\n",
            "progress: 3 w= 1.7 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.6\n",
            "\tgrad:  2.0 4.0 -2.34\n",
            "\tgrad:  3.0 6.0 -4.85\n",
            "progress: 4 w= 1.78 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.44\n",
            "\tgrad:  2.0 4.0 -1.73\n",
            "\tgrad:  3.0 6.0 -3.58\n",
            "progress: 5 w= 1.84 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.33\n",
            "\tgrad:  2.0 4.0 -1.28\n",
            "\tgrad:  3.0 6.0 -2.65\n",
            "progress: 6 w= 1.88 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.24\n",
            "\tgrad:  2.0 4.0 -0.95\n",
            "\tgrad:  3.0 6.0 -1.96\n",
            "progress: 7 w= 1.91 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.18\n",
            "\tgrad:  2.0 4.0 -0.7\n",
            "\tgrad:  3.0 6.0 -1.45\n",
            "progress: 8 w= 1.93 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.13\n",
            "\tgrad:  2.0 4.0 -0.52\n",
            "\tgrad:  3.0 6.0 -1.07\n",
            "progress: 9 w= 1.95 loss= 1\n",
            "predicted score (after training) 4 7.804863933862125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "MVEsUBeOqkZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "w = 1.0\n",
        "\n",
        "def forward(x, w):\n",
        "  return x * w\n",
        "\n",
        "def loss(w, x, y):\n",
        "  y_pred = forward(x, w)\n",
        "  return jnp.mean((y_pred - y) ** 2)\n",
        "\n",
        "# gradient 계산,loss 함수를 입력으로 받아들여 그래디언트 계산하는 함수로 반환\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "print('prediction (before_training)', forward(4, w))\n",
        "\n",
        "# training\n",
        "learning_rate= 0.01\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad_w = grad_loss(w, x_val, y_val)\n",
        "    w -= learning_rate * grad_w\n",
        "    print('\\tgrad:', x_val, y_val, round(grad_w, 2))\n",
        "    loss_val = loss(w, x_val, y_val)\n",
        "  print('progress:', epoch, 'w=', round(w, 2), 'loss=', round(loss_val, 2))\n",
        "\n",
        "# 학습 후 예측\n",
        "print('predicted score (after training):', forward(4, w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXDrwd8rnDUU",
        "outputId": "d5b8da8f-43ac-4ab7-9285-d0747570f06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before_training) 4.0\n",
            "\tgrad: 1.0 2.0 -2.0\n",
            "\tgrad: 2.0 4.0 -7.8399997\n",
            "\tgrad: 3.0 6.0 -16.23\n",
            "progress: 0 w= 1.26 loss= 4.92\n",
            "\tgrad: 1.0 2.0 -1.48\n",
            "\tgrad: 2.0 4.0 -5.7999997\n",
            "\tgrad: 3.0 6.0 -12.0\n",
            "progress: 1 w= 1.4499999 loss= 2.69\n",
            "\tgrad: 1.0 2.0 -1.09\n",
            "\tgrad: 2.0 4.0 -4.29\n",
            "\tgrad: 3.0 6.0 -8.87\n",
            "progress: 2 w= 1.5999999 loss= 1.4699999\n",
            "\tgrad: 1.0 2.0 -0.81\n",
            "\tgrad: 2.0 4.0 -3.1699998\n",
            "\tgrad: 3.0 6.0 -6.56\n",
            "progress: 3 w= 1.6999999 loss= 0.79999995\n",
            "\tgrad: 1.0 2.0 -0.59999996\n",
            "\tgrad: 2.0 4.0 -2.34\n",
            "\tgrad: 3.0 6.0 -4.85\n",
            "progress: 4 w= 1.78 loss= 0.44\n",
            "\tgrad: 1.0 2.0 -0.44\n",
            "\tgrad: 2.0 4.0 -1.73\n",
            "\tgrad: 3.0 6.0 -3.58\n",
            "progress: 5 w= 1.8399999 loss= 0.24\n",
            "\tgrad: 1.0 2.0 -0.32999998\n",
            "\tgrad: 2.0 4.0 -1.28\n",
            "\tgrad: 3.0 6.0 -2.6499999\n",
            "progress: 6 w= 1.88 loss= 0.13\n",
            "\tgrad: 1.0 2.0 -0.24\n",
            "\tgrad: 2.0 4.0 -0.95\n",
            "\tgrad: 3.0 6.0 -1.9599999\n",
            "progress: 7 w= 1.91 loss= 0.07\n",
            "\tgrad: 1.0 2.0 -0.17999999\n",
            "\tgrad: 2.0 4.0 -0.7\n",
            "\tgrad: 3.0 6.0 -1.4499999\n",
            "progress: 8 w= 1.93 loss= 0.04\n",
            "\tgrad: 1.0 2.0 -0.13\n",
            "\tgrad: 2.0 4.0 -0.52\n",
            "\tgrad: 3.0 6.0 -1.0699999\n",
            "progress: 9 w= 1.9499999 loss= 0.02\n",
            "predicted score (after training): 7.8048644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 manual gradient 계산할 때는 loss 계산에서 틀린 것이 없었다."
      ],
      "metadata": {
        "id": "kPHee6J6i1Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_w, loss_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zjYLusNpLwt",
        "outputId": "4e7aaf54-043c-45c0-c742-7b49d6d2e8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(-1.0708666, dtype=float32, weak_type=True),\n",
              " DeviceArray(0.02141885, dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03_auto_gradient.py"
      ],
      "metadata": {
        "id": "UDkcoyF_qV4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l.backward()"
      ],
      "metadata": {
        "id": "LSZ-pAkqlLHJ",
        "outputId": "1fa36e20-45cd-4592-dcf6-46eb5653b2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-99d4765682f1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pdb\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "w = torch.tensor([1.0], requires_grad = True)\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(y_pred, y_val):\n",
        "  return (y_pred - y_val) **2\n",
        "\n",
        "# before training\n",
        "print('prediction (before training)', 4, forward(4).item())\n",
        "\n",
        "# training loop\n",
        "for epoch in range(1):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    y_pred = forward(x_val) # 1) forward pass\n",
        "    l = loss(y_pred, y_val) # 2) compute loss\n",
        "    print(f'{y_pred},{y_val} loss : {l.item()}')\n",
        "    l.backward() # 3) backpropagation to update weights\n",
        "    #print('\\t grad:', x_val, y_val, w.grad.item())\n",
        "    w.data = w.data - 0.01 * w.grad.item()\n",
        "\n",
        "    w.grad.data.zero_()\n",
        "  print(f'Epoch: {epoch} | loss : {l.item()}')\n",
        "\n",
        "# after training\n",
        "print('prediction (after training)', 4, forward(4).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqTUzucjpVbX",
        "outputId": "f4439ef6-5d74-4ce0-f7db-986b9ab34ce3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "tensor([1.], grad_fn=<MulBackward0>),2.0 loss : 1.0\n",
            "tensor([2.0400], grad_fn=<MulBackward0>),4.0 loss : 3.841600179672241\n",
            "tensor([3.2952], grad_fn=<MulBackward0>),6.0 loss : 7.315943717956543\n",
            "Epoch: 0 | loss : 7.315943717956543\n",
            "prediction (after training) 4 5.042752265930176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss(2.04, 4)"
      ],
      "metadata": {
        "id": "DnthykkpriNw",
        "outputId": "26879f86-80f5-4a78-e7b6-c3b2db54cc5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.8415999999999997"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax\n",
        "- torch 구현보다 예측력이 떨어진다!?"
      ],
      "metadata": {
        "id": "kSwVz9EdqnPf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IdiTnoPKro1K",
        "outputId": "4f64b494-5849-4060-f00c-d68209c9c66f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred : 1.0, y_true:2.0\n",
            "torch loss: 1.0, w grad : -2.0\n",
            "jax loss: 1.0, y grad: -2.0\n",
            "jax loss: 1.0, param grad: [-2.]\n",
            "----------\n",
            "y_pred : 2.0, y_true:4.0\n",
            "torch loss: 4.0, w grad : -8.0\n",
            "jax loss: 4.0, y grad: -4.0\n",
            "jax loss: 4.0, param grad: [-8.]\n",
            "----------\n",
            "y_pred : 3.0, y_true:6.0\n",
            "torch loss: 9.0, w grad : -18.0\n",
            "jax loss: 9.0, y grad: -6.0\n",
            "jax loss: 9.0, param grad: [-18.]\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수정 전\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "w = jnp.array([1.0]) # 당연하게 requires_grad = True 는 없다\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "def loss(y_pred, y_val):\n",
        "  return (y_pred - y_val)**2\n",
        "\n",
        "learning_rate = 0.01\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "print('prediction (before training)', 4, forward(4)[0])\n",
        "\n",
        "for epoch in range(20):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    # y_pred = forward(x_val) # 1) forward pass\n",
        "    #l = loss(y_pred, y_val) # 2) compute loss\n",
        "    #l.backward() # 3) backpropagation to update weights\n",
        "    #print('\\t grad:', x_val, y_val, w.grad.item())\n",
        "    y_pred = forward(x_val)\n",
        "    grad_w = grad_loss(y_pred[0],y_val)\n",
        "    w -= learning_rate * grad_w\n",
        "    print('\\tgrad: ', x_val, y_val, grad_w)\n",
        "\n",
        "  print(f'Epoch: {epoch} | Loss: {loss(forward(x_data), y_data)}')\n",
        "\n",
        "print('prediction (after training)', 4, forward(4)[0], '-> 잘못된 grad() 사용으로 인한 결과물 오류 ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTRvGqLtqm7X",
        "outputId": "294d841d-8ea6-41d7-992a-e4106a171a1b"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -3.92\n",
            "\tgrad:  3.0 6.0 -5.6448\n",
            "Epoch: 0 | Loss: [0.7820786 3.1283145 7.038707 ]\n",
            "\tgrad:  1.0 2.0 -1.7687042\n",
            "\tgrad:  2.0 4.0 -3.46666\n",
            "\tgrad:  3.0 6.0 -4.99199\n",
            "Epoch: 1 | Loss: [0.6116468 2.446587  5.5048213]\n",
            "\tgrad:  1.0 2.0 -1.564157\n",
            "\tgrad:  2.0 4.0 -3.0657477\n",
            "\tgrad:  3.0 6.0 -4.4146767\n",
            "Epoch: 2 | Loss: [0.4783557 1.9134228 4.3052006]\n",
            "\tgrad:  1.0 2.0 -1.3832653\n",
            "\tgrad:  2.0 4.0 -2.7111998\n",
            "\tgrad:  3.0 6.0 -3.9041271\n",
            "Epoch: 3 | Loss: [0.37411162 1.4964465  3.3670046 ]\n",
            "\tgrad:  1.0 2.0 -1.2232933\n",
            "\tgrad:  2.0 4.0 -2.397655\n",
            "\tgrad:  3.0 6.0 -3.4526234\n",
            "Epoch: 4 | Loss: [0.29258466 1.1703386  2.6332629 ]\n",
            "\tgrad:  1.0 2.0 -1.0818219\n",
            "\tgrad:  2.0 4.0 -2.1203709\n",
            "\tgrad:  3.0 6.0 -3.0533333\n",
            "Epoch: 5 | Loss: [0.22882412 0.9152965  2.0594177 ]\n",
            "\tgrad:  1.0 2.0 -0.9567113\n",
            "\tgrad:  2.0 4.0 -1.875154\n",
            "\tgrad:  3.0 6.0 -2.700222\n",
            "Epoch: 6 | Loss: [0.17895843 0.7158337  1.6106262 ]\n",
            "\tgrad:  1.0 2.0 -0.8460696\n",
            "\tgrad:  2.0 4.0 -1.6582966\n",
            "\tgrad:  3.0 6.0 -2.387947\n",
            "Epoch: 7 | Loss: [0.13995953 0.5598381  1.2596358 ]\n",
            "\tgrad:  1.0 2.0 -0.7482233\n",
            "\tgrad:  2.0 4.0 -1.4665174\n",
            "\tgrad:  3.0 6.0 -2.111786\n",
            "Epoch: 8 | Loss: [0.10945936 0.43783745 0.9851345 ]\n",
            "\tgrad:  1.0 2.0 -0.66169286\n",
            "\tgrad:  2.0 4.0 -1.2969179\n",
            "\tgrad:  3.0 6.0 -1.8675623\n",
            "Epoch: 9 | Loss: [0.08560585 0.3424234  0.77045244]\n",
            "\tgrad:  1.0 2.0 -0.58516955\n",
            "\tgrad:  2.0 4.0 -1.1469321\n",
            "\tgrad:  3.0 6.0 -1.6515818\n",
            "Epoch: 10 | Loss: [0.06695043 0.26780173 0.60255355]\n",
            "\tgrad:  1.0 2.0 -0.51749563\n",
            "\tgrad:  2.0 4.0 -1.0142913\n",
            "\tgrad:  3.0 6.0 -1.4605789\n",
            "Epoch: 11 | Loss: [0.05236049 0.20944194 0.47124436]\n",
            "\tgrad:  1.0 2.0 -0.45764828\n",
            "\tgrad:  2.0 4.0 -0.8969908\n",
            "\tgrad:  3.0 6.0 -1.291667\n",
            "Epoch: 12 | Loss: [0.04095002 0.16380008 0.36855015]\n",
            "\tgrad:  1.0 2.0 -0.4047222\n",
            "\tgrad:  2.0 4.0 -0.7932553\n",
            "\tgrad:  3.0 6.0 -1.1422882\n",
            "Epoch: 13 | Loss: [0.03202612 0.12810446 0.28823504]\n",
            "\tgrad:  1.0 2.0 -0.35791683\n",
            "\tgrad:  2.0 4.0 -0.7015171\n",
            "\tgrad:  3.0 6.0 -1.0101843\n",
            "Epoch: 14 | Loss: [0.02504694 0.10018776 0.22542247]\n",
            "\tgrad:  1.0 2.0 -0.3165245\n",
            "\tgrad:  2.0 4.0 -0.62038803\n",
            "\tgrad:  3.0 6.0 -0.89335823\n",
            "Epoch: 15 | Loss: [0.01958868 0.07835473 0.17629834]\n",
            "\tgrad:  1.0 2.0 -0.27991915\n",
            "\tgrad:  2.0 4.0 -0.5486417\n",
            "\tgrad:  3.0 6.0 -0.7900448\n",
            "Epoch: 16 | Loss: [0.0153199  0.06127959 0.13787907]\n",
            "\tgrad:  1.0 2.0 -0.24754715\n",
            "\tgrad:  2.0 4.0 -0.4851923\n",
            "\tgrad:  3.0 6.0 -0.69867706\n",
            "Epoch: 17 | Loss: [0.01198136 0.04792544 0.10783225]\n",
            "\tgrad:  1.0 2.0 -0.2189188\n",
            "\tgrad:  2.0 4.0 -0.42908096\n",
            "\tgrad:  3.0 6.0 -0.61787605\n",
            "Epoch: 18 | Loss: [0.00937037 0.03748149 0.08433329]\n",
            "\tgrad:  1.0 2.0 -0.19360137\n",
            "\tgrad:  2.0 4.0 -0.3794589\n",
            "\tgrad:  3.0 6.0 -0.54642105\n",
            "Epoch: 19 | Loss: [0.00732838 0.02931353 0.06595539]\n",
            "prediction (after training) 4 7.657576 -> 잘못된 grad() 사용으로 인한 결과물 오류 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# torch w.grad.data.zeros_() 와 jax.grad(, argnums = (0,1)) 를 비교"
      ],
      "metadata": {
        "id": "5liyTR9e2FIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch, jax 간 같은 loss 함수를 사용함에도 불구하고 사용법에 따라 문제가 발생하는 이유\n",
        "# grad 를 계산하는 방식이 다름\n",
        "## torch\n",
        "# l = loss(y_pred, y_true)\n",
        "# l.backward()\n",
        "\n",
        "## jax\n",
        "# grad_loss=grad(loss)\n",
        "# grad_loss(y_pred, y_true)\n",
        "\n",
        "\n",
        "w_torch = torch.tensor([1.0], requires_grad = True)\n",
        "w_jax = jnp.array([1.0]) # 당연하게 requires_grad = True 는 없다\n",
        "def forward_torch(x):\n",
        "    return x * w_torch\n",
        "def forward_jax(w_jax, x):\n",
        "    return x * w_jax\n",
        "\n",
        "def loss(y_pred, y_true):\n",
        "    return (y_pred - y_true) **2\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "    return jnp.mean((forward_jax(params, x) - y) **2)\n",
        "\n",
        "\n",
        "x_data_torch = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\n",
        "y_data_torch = torch.tensor([2.0, 4.0, 6.0], requires_grad=False)\n",
        "\n",
        "x_data_jax = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data_jax = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "for x_val_t, y_val_t, x_val_j, y_val_j in zip(x_data_torch, y_data_torch, x_data_jax, y_data_jax):\n",
        "\n",
        "    # torch\n",
        "    y_pred_torch = forward_torch(x_val_t)\n",
        "    l = loss(y_pred_torch, y_val_t)\n",
        "    l.backward()\n",
        "    print(f'y_pred : {y_pred_torch.item()}, y_true:{y_val_t}')\n",
        "    print(f'torch loss: {l.item()}, w grad : {w_torch.grad.item()}') # w에 대한 gradient 였음\n",
        "    w_torch.grad.data.zero_()\n",
        "\n",
        "    # jax\n",
        "    y_pred_jax = forward_jax(w_jax, x_val_j)\n",
        "   #print(f'y_pred : {y_pred_jax[0]}, y_true:{y_val}')\n",
        "    jax_loss = loss(y_pred_jax[0], y_val_j)\n",
        "    grad_ = grad_loss(y_pred_jax[0], y_val_j) # 이런 방식으로 gradient 를 구한다면, y에 대한 gradient 가 출력됨\n",
        "    print(f'jax loss: {jax_loss}, y grad: {grad_}')\n",
        "\n",
        "    grad2 = jax.grad(loss_fn, argnums=(0, 1))(w_jax, x_val_j, y_val_j)\n",
        "    # 위와 같은 방식으로 해야만 제대로 weight 에 대한 gradient 를 계산할 수 있음\n",
        "    print(f'jax loss: {jax_loss}, param grad: {grad2[0]}')\n",
        "    print('-' * 10)"
      ],
      "metadata": {
        "id": "aGg1kVUlml2y",
        "outputId": "ebadc42b-1245-4120-8b0f-c3b5278cd26e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(7.657576, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수정 후\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "w = jnp.array([1.0]) # 당연하게 requires_grad = True 는 없다\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "def loss(y_pred, y_val):\n",
        "  return (y_pred - y_val)**2\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "print('prediction (before training)', 4, forward(4)[0])\n",
        "\n",
        "for epoch in range(20):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    # y_pred = forward(x_val) # 1) forward pass\n",
        "    #l = loss(y_pred, y_val) # 2) compute loss\n",
        "    #l.backward() # 3) backpropagation to update weights\n",
        "    #print('\\t grad:', x_val, y_val, w.grad.item())\n",
        "    y_pred = forward(x_val)\n",
        "    grad_w = jax.grad(loss_fn, argnums=(0, 1))(w, x_val, y_val)\n",
        "    w -= learning_rate * grad_w[0]\n",
        "    print('\\tgrad: ', x_val, y_val, grad_w[0])\n",
        "\n",
        "  print(f'Epoch: {epoch} | Loss: {loss(forward(x_data), y_data)}')\n",
        "\n",
        "print('prediction (after training)', 4, forward(4)[0], '->제대로 학습한 결과물 ')"
      ],
      "metadata": {
        "id": "wKoR3NMTmnRz",
        "outputId": "ef88176d-14bc-49c9-d6a0-332d3b7e3771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 [-2.]\n",
            "\tgrad:  2.0 4.0 [-7.84]\n",
            "\tgrad:  3.0 6.0 [-16.228802]\n",
            "Epoch: 0 | Loss: [0.54658216 2.1863286  4.919239  ]\n",
            "\tgrad:  1.0 2.0 [-1.4786239]\n",
            "\tgrad:  2.0 4.0 [-5.7962055]\n",
            "\tgrad:  3.0 6.0 [-11.998146]\n",
            "Epoch: 1 | Loss: [0.29875213 1.1950085  2.688769  ]\n",
            "\tgrad:  1.0 2.0 [-1.0931644]\n",
            "\tgrad:  2.0 4.0 [-4.285205]\n",
            "\tgrad:  3.0 6.0 [-8.870373]\n",
            "Epoch: 2 | Loss: [0.16329262 0.65317047 1.4696338 ]\n",
            "\tgrad:  1.0 2.0 [-0.80818963]\n",
            "\tgrad:  2.0 4.0 [-3.1681032]\n",
            "\tgrad:  3.0 6.0 [-6.557974]\n",
            "Epoch: 3 | Loss: [0.0892528  0.3570112  0.80327564]\n",
            "\tgrad:  1.0 2.0 [-0.59750414]\n",
            "\tgrad:  2.0 4.0 [-2.3422165]\n",
            "\tgrad:  3.0 6.0 [-4.8483896]\n",
            "Epoch: 4 | Loss: [0.04878404 0.19513616 0.43905652]\n",
            "\tgrad:  1.0 2.0 [-0.44174218]\n",
            "\tgrad:  2.0 4.0 [-1.7316294]\n",
            "\tgrad:  3.0 6.0 [-3.5844727]\n",
            "Epoch: 5 | Loss: [0.02666449 0.10665795 0.23998016]\n",
            "\tgrad:  1.0 2.0 [-0.3265853]\n",
            "\tgrad:  2.0 4.0 [-1.2802143]\n",
            "\tgrad:  3.0 6.0 [-2.6500454]\n",
            "Epoch: 6 | Loss: [0.01457433 0.05829733 0.13116899]\n",
            "\tgrad:  1.0 2.0 [-0.2414484]\n",
            "\tgrad:  2.0 4.0 [-0.9464779]\n",
            "\tgrad:  3.0 6.0 [-1.9592113]\n",
            "Epoch: 7 | Loss: [0.00796607 0.03186427 0.07169455]\n",
            "\tgrad:  1.0 2.0 [-0.17850566]\n",
            "\tgrad:  2.0 4.0 [-0.6997423]\n",
            "\tgrad:  3.0 6.0 [-1.4484673]\n",
            "Epoch: 8 | Loss: [0.00435411 0.01741644 0.03918699]\n",
            "\tgrad:  1.0 2.0 [-0.13197136]\n",
            "\tgrad:  2.0 4.0 [-0.5173273]\n",
            "\tgrad:  3.0 6.0 [-1.0708666]\n",
            "Epoch: 9 | Loss: [0.00237987 0.00951947 0.02141885]\n",
            "\tgrad:  1.0 2.0 [-0.0975678]\n",
            "\tgrad:  2.0 4.0 [-0.38246536]\n",
            "\tgrad:  3.0 6.0 [-0.7917023]\n",
            "Epoch: 10 | Loss: [0.00130079 0.00520314 0.01170705]\n",
            "\tgrad:  1.0 2.0 [-0.07213283]\n",
            "\tgrad:  2.0 4.0 [-0.28276062]\n",
            "\tgrad:  3.0 6.0 [-0.5853138]\n",
            "Epoch: 11 | Loss: [0.00071098 0.00284393 0.00639884]\n",
            "\tgrad:  1.0 2.0 [-0.05332851]\n",
            "\tgrad:  2.0 4.0 [-0.20904732]\n",
            "\tgrad:  3.0 6.0 [-0.43272972]\n",
            "Epoch: 12 | Loss: [0.00038861 0.00155444 0.00349745]\n",
            "\tgrad:  1.0 2.0 [-0.03942633]\n",
            "\tgrad:  2.0 4.0 [-0.1545515]\n",
            "\tgrad:  3.0 6.0 [-0.3199196]\n",
            "Epoch: 13 | Loss: [0.00021241 0.00084963 0.00191167]\n",
            "\tgrad:  1.0 2.0 [-0.02914834]\n",
            "\tgrad:  2.0 4.0 [-0.11426163]\n",
            "\tgrad:  3.0 6.0 [-0.23652077]\n",
            "Epoch: 14 | Loss: [0.0001161  0.00046439 0.00104489]\n",
            "\tgrad:  1.0 2.0 [-0.0215497]\n",
            "\tgrad:  2.0 4.0 [-0.08447456]\n",
            "\tgrad:  3.0 6.0 [-0.17486286]\n",
            "Epoch: 15 | Loss: [6.3455918e-05 2.5382367e-04 5.7109760e-04]\n",
            "\tgrad:  1.0 2.0 [-0.01593184]\n",
            "\tgrad:  2.0 4.0 [-0.06245327]\n",
            "\tgrad:  3.0 6.0 [-0.12927818]\n",
            "Epoch: 16 | Loss: [3.4683813e-05 1.3873525e-04 3.1215011e-04]\n",
            "\tgrad:  1.0 2.0 [-0.01177859]\n",
            "\tgrad:  2.0 4.0 [-0.04617214]\n",
            "\tgrad:  3.0 6.0 [-0.09557533]\n",
            "Epoch: 17 | Loss: [1.8958355e-05 7.5833421e-05 1.7062831e-04]\n",
            "\tgrad:  1.0 2.0 [-0.00870824]\n",
            "\tgrad:  2.0 4.0 [-0.03413582]\n",
            "\tgrad:  3.0 6.0 [-0.07066154]\n",
            "Epoch: 18 | Loss: [1.0361248e-05 4.1444993e-05 9.3255832e-05]\n",
            "\tgrad:  1.0 2.0 [-0.00643778]\n",
            "\tgrad:  2.0 4.0 [-0.02523613]\n",
            "\tgrad:  3.0 6.0 [-0.05223942]\n",
            "Epoch: 19 | Loss: [5.6633294e-06 2.2653318e-05 5.0968261e-05]\n",
            "prediction (after training) 4 7.990481 ->제대로 학습한 결과물 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05_linear_regression.py"
      ],
      "metadata": {
        "id": "hyGPMymMsn74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear module\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1) # one in and one out\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and\n",
        "    we must return a Variable of output data.\n",
        "    We can use Modules defined in the constructor as well as arbitary operators\n",
        "    on Variables.\n",
        "    \"\"\"\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "5w1GKtijsTJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEzueZR5s7Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "\n",
        "# loss function & optimizer\n",
        "criterion = torch.nn.MSELoss(reduction = 'sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "yVbyU1NBtmtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# traning loop\n",
        "for epoch in range(500):\n",
        "  # forward pass : compute prediction y by passing x to the model\n",
        "  y_pred = model(x_data)\n",
        "\n",
        "  # compute and print loss\n",
        "  loss = criterion(y_pred, y_data)\n",
        "  print(f'Epoch: {epoch} | Loss: {loss.item()}')\n",
        "\n",
        "  # zero gradients, backward pass, update the weights\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# after training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print('prediction (after training)', 4, model(hour_var).data[0][0].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4BH4Ro7tobL",
        "outputId": "a7ccbe2f-a526-4901-f61d-a3804917c4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 23.98592185974121\n",
            "Epoch: 1 | Loss: 11.040203094482422\n",
            "Epoch: 2 | Loss: 5.271928787231445\n",
            "Epoch: 3 | Loss: 2.6989245414733887\n",
            "Epoch: 4 | Loss: 1.5484364032745361\n",
            "Epoch: 5 | Loss: 1.0312860012054443\n",
            "Epoch: 6 | Loss: 0.796151340007782\n",
            "Epoch: 7 | Loss: 0.6866306066513062\n",
            "Epoch: 8 | Loss: 0.6331018209457397\n",
            "Epoch: 9 | Loss: 0.6045660972595215\n",
            "Epoch: 10 | Loss: 0.587225079536438\n",
            "Epoch: 11 | Loss: 0.5749333500862122\n",
            "Epoch: 12 | Loss: 0.5649558901786804\n",
            "Epoch: 13 | Loss: 0.5560733675956726\n",
            "Epoch: 14 | Loss: 0.5477418899536133\n",
            "Epoch: 15 | Loss: 0.5397184491157532\n",
            "Epoch: 16 | Loss: 0.5318945646286011\n",
            "Epoch: 17 | Loss: 0.5242207646369934\n",
            "Epoch: 18 | Loss: 0.5166733860969543\n",
            "Epoch: 19 | Loss: 0.5092422366142273\n",
            "Epoch: 20 | Loss: 0.5019209980964661\n",
            "Epoch: 21 | Loss: 0.49470603466033936\n",
            "Epoch: 22 | Loss: 0.48759573698043823\n",
            "Epoch: 23 | Loss: 0.48058784008026123\n",
            "Epoch: 24 | Loss: 0.4736814498901367\n",
            "Epoch: 25 | Loss: 0.4668741226196289\n",
            "Epoch: 26 | Loss: 0.4601641595363617\n",
            "Epoch: 27 | Loss: 0.45355087518692017\n",
            "Epoch: 28 | Loss: 0.4470323324203491\n",
            "Epoch: 29 | Loss: 0.44060802459716797\n",
            "Epoch: 30 | Loss: 0.4342755675315857\n",
            "Epoch: 31 | Loss: 0.42803439497947693\n",
            "Epoch: 32 | Loss: 0.42188286781311035\n",
            "Epoch: 33 | Loss: 0.41581958532333374\n",
            "Epoch: 34 | Loss: 0.4098435342311859\n",
            "Epoch: 35 | Loss: 0.4039537310600281\n",
            "Epoch: 36 | Loss: 0.3981482684612274\n",
            "Epoch: 37 | Loss: 0.39242616295814514\n",
            "Epoch: 38 | Loss: 0.3867865800857544\n",
            "Epoch: 39 | Loss: 0.38122737407684326\n",
            "Epoch: 40 | Loss: 0.3757486343383789\n",
            "Epoch: 41 | Loss: 0.37034866213798523\n",
            "Epoch: 42 | Loss: 0.3650261163711548\n",
            "Epoch: 43 | Loss: 0.3597802221775055\n",
            "Epoch: 44 | Loss: 0.35460975766181946\n",
            "Epoch: 45 | Loss: 0.349513441324234\n",
            "Epoch: 46 | Loss: 0.3444899618625641\n",
            "Epoch: 47 | Loss: 0.3395392894744873\n",
            "Epoch: 48 | Loss: 0.3346596658229828\n",
            "Epoch: 49 | Loss: 0.32985004782676697\n",
            "Epoch: 50 | Loss: 0.325109601020813\n",
            "Epoch: 51 | Loss: 0.3204370141029358\n",
            "Epoch: 52 | Loss: 0.31583234667778015\n",
            "Epoch: 53 | Loss: 0.3112930655479431\n",
            "Epoch: 54 | Loss: 0.3068193793296814\n",
            "Epoch: 55 | Loss: 0.3024097979068756\n",
            "Epoch: 56 | Loss: 0.2980636954307556\n",
            "Epoch: 57 | Loss: 0.29378020763397217\n",
            "Epoch: 58 | Loss: 0.2895580530166626\n",
            "Epoch: 59 | Loss: 0.2853963375091553\n",
            "Epoch: 60 | Loss: 0.2812947928905487\n",
            "Epoch: 61 | Loss: 0.2772524058818817\n",
            "Epoch: 62 | Loss: 0.2732677161693573\n",
            "Epoch: 63 | Loss: 0.2693404257297516\n",
            "Epoch: 64 | Loss: 0.26546961069107056\n",
            "Epoch: 65 | Loss: 0.26165440678596497\n",
            "Epoch: 66 | Loss: 0.25789394974708557\n",
            "Epoch: 67 | Loss: 0.25418752431869507\n",
            "Epoch: 68 | Loss: 0.2505345940589905\n",
            "Epoch: 69 | Loss: 0.246934175491333\n",
            "Epoch: 70 | Loss: 0.24338507652282715\n",
            "Epoch: 71 | Loss: 0.23988723754882812\n",
            "Epoch: 72 | Loss: 0.23643989861011505\n",
            "Epoch: 73 | Loss: 0.23304179310798645\n",
            "Epoch: 74 | Loss: 0.22969262301921844\n",
            "Epoch: 75 | Loss: 0.2263915240764618\n",
            "Epoch: 76 | Loss: 0.22313787043094635\n",
            "Epoch: 77 | Loss: 0.21993118524551392\n",
            "Epoch: 78 | Loss: 0.2167704701423645\n",
            "Epoch: 79 | Loss: 0.2136549949645996\n",
            "Epoch: 80 | Loss: 0.21058450639247894\n",
            "Epoch: 81 | Loss: 0.20755793154239655\n",
            "Epoch: 82 | Loss: 0.2045753002166748\n",
            "Epoch: 83 | Loss: 0.20163494348526\n",
            "Epoch: 84 | Loss: 0.1987370252609253\n",
            "Epoch: 85 | Loss: 0.1958811730146408\n",
            "Epoch: 86 | Loss: 0.19306588172912598\n",
            "Epoch: 87 | Loss: 0.19029119610786438\n",
            "Epoch: 88 | Loss: 0.18755638599395752\n",
            "Epoch: 89 | Loss: 0.1848609745502472\n",
            "Epoch: 90 | Loss: 0.1822042316198349\n",
            "Epoch: 91 | Loss: 0.17958565056324005\n",
            "Epoch: 92 | Loss: 0.1770048588514328\n",
            "Epoch: 93 | Loss: 0.17446084320545197\n",
            "Epoch: 94 | Loss: 0.17195361852645874\n",
            "Epoch: 95 | Loss: 0.16948246955871582\n",
            "Epoch: 96 | Loss: 0.1670466512441635\n",
            "Epoch: 97 | Loss: 0.16464582085609436\n",
            "Epoch: 98 | Loss: 0.16227978467941284\n",
            "Epoch: 99 | Loss: 0.1599474549293518\n",
            "Epoch: 100 | Loss: 0.15764881670475006\n",
            "Epoch: 101 | Loss: 0.15538325905799866\n",
            "Epoch: 102 | Loss: 0.1531500667333603\n",
            "Epoch: 103 | Loss: 0.15094903111457825\n",
            "Epoch: 104 | Loss: 0.14877952635288239\n",
            "Epoch: 105 | Loss: 0.14664140343666077\n",
            "Epoch: 106 | Loss: 0.1445339322090149\n",
            "Epoch: 107 | Loss: 0.14245682954788208\n",
            "Epoch: 108 | Loss: 0.14040938019752502\n",
            "Epoch: 109 | Loss: 0.1383916437625885\n",
            "Epoch: 110 | Loss: 0.13640262186527252\n",
            "Epoch: 111 | Loss: 0.1344422996044159\n",
            "Epoch: 112 | Loss: 0.1325100064277649\n",
            "Epoch: 113 | Loss: 0.1306057721376419\n",
            "Epoch: 114 | Loss: 0.12872876226902008\n",
            "Epoch: 115 | Loss: 0.126878559589386\n",
            "Epoch: 116 | Loss: 0.12505538761615753\n",
            "Epoch: 117 | Loss: 0.12325797975063324\n",
            "Epoch: 118 | Loss: 0.12148678302764893\n",
            "Epoch: 119 | Loss: 0.11974048614501953\n",
            "Epoch: 120 | Loss: 0.11801991611719131\n",
            "Epoch: 121 | Loss: 0.1163238137960434\n",
            "Epoch: 122 | Loss: 0.11465199291706085\n",
            "Epoch: 123 | Loss: 0.113004170358181\n",
            "Epoch: 124 | Loss: 0.11138017475605011\n",
            "Epoch: 125 | Loss: 0.10977927595376968\n",
            "Epoch: 126 | Loss: 0.10820183902978897\n",
            "Epoch: 127 | Loss: 0.10664672404527664\n",
            "Epoch: 128 | Loss: 0.10511399805545807\n",
            "Epoch: 129 | Loss: 0.10360344499349594\n",
            "Epoch: 130 | Loss: 0.10211433470249176\n",
            "Epoch: 131 | Loss: 0.10064680874347687\n",
            "Epoch: 132 | Loss: 0.09920036792755127\n",
            "Epoch: 133 | Loss: 0.09777489304542542\n",
            "Epoch: 134 | Loss: 0.09636962413787842\n",
            "Epoch: 135 | Loss: 0.0949845016002655\n",
            "Epoch: 136 | Loss: 0.09361957013607025\n",
            "Epoch: 137 | Loss: 0.09227411448955536\n",
            "Epoch: 138 | Loss: 0.0909479409456253\n",
            "Epoch: 139 | Loss: 0.0896407961845398\n",
            "Epoch: 140 | Loss: 0.08835262060165405\n",
            "Epoch: 141 | Loss: 0.0870828852057457\n",
            "Epoch: 142 | Loss: 0.08583122491836548\n",
            "Epoch: 143 | Loss: 0.08459785580635071\n",
            "Epoch: 144 | Loss: 0.08338195830583572\n",
            "Epoch: 145 | Loss: 0.08218372613191605\n",
            "Epoch: 146 | Loss: 0.08100269734859467\n",
            "Epoch: 147 | Loss: 0.07983844727277756\n",
            "Epoch: 148 | Loss: 0.0786910206079483\n",
            "Epoch: 149 | Loss: 0.07756008207798004\n",
            "Epoch: 150 | Loss: 0.07644544541835785\n",
            "Epoch: 151 | Loss: 0.07534673064947128\n",
            "Epoch: 152 | Loss: 0.07426400482654572\n",
            "Epoch: 153 | Loss: 0.07319651544094086\n",
            "Epoch: 154 | Loss: 0.07214469462633133\n",
            "Epoch: 155 | Loss: 0.07110776007175446\n",
            "Epoch: 156 | Loss: 0.07008598744869232\n",
            "Epoch: 157 | Loss: 0.06907875090837479\n",
            "Epoch: 158 | Loss: 0.06808581203222275\n",
            "Epoch: 159 | Loss: 0.06710731983184814\n",
            "Epoch: 160 | Loss: 0.06614284217357635\n",
            "Epoch: 161 | Loss: 0.06519242376089096\n",
            "Epoch: 162 | Loss: 0.06425550580024719\n",
            "Epoch: 163 | Loss: 0.06333200633525848\n",
            "Epoch: 164 | Loss: 0.06242184340953827\n",
            "Epoch: 165 | Loss: 0.06152470037341118\n",
            "Epoch: 166 | Loss: 0.060640498995780945\n",
            "Epoch: 167 | Loss: 0.05976897478103638\n",
            "Epoch: 168 | Loss: 0.058910056948661804\n",
            "Epoch: 169 | Loss: 0.05806342139840126\n",
            "Epoch: 170 | Loss: 0.057228878140449524\n",
            "Epoch: 171 | Loss: 0.05640650913119316\n",
            "Epoch: 172 | Loss: 0.055595770478248596\n",
            "Epoch: 173 | Loss: 0.05479680374264717\n",
            "Epoch: 174 | Loss: 0.05400935187935829\n",
            "Epoch: 175 | Loss: 0.05323316529393196\n",
            "Epoch: 176 | Loss: 0.05246824026107788\n",
            "Epoch: 177 | Loss: 0.051713958382606506\n",
            "Epoch: 178 | Loss: 0.05097082257270813\n",
            "Epoch: 179 | Loss: 0.05023825913667679\n",
            "Epoch: 180 | Loss: 0.04951632395386696\n",
            "Epoch: 181 | Loss: 0.04880465567111969\n",
            "Epoch: 182 | Loss: 0.04810333251953125\n",
            "Epoch: 183 | Loss: 0.047411996871232986\n",
            "Epoch: 184 | Loss: 0.04673061892390251\n",
            "Epoch: 185 | Loss: 0.04605903476476669\n",
            "Epoch: 186 | Loss: 0.04539700970053673\n",
            "Epoch: 187 | Loss: 0.044744595885276794\n",
            "Epoch: 188 | Loss: 0.04410150647163391\n",
            "Epoch: 189 | Loss: 0.043467819690704346\n",
            "Epoch: 190 | Loss: 0.0428430438041687\n",
            "Epoch: 191 | Loss: 0.04222732037305832\n",
            "Epoch: 192 | Loss: 0.041620515286922455\n",
            "Epoch: 193 | Loss: 0.041022397577762604\n",
            "Epoch: 194 | Loss: 0.040432773530483246\n",
            "Epoch: 195 | Loss: 0.039851702749729156\n",
            "Epoch: 196 | Loss: 0.03927887976169586\n",
            "Epoch: 197 | Loss: 0.0387144461274147\n",
            "Epoch: 198 | Loss: 0.038158122450113297\n",
            "Epoch: 199 | Loss: 0.037609606981277466\n",
            "Epoch: 200 | Loss: 0.037069179117679596\n",
            "Epoch: 201 | Loss: 0.03653636947274208\n",
            "Epoch: 202 | Loss: 0.03601137921214104\n",
            "Epoch: 203 | Loss: 0.03549373894929886\n",
            "Epoch: 204 | Loss: 0.03498363494873047\n",
            "Epoch: 205 | Loss: 0.03448095545172691\n",
            "Epoch: 206 | Loss: 0.033985331654548645\n",
            "Epoch: 207 | Loss: 0.03349696099758148\n",
            "Epoch: 208 | Loss: 0.033015523105859756\n",
            "Epoch: 209 | Loss: 0.03254109248518944\n",
            "Epoch: 210 | Loss: 0.03207331523299217\n",
            "Epoch: 211 | Loss: 0.03161243349313736\n",
            "Epoch: 212 | Loss: 0.031158145517110825\n",
            "Epoch: 213 | Loss: 0.030710384249687195\n",
            "Epoch: 214 | Loss: 0.030269000679254532\n",
            "Epoch: 215 | Loss: 0.029834022745490074\n",
            "Epoch: 216 | Loss: 0.02940523624420166\n",
            "Epoch: 217 | Loss: 0.02898259088397026\n",
            "Epoch: 218 | Loss: 0.028566014021635056\n",
            "Epoch: 219 | Loss: 0.028155535459518433\n",
            "Epoch: 220 | Loss: 0.027750903740525246\n",
            "Epoch: 221 | Loss: 0.027352064847946167\n",
            "Epoch: 222 | Loss: 0.02695898897945881\n",
            "Epoch: 223 | Loss: 0.02657151408493519\n",
            "Epoch: 224 | Loss: 0.026189666241407394\n",
            "Epoch: 225 | Loss: 0.025813285261392593\n",
            "Epoch: 226 | Loss: 0.025442268699407578\n",
            "Epoch: 227 | Loss: 0.025076618418097496\n",
            "Epoch: 228 | Loss: 0.0247162114828825\n",
            "Epoch: 229 | Loss: 0.024361060932278633\n",
            "Epoch: 230 | Loss: 0.024010978639125824\n",
            "Epoch: 231 | Loss: 0.023665808141231537\n",
            "Epoch: 232 | Loss: 0.023325756192207336\n",
            "Epoch: 233 | Loss: 0.022990459576249123\n",
            "Epoch: 234 | Loss: 0.02266007289290428\n",
            "Epoch: 235 | Loss: 0.022334495559334755\n",
            "Epoch: 236 | Loss: 0.022013429552316666\n",
            "Epoch: 237 | Loss: 0.021697085350751877\n",
            "Epoch: 238 | Loss: 0.021385250613093376\n",
            "Epoch: 239 | Loss: 0.021077943965792656\n",
            "Epoch: 240 | Loss: 0.020774973556399345\n",
            "Epoch: 241 | Loss: 0.020476458594202995\n",
            "Epoch: 242 | Loss: 0.02018211968243122\n",
            "Epoch: 243 | Loss: 0.01989208534359932\n",
            "Epoch: 244 | Loss: 0.01960625872015953\n",
            "Epoch: 245 | Loss: 0.019324416294693947\n",
            "Epoch: 246 | Loss: 0.019046757370233536\n",
            "Epoch: 247 | Loss: 0.0187730323523283\n",
            "Epoch: 248 | Loss: 0.018503213301301003\n",
            "Epoch: 249 | Loss: 0.018237262964248657\n",
            "Epoch: 250 | Loss: 0.01797514781355858\n",
            "Epoch: 251 | Loss: 0.017716864123940468\n",
            "Epoch: 252 | Loss: 0.01746220886707306\n",
            "Epoch: 253 | Loss: 0.01721123605966568\n",
            "Epoch: 254 | Loss: 0.01696389727294445\n",
            "Epoch: 255 | Loss: 0.016720116138458252\n",
            "Epoch: 256 | Loss: 0.016479793936014175\n",
            "Epoch: 257 | Loss: 0.016242995858192444\n",
            "Epoch: 258 | Loss: 0.016009530052542686\n",
            "Epoch: 259 | Loss: 0.015779471024870872\n",
            "Epoch: 260 | Loss: 0.015552693977952003\n",
            "Epoch: 261 | Loss: 0.015329164452850819\n",
            "Epoch: 262 | Loss: 0.015108916908502579\n",
            "Epoch: 263 | Loss: 0.01489170454442501\n",
            "Epoch: 264 | Loss: 0.014677727594971657\n",
            "Epoch: 265 | Loss: 0.014466779306530952\n",
            "Epoch: 266 | Loss: 0.014258846640586853\n",
            "Epoch: 267 | Loss: 0.014053968712687492\n",
            "Epoch: 268 | Loss: 0.013851970434188843\n",
            "Epoch: 269 | Loss: 0.013652879744768143\n",
            "Epoch: 270 | Loss: 0.013456720858812332\n",
            "Epoch: 271 | Loss: 0.01326325535774231\n",
            "Epoch: 272 | Loss: 0.013072645291686058\n",
            "Epoch: 273 | Loss: 0.012884794734418392\n",
            "Epoch: 274 | Loss: 0.01269963476806879\n",
            "Epoch: 275 | Loss: 0.012517075054347515\n",
            "Epoch: 276 | Loss: 0.012337214313447475\n",
            "Epoch: 277 | Loss: 0.01215988490730524\n",
            "Epoch: 278 | Loss: 0.011985150165855885\n",
            "Epoch: 279 | Loss: 0.01181291975080967\n",
            "Epoch: 280 | Loss: 0.01164313405752182\n",
            "Epoch: 281 | Loss: 0.011475816369056702\n",
            "Epoch: 282 | Loss: 0.011310847476124763\n",
            "Epoch: 283 | Loss: 0.011148306541144848\n",
            "Epoch: 284 | Loss: 0.01098813209682703\n",
            "Epoch: 285 | Loss: 0.010830162093043327\n",
            "Epoch: 286 | Loss: 0.010674508288502693\n",
            "Epoch: 287 | Loss: 0.010521089658141136\n",
            "Epoch: 288 | Loss: 0.010369918309152126\n",
            "Epoch: 289 | Loss: 0.010220911353826523\n",
            "Epoch: 290 | Loss: 0.010074026882648468\n",
            "Epoch: 291 | Loss: 0.009929241612553596\n",
            "Epoch: 292 | Loss: 0.009786535054445267\n",
            "Epoch: 293 | Loss: 0.009645872749388218\n",
            "Epoch: 294 | Loss: 0.00950722023844719\n",
            "Epoch: 295 | Loss: 0.009370624087750912\n",
            "Epoch: 296 | Loss: 0.009235932491719723\n",
            "Epoch: 297 | Loss: 0.009103198535740376\n",
            "Epoch: 298 | Loss: 0.008972426876425743\n",
            "Epoch: 299 | Loss: 0.008843416348099709\n",
            "Epoch: 300 | Loss: 0.008716349489986897\n",
            "Epoch: 301 | Loss: 0.008591048419475555\n",
            "Epoch: 302 | Loss: 0.008467612788081169\n",
            "Epoch: 303 | Loss: 0.008345909416675568\n",
            "Epoch: 304 | Loss: 0.008225951343774796\n",
            "Epoch: 305 | Loss: 0.008107767440378666\n",
            "Epoch: 306 | Loss: 0.007991203106939793\n",
            "Epoch: 307 | Loss: 0.007876411080360413\n",
            "Epoch: 308 | Loss: 0.00776319345459342\n",
            "Epoch: 309 | Loss: 0.007651623338460922\n",
            "Epoch: 310 | Loss: 0.007541689556092024\n",
            "Epoch: 311 | Loss: 0.007433292921632528\n",
            "Epoch: 312 | Loss: 0.007326466962695122\n",
            "Epoch: 313 | Loss: 0.007221141364425421\n",
            "Epoch: 314 | Loss: 0.007117402274161577\n",
            "Epoch: 315 | Loss: 0.007015055976808071\n",
            "Epoch: 316 | Loss: 0.006914287805557251\n",
            "Epoch: 317 | Loss: 0.006814900785684586\n",
            "Epoch: 318 | Loss: 0.006716934498399496\n",
            "Epoch: 319 | Loss: 0.006620424333959818\n",
            "Epoch: 320 | Loss: 0.006525289732962847\n",
            "Epoch: 321 | Loss: 0.006431490648537874\n",
            "Epoch: 322 | Loss: 0.00633907038718462\n",
            "Epoch: 323 | Loss: 0.006247962359338999\n",
            "Epoch: 324 | Loss: 0.006158170755952597\n",
            "Epoch: 325 | Loss: 0.006069663446396589\n",
            "Epoch: 326 | Loss: 0.005982448812574148\n",
            "Epoch: 327 | Loss: 0.00589648587629199\n",
            "Epoch: 328 | Loss: 0.005811708047986031\n",
            "Epoch: 329 | Loss: 0.005728212650865316\n",
            "Epoch: 330 | Loss: 0.005645867437124252\n",
            "Epoch: 331 | Loss: 0.005564738996326923\n",
            "Epoch: 332 | Loss: 0.005484774708747864\n",
            "Epoch: 333 | Loss: 0.0054059321992099285\n",
            "Epoch: 334 | Loss: 0.005328207742422819\n",
            "Epoch: 335 | Loss: 0.005251660943031311\n",
            "Epoch: 336 | Loss: 0.005176216829568148\n",
            "Epoch: 337 | Loss: 0.005101799499243498\n",
            "Epoch: 338 | Loss: 0.005028477869927883\n",
            "Epoch: 339 | Loss: 0.0049562156200408936\n",
            "Epoch: 340 | Loss: 0.0048849876038730145\n",
            "Epoch: 341 | Loss: 0.004814784042537212\n",
            "Epoch: 342 | Loss: 0.004745604936033487\n",
            "Epoch: 343 | Loss: 0.004677367862313986\n",
            "Epoch: 344 | Loss: 0.004610181786119938\n",
            "Epoch: 345 | Loss: 0.004543893039226532\n",
            "Epoch: 346 | Loss: 0.004478613380342722\n",
            "Epoch: 347 | Loss: 0.004414208233356476\n",
            "Epoch: 348 | Loss: 0.004350775387138128\n",
            "Epoch: 349 | Loss: 0.0042882743291556835\n",
            "Epoch: 350 | Loss: 0.0042266445234417915\n",
            "Epoch: 351 | Loss: 0.004165896214544773\n",
            "Epoch: 352 | Loss: 0.004106012638658285\n",
            "Epoch: 353 | Loss: 0.004047031980007887\n",
            "Epoch: 354 | Loss: 0.003988838288933039\n",
            "Epoch: 355 | Loss: 0.003931528888642788\n",
            "Epoch: 356 | Loss: 0.0038750467356294394\n",
            "Epoch: 357 | Loss: 0.0038193364161998034\n",
            "Epoch: 358 | Loss: 0.003764464519917965\n",
            "Epoch: 359 | Loss: 0.0037103542126715183\n",
            "Epoch: 360 | Loss: 0.00365702947601676\n",
            "Epoch: 361 | Loss: 0.0036044821608811617\n",
            "Epoch: 362 | Loss: 0.0035526822321116924\n",
            "Epoch: 363 | Loss: 0.0035016185138374567\n",
            "Epoch: 364 | Loss: 0.003451275872066617\n",
            "Epoch: 365 | Loss: 0.0034016878344118595\n",
            "Epoch: 366 | Loss: 0.0033527992200106382\n",
            "Epoch: 367 | Loss: 0.0033045881427824497\n",
            "Epoch: 368 | Loss: 0.003257104894146323\n",
            "Epoch: 369 | Loss: 0.003210310824215412\n",
            "Epoch: 370 | Loss: 0.003164146561175585\n",
            "Epoch: 371 | Loss: 0.00311870314180851\n",
            "Epoch: 372 | Loss: 0.0030738781206309795\n",
            "Epoch: 373 | Loss: 0.003029711078852415\n",
            "Epoch: 374 | Loss: 0.00298617803491652\n",
            "Epoch: 375 | Loss: 0.0029432561714202166\n",
            "Epoch: 376 | Loss: 0.0029009333811700344\n",
            "Epoch: 377 | Loss: 0.002859243657439947\n",
            "Epoch: 378 | Loss: 0.002818179549649358\n",
            "Epoch: 379 | Loss: 0.002777660731226206\n",
            "Epoch: 380 | Loss: 0.002737754723057151\n",
            "Epoch: 381 | Loss: 0.0026983823627233505\n",
            "Epoch: 382 | Loss: 0.00265961280092597\n",
            "Epoch: 383 | Loss: 0.002621385734528303\n",
            "Epoch: 384 | Loss: 0.00258372793905437\n",
            "Epoch: 385 | Loss: 0.0025465802755206823\n",
            "Epoch: 386 | Loss: 0.0025099841877818108\n",
            "Epoch: 387 | Loss: 0.0024739133659750223\n",
            "Epoch: 388 | Loss: 0.002438367810100317\n",
            "Epoch: 389 | Loss: 0.0024033072404563427\n",
            "Epoch: 390 | Loss: 0.002368772402405739\n",
            "Epoch: 391 | Loss: 0.0023347458336502314\n",
            "Epoch: 392 | Loss: 0.002301191445440054\n",
            "Epoch: 393 | Loss: 0.00226810434833169\n",
            "Epoch: 394 | Loss: 0.00223550689406693\n",
            "Epoch: 395 | Loss: 0.002203390933573246\n",
            "Epoch: 396 | Loss: 0.0021717322524636984\n",
            "Epoch: 397 | Loss: 0.0021404994186013937\n",
            "Epoch: 398 | Loss: 0.0021097566932439804\n",
            "Epoch: 399 | Loss: 0.0020794328302145004\n",
            "Epoch: 400 | Loss: 0.002049539005383849\n",
            "Epoch: 401 | Loss: 0.002020081039518118\n",
            "Epoch: 402 | Loss: 0.0019910673145204782\n",
            "Epoch: 403 | Loss: 0.0019624438136816025\n",
            "Epoch: 404 | Loss: 0.001934249885380268\n",
            "Epoch: 405 | Loss: 0.001906450605019927\n",
            "Epoch: 406 | Loss: 0.0018790590111166239\n",
            "Epoch: 407 | Loss: 0.001852035871706903\n",
            "Epoch: 408 | Loss: 0.0018254319438710809\n",
            "Epoch: 409 | Loss: 0.0017991979839280248\n",
            "Epoch: 410 | Loss: 0.0017733280546963215\n",
            "Epoch: 411 | Loss: 0.0017478371737524867\n",
            "Epoch: 412 | Loss: 0.0017227143980562687\n",
            "Epoch: 413 | Loss: 0.0016979530919343233\n",
            "Epoch: 414 | Loss: 0.0016735686222091317\n",
            "Epoch: 415 | Loss: 0.0016495019663125277\n",
            "Epoch: 416 | Loss: 0.0016258162213489413\n",
            "Epoch: 417 | Loss: 0.0016024389769881964\n",
            "Epoch: 418 | Loss: 0.0015794065548107028\n",
            "Epoch: 419 | Loss: 0.0015567129012197256\n",
            "Epoch: 420 | Loss: 0.001534338342025876\n",
            "Epoch: 421 | Loss: 0.001512272865511477\n",
            "Epoch: 422 | Loss: 0.0014905601274222136\n",
            "Epoch: 423 | Loss: 0.0014691315591335297\n",
            "Epoch: 424 | Loss: 0.0014480085810646415\n",
            "Epoch: 425 | Loss: 0.0014272163389250636\n",
            "Epoch: 426 | Loss: 0.001406703144311905\n",
            "Epoch: 427 | Loss: 0.0013864922802895308\n",
            "Epoch: 428 | Loss: 0.0013665629085153341\n",
            "Epoch: 429 | Loss: 0.001346912351436913\n",
            "Epoch: 430 | Loss: 0.0013275418896228075\n",
            "Epoch: 431 | Loss: 0.00130847143009305\n",
            "Epoch: 432 | Loss: 0.001289664302021265\n",
            "Epoch: 433 | Loss: 0.0012711507733911276\n",
            "Epoch: 434 | Loss: 0.001252874149940908\n",
            "Epoch: 435 | Loss: 0.0012348482850939035\n",
            "Epoch: 436 | Loss: 0.0012171112466603518\n",
            "Epoch: 437 | Loss: 0.0011996254324913025\n",
            "Epoch: 438 | Loss: 0.0011823959648609161\n",
            "Epoch: 439 | Loss: 0.0011654063127934933\n",
            "Epoch: 440 | Loss: 0.0011486499570310116\n",
            "Epoch: 441 | Loss: 0.0011321421479806304\n",
            "Epoch: 442 | Loss: 0.0011158722918480635\n",
            "Epoch: 443 | Loss: 0.0010998320067301393\n",
            "Epoch: 444 | Loss: 0.0010840246686711907\n",
            "Epoch: 445 | Loss: 0.0010684457374736667\n",
            "Epoch: 446 | Loss: 0.0010530739091336727\n",
            "Epoch: 447 | Loss: 0.001037942711263895\n",
            "Epoch: 448 | Loss: 0.001023034448735416\n",
            "Epoch: 449 | Loss: 0.0010083264205604792\n",
            "Epoch: 450 | Loss: 0.0009938385337591171\n",
            "Epoch: 451 | Loss: 0.0009795499499887228\n",
            "Epoch: 452 | Loss: 0.0009654697496443987\n",
            "Epoch: 453 | Loss: 0.0009516151621937752\n",
            "Epoch: 454 | Loss: 0.0009379142429679632\n",
            "Epoch: 455 | Loss: 0.0009244434768334031\n",
            "Epoch: 456 | Loss: 0.0009111578110605478\n",
            "Epoch: 457 | Loss: 0.00089806760661304\n",
            "Epoch: 458 | Loss: 0.0008851708262227476\n",
            "Epoch: 459 | Loss: 0.0008724291110411286\n",
            "Epoch: 460 | Loss: 0.0008598973508924246\n",
            "Epoch: 461 | Loss: 0.0008475325885228813\n",
            "Epoch: 462 | Loss: 0.0008353614248335361\n",
            "Epoch: 463 | Loss: 0.0008233566768467426\n",
            "Epoch: 464 | Loss: 0.0008115333039313555\n",
            "Epoch: 465 | Loss: 0.0007998530054464936\n",
            "Epoch: 466 | Loss: 0.0007883699145168066\n",
            "Epoch: 467 | Loss: 0.0007770283264108002\n",
            "Epoch: 468 | Loss: 0.0007658782415091991\n",
            "Epoch: 469 | Loss: 0.0007548663415946066\n",
            "Epoch: 470 | Loss: 0.0007440054905600846\n",
            "Epoch: 471 | Loss: 0.0007333177491091192\n",
            "Epoch: 472 | Loss: 0.0007227902533486485\n",
            "Epoch: 473 | Loss: 0.0007123854593373835\n",
            "Epoch: 474 | Loss: 0.0007021607598289847\n",
            "Epoch: 475 | Loss: 0.0006920743035152555\n",
            "Epoch: 476 | Loss: 0.0006821078713983297\n",
            "Epoch: 477 | Loss: 0.0006723178084939718\n",
            "Epoch: 478 | Loss: 0.0006626523099839687\n",
            "Epoch: 479 | Loss: 0.000653125171083957\n",
            "Epoch: 480 | Loss: 0.0006437422707676888\n",
            "Epoch: 481 | Loss: 0.0006345019210129976\n",
            "Epoch: 482 | Loss: 0.0006253662286326289\n",
            "Epoch: 483 | Loss: 0.0006163775688037276\n",
            "Epoch: 484 | Loss: 0.0006075198762118816\n",
            "Epoch: 485 | Loss: 0.0005987918120808899\n",
            "Epoch: 486 | Loss: 0.000590189010836184\n",
            "Epoch: 487 | Loss: 0.0005817129858769476\n",
            "Epoch: 488 | Loss: 0.0005733525613322854\n",
            "Epoch: 489 | Loss: 0.00056510791182518\n",
            "Epoch: 490 | Loss: 0.0005569984787143767\n",
            "Epoch: 491 | Loss: 0.0005489790346473455\n",
            "Epoch: 492 | Loss: 0.0005410866579040885\n",
            "Epoch: 493 | Loss: 0.0005333104636520147\n",
            "Epoch: 494 | Loss: 0.000525651965290308\n",
            "Epoch: 495 | Loss: 0.0005181003944016993\n",
            "Epoch: 496 | Loss: 0.0005106612807139754\n",
            "Epoch: 497 | Loss: 0.0005033125635236502\n",
            "Epoch: 498 | Loss: 0.0004960766527801752\n",
            "Epoch: 499 | Loss: 0.0004889509291388094\n",
            "prediction (after training) 4 7.974580764770508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax\n",
        "- jax 는 pytorch와 다르게 함수형 프로그램을 강조하고 있음 (class 가 필요없음)\n",
        "- 모델과 관련된 파라미터와 계산을 함수로 구현하여 사용하는 것이 일반적임\n",
        "\n",
        "\n",
        "### 변경사항\n",
        "  - loss 를 optax 내장함수로 바꾸어줌\n",
        "  - opimizer 가 2줄짜리로 변경 : opt_state, params 업데이트 필요\n",
        "\n",
        "### optax 를 사용한 최적화 코드\n",
        "\n",
        "[참고자료](https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.sgd)\n",
        "\n",
        "```\n",
        "# optax.sgd 를 활용해 optimizer 를 초기화 할때, params 가 jax 배열이 아니기때문에 오류가 발생\n",
        "optimizer = optax.sgd(learning_rate = 0.01)\n",
        "\n",
        "# 아래와 같이 한번 init 해주어야함\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "\n",
        "# optimizer 를 활용해 업데이트를 하면 opt_state 가 업데이트 되며,\n",
        "# 이를 다시 apply_updates 로 처리해서 실제 업데이트도 진행\n",
        "updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "params = optax.apply_updates(params, updates)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Q. jit 을 gradient 계산하는 부분에 적용하기 위해서는 어떻게 해야 하는가?\n",
        "\n",
        "\n",
        "chat gpt 설명\n",
        "```\n",
        "@jit\n",
        "def value_and_grad(params, x, y):\n",
        "    return jax.value_and_grad(loss)(params, x, y)\n",
        "```\n"
      ],
      "metadata": {
        "id": "5t0YGypVuwGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit\n",
        "import optax\n",
        "\n",
        "x_data = jnp.array([[1.0], [2.0], [3.0]])\n",
        "y_data = jnp.array([[2.0], [4.0], [6.0]])\n",
        "\n",
        "# define model\n",
        "def model(params, x):\n",
        "  return jnp.dot(x, params['weight']) + params['bias']\n",
        "\n",
        "# define loss(params)\n",
        "def loss(params, x, y):\n",
        "  y_pred = model(params, x)\n",
        "  return jnp.mean((y_pred - y) ** 2)\n",
        "\n",
        "# gradient computation\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "# initialize parameters\n",
        "init_params = {'weight': jnp.array([[1.0]]), 'bias': jnp.array([0.0])}\n",
        "\n",
        "# initialize optimizer\n"
      ],
      "metadata": {
        "id": "LyIA4C4Puh_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit\n",
        "import optax\n",
        "\n",
        "# Define data\n",
        "x_data = jnp.array([[1.0], [2.0], [3.0]])\n",
        "y_data = jnp.array([[2.0], [4.0], [6.0]])\n",
        "\n",
        "# Define model\n",
        "def model(params, x):\n",
        "    return jnp.dot(x, params['weight']) + params['bias']\n",
        "\n",
        "# Loss function\n",
        "def loss(params, x, y):\n",
        "    y_pred = model(params, x)\n",
        "    return jnp.mean((y_pred - y) ** 2)\n",
        "\n",
        "# Gradient computation\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "# Initialize parameters\n",
        "init_params = {'weight': jnp.array([[1.0]]), 'bias': jnp.array([0.0])}\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optax.sgd(learning_rate=0.01)\n",
        "\n",
        "@jit\n",
        "def update(params, opt_state, x, y):\n",
        "    grads = grad_loss(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, opt_state\n",
        "\n",
        "# Initialize optimizer state\n",
        "opt_state = optimizer.init(init_params)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "params = init_params\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    params, opt_state = update(params, opt_state, x_data, y_data)\n",
        "    loss_val = loss(params, x_data, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss_val}')\n",
        "\n",
        "# After training\n",
        "hour_var = jnp.array([[4.0]])\n",
        "y_pred = model(params, hour_var)\n",
        "print(\"Prediction (after training):\", 4, y_pred[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba3XGbKLxuUe",
        "outputId": "9bacd74c-cc28-4b3d-d7e8-d66ef28fd47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 3.6927404403686523\n",
            "Epoch: 1 | Loss: 2.9228854179382324\n",
            "Epoch: 2 | Loss: 2.3143362998962402\n",
            "Epoch: 3 | Loss: 1.8332923650741577\n",
            "Epoch: 4 | Loss: 1.4530338048934937\n",
            "Epoch: 5 | Loss: 1.1524405479431152\n",
            "Epoch: 6 | Loss: 0.9148194193840027\n",
            "Epoch: 7 | Loss: 0.7269737720489502\n",
            "Epoch: 8 | Loss: 0.5784738659858704\n",
            "Epoch: 9 | Loss: 0.46107369661331177\n",
            "Epoch: 10 | Loss: 0.36825668811798096\n",
            "Epoch: 11 | Loss: 0.2948717176914215\n",
            "Epoch: 12 | Loss: 0.23684683442115784\n",
            "Epoch: 13 | Loss: 0.19096308946609497\n",
            "Epoch: 14 | Loss: 0.15467634797096252\n",
            "Epoch: 15 | Loss: 0.1259755939245224\n",
            "Epoch: 16 | Loss: 0.1032714694738388\n",
            "Epoch: 17 | Loss: 0.08530738204717636\n",
            "Epoch: 18 | Loss: 0.07108993083238602\n",
            "Epoch: 19 | Loss: 0.05983422324061394\n",
            "Epoch: 20 | Loss: 0.05091980844736099\n",
            "Epoch: 21 | Loss: 0.04385606199502945\n",
            "Epoch: 22 | Loss: 0.03825516626238823\n",
            "Epoch: 23 | Loss: 0.03381074592471123\n",
            "Epoch: 24 | Loss: 0.0302805807441473\n",
            "Epoch: 25 | Loss: 0.027473006397485733\n",
            "Epoch: 26 | Loss: 0.02523679845035076\n",
            "Epoch: 27 | Loss: 0.02345230244100094\n",
            "Epoch: 28 | Loss: 0.022024905309081078\n",
            "Epoch: 29 | Loss: 0.02087988518178463\n",
            "Epoch: 30 | Loss: 0.019958246499300003\n",
            "Epoch: 31 | Loss: 0.019212985411286354\n",
            "Epoch: 32 | Loss: 0.018607493489980698\n",
            "Epoch: 33 | Loss: 0.018112465739250183\n",
            "Epoch: 34 | Loss: 0.01770477555692196\n",
            "Epoch: 35 | Loss: 0.01736629009246826\n",
            "Epoch: 36 | Loss: 0.01708250865340233\n",
            "Epoch: 37 | Loss: 0.01684209704399109\n",
            "Epoch: 38 | Loss: 0.01663602516055107\n",
            "Epoch: 39 | Loss: 0.016457153484225273\n",
            "Epoch: 40 | Loss: 0.016299894079566002\n",
            "Epoch: 41 | Loss: 0.01615983620285988\n",
            "Epoch: 42 | Loss: 0.016033349558711052\n",
            "Epoch: 43 | Loss: 0.015917692333459854\n",
            "Epoch: 44 | Loss: 0.015810763463377953\n",
            "Epoch: 45 | Loss: 0.01571068912744522\n",
            "Epoch: 46 | Loss: 0.015616197139024734\n",
            "Epoch: 47 | Loss: 0.015526125207543373\n",
            "Epoch: 48 | Loss: 0.015439623966813087\n",
            "Epoch: 49 | Loss: 0.015356041491031647\n",
            "Epoch: 50 | Loss: 0.015274867415428162\n",
            "Epoch: 51 | Loss: 0.015195593237876892\n",
            "Epoch: 52 | Loss: 0.015118014067411423\n",
            "Epoch: 53 | Loss: 0.015041693113744259\n",
            "Epoch: 54 | Loss: 0.014966603368520737\n",
            "Epoch: 55 | Loss: 0.01489241048693657\n",
            "Epoch: 56 | Loss: 0.014819113537669182\n",
            "Epoch: 57 | Loss: 0.01474647969007492\n",
            "Epoch: 58 | Loss: 0.014674535021185875\n",
            "Epoch: 59 | Loss: 0.01460318174213171\n",
            "Epoch: 60 | Loss: 0.014532344415783882\n",
            "Epoch: 61 | Loss: 0.014461960643529892\n",
            "Epoch: 62 | Loss: 0.014392105862498283\n",
            "Epoch: 63 | Loss: 0.014322626404464245\n",
            "Epoch: 64 | Loss: 0.014253555797040462\n",
            "Epoch: 65 | Loss: 0.014184905216097832\n",
            "Epoch: 66 | Loss: 0.01411665789783001\n",
            "Epoch: 67 | Loss: 0.014048680663108826\n",
            "Epoch: 68 | Loss: 0.013981116935610771\n",
            "Epoch: 69 | Loss: 0.0139138950034976\n",
            "Epoch: 70 | Loss: 0.013847004622220993\n",
            "Epoch: 71 | Loss: 0.01378045417368412\n",
            "Epoch: 72 | Loss: 0.013714270666241646\n",
            "Epoch: 73 | Loss: 0.013648368418216705\n",
            "Epoch: 74 | Loss: 0.013582756742835045\n",
            "Epoch: 75 | Loss: 0.01351756602525711\n",
            "Epoch: 76 | Loss: 0.013452589511871338\n",
            "Epoch: 77 | Loss: 0.013388006016612053\n",
            "Epoch: 78 | Loss: 0.013323701918125153\n",
            "Epoch: 79 | Loss: 0.013259721919894218\n",
            "Epoch: 80 | Loss: 0.013196036219596863\n",
            "Epoch: 81 | Loss: 0.013132672756910324\n",
            "Epoch: 82 | Loss: 0.013069591484963894\n",
            "Epoch: 83 | Loss: 0.013006842695176601\n",
            "Epoch: 84 | Loss: 0.012944364920258522\n",
            "Epoch: 85 | Loss: 0.012882212176918983\n",
            "Epoch: 86 | Loss: 0.01282031461596489\n",
            "Epoch: 87 | Loss: 0.012758749537169933\n",
            "Epoch: 88 | Loss: 0.012697495520114899\n",
            "Epoch: 89 | Loss: 0.012636538594961166\n",
            "Epoch: 90 | Loss: 0.01257583312690258\n",
            "Epoch: 91 | Loss: 0.012515469454228878\n",
            "Epoch: 92 | Loss: 0.012455343268811703\n",
            "Epoch: 93 | Loss: 0.012395529076457024\n",
            "Epoch: 94 | Loss: 0.01233602687716484\n",
            "Epoch: 95 | Loss: 0.012276794761419296\n",
            "Epoch: 96 | Loss: 0.012217829935252666\n",
            "Epoch: 97 | Loss: 0.012159126810729504\n",
            "Epoch: 98 | Loss: 0.01210075058043003\n",
            "Epoch: 99 | Loss: 0.012042644433677197\n",
            "Epoch: 100 | Loss: 0.011984815821051598\n",
            "Epoch: 101 | Loss: 0.01192726194858551\n",
            "Epoch: 102 | Loss: 0.011870015412569046\n",
            "Epoch: 103 | Loss: 0.011813003569841385\n",
            "Epoch: 104 | Loss: 0.011756262741982937\n",
            "Epoch: 105 | Loss: 0.011699827387928963\n",
            "Epoch: 106 | Loss: 0.011643621139228344\n",
            "Epoch: 107 | Loss: 0.011587703600525856\n",
            "Epoch: 108 | Loss: 0.011532067321240902\n",
            "Epoch: 109 | Loss: 0.011476724408566952\n",
            "Epoch: 110 | Loss: 0.011421570554375648\n",
            "Epoch: 111 | Loss: 0.011366765014827251\n",
            "Epoch: 112 | Loss: 0.011312148533761501\n",
            "Epoch: 113 | Loss: 0.011257810518145561\n",
            "Epoch: 114 | Loss: 0.011203768663108349\n",
            "Epoch: 115 | Loss: 0.01114995963871479\n",
            "Epoch: 116 | Loss: 0.011096443049609661\n",
            "Epoch: 117 | Loss: 0.011043129488825798\n",
            "Epoch: 118 | Loss: 0.010990103706717491\n",
            "Epoch: 119 | Loss: 0.010937312617897987\n",
            "Epoch: 120 | Loss: 0.010884800925850868\n",
            "Epoch: 121 | Loss: 0.010832518339157104\n",
            "Epoch: 122 | Loss: 0.010780523531138897\n",
            "Epoch: 123 | Loss: 0.010728754103183746\n",
            "Epoch: 124 | Loss: 0.010677244514226913\n",
            "Epoch: 125 | Loss: 0.010625950060784817\n",
            "Epoch: 126 | Loss: 0.010574947111308575\n",
            "Epoch: 127 | Loss: 0.01052413322031498\n",
            "Epoch: 128 | Loss: 0.01047360897064209\n",
            "Epoch: 129 | Loss: 0.010423323139548302\n",
            "Epoch: 130 | Loss: 0.010373263619840145\n",
            "Epoch: 131 | Loss: 0.010323448106646538\n",
            "Epoch: 132 | Loss: 0.010273878462612629\n",
            "Epoch: 133 | Loss: 0.010224560275673866\n",
            "Epoch: 134 | Loss: 0.0101754330098629\n",
            "Epoch: 135 | Loss: 0.01012658141553402\n",
            "Epoch: 136 | Loss: 0.01007796823978424\n",
            "Epoch: 137 | Loss: 0.010029580444097519\n",
            "Epoch: 138 | Loss: 0.00998140312731266\n",
            "Epoch: 139 | Loss: 0.009933466091752052\n",
            "Epoch: 140 | Loss: 0.009885763749480247\n",
            "Epoch: 141 | Loss: 0.009838307276368141\n",
            "Epoch: 142 | Loss: 0.00979106966406107\n",
            "Epoch: 143 | Loss: 0.009744038805365562\n",
            "Epoch: 144 | Loss: 0.009697243571281433\n",
            "Epoch: 145 | Loss: 0.009650657884776592\n",
            "Epoch: 146 | Loss: 0.009604334831237793\n",
            "Epoch: 147 | Loss: 0.009558221325278282\n",
            "Epoch: 148 | Loss: 0.009512318298220634\n",
            "Epoch: 149 | Loss: 0.00946661178022623\n",
            "Epoch: 150 | Loss: 0.00942117627710104\n",
            "Epoch: 151 | Loss: 0.0093759223818779\n",
            "Epoch: 152 | Loss: 0.009330915287137032\n",
            "Epoch: 153 | Loss: 0.009286114014685154\n",
            "Epoch: 154 | Loss: 0.009241517633199692\n",
            "Epoch: 155 | Loss: 0.00919712521135807\n",
            "Epoch: 156 | Loss: 0.009152939543128014\n",
            "Epoch: 157 | Loss: 0.009109025821089745\n",
            "Epoch: 158 | Loss: 0.009065285325050354\n",
            "Epoch: 159 | Loss: 0.009021727368235588\n",
            "Epoch: 160 | Loss: 0.008978413417935371\n",
            "Epoch: 161 | Loss: 0.008935295976698399\n",
            "Epoch: 162 | Loss: 0.008892405778169632\n",
            "Epoch: 163 | Loss: 0.008849685080349445\n",
            "Epoch: 164 | Loss: 0.0088071683421731\n",
            "Epoch: 165 | Loss: 0.008764892816543579\n",
            "Epoch: 166 | Loss: 0.00872280914336443\n",
            "Epoch: 167 | Loss: 0.008680938743054867\n",
            "Epoch: 168 | Loss: 0.008639222010970116\n",
            "Epoch: 169 | Loss: 0.00859774462878704\n",
            "Epoch: 170 | Loss: 0.00855646189302206\n",
            "Epoch: 171 | Loss: 0.008515377528965473\n",
            "Epoch: 172 | Loss: 0.008474498055875301\n",
            "Epoch: 173 | Loss: 0.008433780632913113\n",
            "Epoch: 174 | Loss: 0.00839326623827219\n",
            "Epoch: 175 | Loss: 0.008352978155016899\n",
            "Epoch: 176 | Loss: 0.008312857709825039\n",
            "Epoch: 177 | Loss: 0.008272949606180191\n",
            "Epoch: 178 | Loss: 0.008233223110437393\n",
            "Epoch: 179 | Loss: 0.008193688467144966\n",
            "Epoch: 180 | Loss: 0.008154338225722313\n",
            "Epoch: 181 | Loss: 0.008115176111459732\n",
            "Epoch: 182 | Loss: 0.008076219819486141\n",
            "Epoch: 183 | Loss: 0.008037419989705086\n",
            "Epoch: 184 | Loss: 0.007998834364116192\n",
            "Epoch: 185 | Loss: 0.007960425689816475\n",
            "Epoch: 186 | Loss: 0.00792221911251545\n",
            "Epoch: 187 | Loss: 0.007884152233600616\n",
            "Epoch: 188 | Loss: 0.007846287451684475\n",
            "Epoch: 189 | Loss: 0.007808598689734936\n",
            "Epoch: 190 | Loss: 0.007771120872348547\n",
            "Epoch: 191 | Loss: 0.007733791135251522\n",
            "Epoch: 192 | Loss: 0.007696646265685558\n",
            "Epoch: 193 | Loss: 0.007659689988940954\n",
            "Epoch: 194 | Loss: 0.007622893434017897\n",
            "Epoch: 195 | Loss: 0.007586302235722542\n",
            "Epoch: 196 | Loss: 0.0075498949736356735\n",
            "Epoch: 197 | Loss: 0.007513602264225483\n",
            "Epoch: 198 | Loss: 0.007477547042071819\n",
            "Epoch: 199 | Loss: 0.007441628724336624\n",
            "Epoch: 200 | Loss: 0.0074059003964066505\n",
            "Epoch: 201 | Loss: 0.007370343431830406\n",
            "Epoch: 202 | Loss: 0.007334940135478973\n",
            "Epoch: 203 | Loss: 0.007299704011529684\n",
            "Epoch: 204 | Loss: 0.0072646597400307655\n",
            "Epoch: 205 | Loss: 0.0072297584265470505\n",
            "Epoch: 206 | Loss: 0.007195055019110441\n",
            "Epoch: 207 | Loss: 0.00716052483767271\n",
            "Epoch: 208 | Loss: 0.007126107346266508\n",
            "Epoch: 209 | Loss: 0.00709188636392355\n",
            "Epoch: 210 | Loss: 0.007057852111756802\n",
            "Epoch: 211 | Loss: 0.007023964077234268\n",
            "Epoch: 212 | Loss: 0.0069902194663882256\n",
            "Epoch: 213 | Loss: 0.00695665692910552\n",
            "Epoch: 214 | Loss: 0.006923259235918522\n",
            "Epoch: 215 | Loss: 0.006889985874295235\n",
            "Epoch: 216 | Loss: 0.006856904365122318\n",
            "Epoch: 217 | Loss: 0.006823994219303131\n",
            "Epoch: 218 | Loss: 0.0067912316881120205\n",
            "Epoch: 219 | Loss: 0.006758606992661953\n",
            "Epoch: 220 | Loss: 0.006726167630404234\n",
            "Epoch: 221 | Loss: 0.006693858187645674\n",
            "Epoch: 222 | Loss: 0.006661722436547279\n",
            "Epoch: 223 | Loss: 0.00662971381098032\n",
            "Epoch: 224 | Loss: 0.006597890984266996\n",
            "Epoch: 225 | Loss: 0.006566202268004417\n",
            "Epoch: 226 | Loss: 0.006534667685627937\n",
            "Epoch: 227 | Loss: 0.00650327792391181\n",
            "Epoch: 228 | Loss: 0.006472059525549412\n",
            "Epoch: 229 | Loss: 0.006440976168960333\n",
            "Epoch: 230 | Loss: 0.006410051137208939\n",
            "Epoch: 231 | Loss: 0.006379272788763046\n",
            "Epoch: 232 | Loss: 0.0063486406579613686\n",
            "Epoch: 233 | Loss: 0.0063181305304169655\n",
            "Epoch: 234 | Loss: 0.006287800148129463\n",
            "Epoch: 235 | Loss: 0.006257619243115187\n",
            "Epoch: 236 | Loss: 0.006227556616067886\n",
            "Epoch: 237 | Loss: 0.006197643466293812\n",
            "Epoch: 238 | Loss: 0.0061678956262767315\n",
            "Epoch: 239 | Loss: 0.006138278171420097\n",
            "Epoch: 240 | Loss: 0.006108790170401335\n",
            "Epoch: 241 | Loss: 0.006079461425542831\n",
            "Epoch: 242 | Loss: 0.006050271913409233\n",
            "Epoch: 243 | Loss: 0.006021230947226286\n",
            "Epoch: 244 | Loss: 0.005992300342768431\n",
            "Epoch: 245 | Loss: 0.005963518284261227\n",
            "Epoch: 246 | Loss: 0.005934884771704674\n",
            "Epoch: 247 | Loss: 0.005906369537115097\n",
            "Epoch: 248 | Loss: 0.005878006108105183\n",
            "Epoch: 249 | Loss: 0.00584978936240077\n",
            "Epoch: 250 | Loss: 0.00582171231508255\n",
            "Epoch: 251 | Loss: 0.00579374423250556\n",
            "Epoch: 252 | Loss: 0.005765925161540508\n",
            "Epoch: 253 | Loss: 0.005738237872719765\n",
            "Epoch: 254 | Loss: 0.005710681900382042\n",
            "Epoch: 255 | Loss: 0.005683263298124075\n",
            "Epoch: 256 | Loss: 0.0056559862568974495\n",
            "Epoch: 257 | Loss: 0.005628816783428192\n",
            "Epoch: 258 | Loss: 0.005601784214377403\n",
            "Epoch: 259 | Loss: 0.005574870388954878\n",
            "Epoch: 260 | Loss: 0.005548086017370224\n",
            "Epoch: 261 | Loss: 0.005521473940461874\n",
            "Epoch: 262 | Loss: 0.005494961515069008\n",
            "Epoch: 263 | Loss: 0.005468565039336681\n",
            "Epoch: 264 | Loss: 0.0054423133842647076\n",
            "Epoch: 265 | Loss: 0.005416177213191986\n",
            "Epoch: 266 | Loss: 0.005390155594795942\n",
            "Epoch: 267 | Loss: 0.005364258773624897\n",
            "Epoch: 268 | Loss: 0.005338494665920734\n",
            "Epoch: 269 | Loss: 0.005312883295118809\n",
            "Epoch: 270 | Loss: 0.005287367384880781\n",
            "Epoch: 271 | Loss: 0.005261990707367659\n",
            "Epoch: 272 | Loss: 0.005236702039837837\n",
            "Epoch: 273 | Loss: 0.0052115595899522305\n",
            "Epoch: 274 | Loss: 0.005186540074646473\n",
            "Epoch: 275 | Loss: 0.0051616220735013485\n",
            "Epoch: 276 | Loss: 0.0051368409767746925\n",
            "Epoch: 277 | Loss: 0.005112162791192532\n",
            "Epoch: 278 | Loss: 0.005087622441351414\n",
            "Epoch: 279 | Loss: 0.00506318686529994\n",
            "Epoch: 280 | Loss: 0.005038886331021786\n",
            "Epoch: 281 | Loss: 0.005014684516936541\n",
            "Epoch: 282 | Loss: 0.0049905842170119286\n",
            "Epoch: 283 | Loss: 0.0049666245467960835\n",
            "Epoch: 284 | Loss: 0.004942798055708408\n",
            "Epoch: 285 | Loss: 0.004919035360217094\n",
            "Epoch: 286 | Loss: 0.00489541981369257\n",
            "Epoch: 287 | Loss: 0.004871909972280264\n",
            "Epoch: 288 | Loss: 0.004848513286560774\n",
            "Epoch: 289 | Loss: 0.0048252190463244915\n",
            "Epoch: 290 | Loss: 0.004802071955054998\n",
            "Epoch: 291 | Loss: 0.004778996575623751\n",
            "Epoch: 292 | Loss: 0.00475606694817543\n",
            "Epoch: 293 | Loss: 0.004733202047646046\n",
            "Epoch: 294 | Loss: 0.00471048429608345\n",
            "Epoch: 295 | Loss: 0.004687868990004063\n",
            "Epoch: 296 | Loss: 0.004665365908294916\n",
            "Epoch: 297 | Loss: 0.004642965272068977\n",
            "Epoch: 298 | Loss: 0.004620668478310108\n",
            "Epoch: 299 | Loss: 0.004598475061357021\n",
            "Epoch: 300 | Loss: 0.0045763831585645676\n",
            "Epoch: 301 | Loss: 0.004554418846964836\n",
            "Epoch: 302 | Loss: 0.004532536957412958\n",
            "Epoch: 303 | Loss: 0.004510757513344288\n",
            "Epoch: 304 | Loss: 0.004489120561629534\n",
            "Epoch: 305 | Loss: 0.004467566963285208\n",
            "Epoch: 306 | Loss: 0.004446107428520918\n",
            "Epoch: 307 | Loss: 0.004424760118126869\n",
            "Epoch: 308 | Loss: 0.004403506405651569\n",
            "Epoch: 309 | Loss: 0.00438235979527235\n",
            "Epoch: 310 | Loss: 0.004361310042440891\n",
            "Epoch: 311 | Loss: 0.004340382292866707\n",
            "Epoch: 312 | Loss: 0.004319530911743641\n",
            "Epoch: 313 | Loss: 0.004298782907426357\n",
            "Epoch: 314 | Loss: 0.004278130829334259\n",
            "Epoch: 315 | Loss: 0.004257585387676954\n",
            "Epoch: 316 | Loss: 0.004237161483615637\n",
            "Epoch: 317 | Loss: 0.00421679113060236\n",
            "Epoch: 318 | Loss: 0.004196550231426954\n",
            "Epoch: 319 | Loss: 0.004176395013928413\n",
            "Epoch: 320 | Loss: 0.004156344570219517\n",
            "Epoch: 321 | Loss: 0.004136381205171347\n",
            "Epoch: 322 | Loss: 0.004116510972380638\n",
            "Epoch: 323 | Loss: 0.004096757620573044\n",
            "Epoch: 324 | Loss: 0.004077077377587557\n",
            "Epoch: 325 | Loss: 0.004057512618601322\n",
            "Epoch: 326 | Loss: 0.004038025625050068\n",
            "Epoch: 327 | Loss: 0.004018629901111126\n",
            "Epoch: 328 | Loss: 0.0039993333630263805\n",
            "Epoch: 329 | Loss: 0.003980129957199097\n",
            "Epoch: 330 | Loss: 0.003961019683629274\n",
            "Epoch: 331 | Loss: 0.003941982984542847\n",
            "Epoch: 332 | Loss: 0.003923061303794384\n",
            "Epoch: 333 | Loss: 0.0039042308926582336\n",
            "Epoch: 334 | Loss: 0.003885469166561961\n",
            "Epoch: 335 | Loss: 0.003866808954626322\n",
            "Epoch: 336 | Loss: 0.0038482355885207653\n",
            "Epoch: 337 | Loss: 0.003829773049801588\n",
            "Epoch: 338 | Loss: 0.0038113673217594624\n",
            "Epoch: 339 | Loss: 0.0037930821999907494\n",
            "Epoch: 340 | Loss: 0.003774859942495823\n",
            "Epoch: 341 | Loss: 0.0037567371036857367\n",
            "Epoch: 342 | Loss: 0.0037386827170848846\n",
            "Epoch: 343 | Loss: 0.0037207268178462982\n",
            "Epoch: 344 | Loss: 0.0037028572987765074\n",
            "Epoch: 345 | Loss: 0.0036850813776254654\n",
            "Epoch: 346 | Loss: 0.003667393233627081\n",
            "Epoch: 347 | Loss: 0.0036497614346444607\n",
            "Epoch: 348 | Loss: 0.00363224558532238\n",
            "Epoch: 349 | Loss: 0.00361480750143528\n",
            "Epoch: 350 | Loss: 0.0035974541679024696\n",
            "Epoch: 351 | Loss: 0.0035801706835627556\n",
            "Epoch: 352 | Loss: 0.003562991274520755\n",
            "Epoch: 353 | Loss: 0.003545862389728427\n",
            "Epoch: 354 | Loss: 0.0035288408398628235\n",
            "Epoch: 355 | Loss: 0.0035119024105370045\n",
            "Epoch: 356 | Loss: 0.003495034296065569\n",
            "Epoch: 357 | Loss: 0.003478253958746791\n",
            "Epoch: 358 | Loss: 0.0034615383483469486\n",
            "Epoch: 359 | Loss: 0.0034449249505996704\n",
            "Epoch: 360 | Loss: 0.003428395837545395\n",
            "Epoch: 361 | Loss: 0.003411923535168171\n",
            "Epoch: 362 | Loss: 0.00339552853256464\n",
            "Epoch: 363 | Loss: 0.0033792369067668915\n",
            "Epoch: 364 | Loss: 0.0033630123361945152\n",
            "Epoch: 365 | Loss: 0.003346853656694293\n",
            "Epoch: 366 | Loss: 0.0033307764679193497\n",
            "Epoch: 367 | Loss: 0.00331478426232934\n",
            "Epoch: 368 | Loss: 0.00329885957762599\n",
            "Epoch: 369 | Loss: 0.0032830291893333197\n",
            "Epoch: 370 | Loss: 0.003267263527959585\n",
            "Epoch: 371 | Loss: 0.0032515707425773144\n",
            "Epoch: 372 | Loss: 0.0032359552569687366\n",
            "Epoch: 373 | Loss: 0.003220404265448451\n",
            "Epoch: 374 | Loss: 0.0032049627043306828\n",
            "Epoch: 375 | Loss: 0.0031895549036562443\n",
            "Epoch: 376 | Loss: 0.0031742460560053587\n",
            "Epoch: 377 | Loss: 0.003159001236781478\n",
            "Epoch: 378 | Loss: 0.0031438337173312902\n",
            "Epoch: 379 | Loss: 0.0031287295278161764\n",
            "Epoch: 380 | Loss: 0.003113723127171397\n",
            "Epoch: 381 | Loss: 0.0030987586360424757\n",
            "Epoch: 382 | Loss: 0.0030838767997920513\n",
            "Epoch: 383 | Loss: 0.0030690773855894804\n",
            "Epoch: 384 | Loss: 0.0030543338507413864\n",
            "Epoch: 385 | Loss: 0.0030396683141589165\n",
            "Epoch: 386 | Loss: 0.0030250679701566696\n",
            "Epoch: 387 | Loss: 0.0030105430632829666\n",
            "Epoch: 388 | Loss: 0.002996093360707164\n",
            "Epoch: 389 | Loss: 0.002981704194098711\n",
            "Epoch: 390 | Loss: 0.0029673806857317686\n",
            "Epoch: 391 | Loss: 0.0029531321488320827\n",
            "Epoch: 392 | Loss: 0.002938944613561034\n",
            "Epoch: 393 | Loss: 0.002924820873886347\n",
            "Epoch: 394 | Loss: 0.0029107797890901566\n",
            "Epoch: 395 | Loss: 0.0028968031983822584\n",
            "Epoch: 396 | Loss: 0.0028828997164964676\n",
            "Epoch: 397 | Loss: 0.0028690570034086704\n",
            "Epoch: 398 | Loss: 0.0028552727308124304\n",
            "Epoch: 399 | Loss: 0.0028415662236511707\n",
            "Epoch: 400 | Loss: 0.002827912336215377\n",
            "Epoch: 401 | Loss: 0.002814335748553276\n",
            "Epoch: 402 | Loss: 0.0028008311055600643\n",
            "Epoch: 403 | Loss: 0.002787369303405285\n",
            "Epoch: 404 | Loss: 0.0027739848010241985\n",
            "Epoch: 405 | Loss: 0.0027606706134974957\n",
            "Epoch: 406 | Loss: 0.0027474132366478443\n",
            "Epoch: 407 | Loss: 0.0027342194225639105\n",
            "Epoch: 408 | Loss: 0.002721088705584407\n",
            "Epoch: 409 | Loss: 0.0027080215513706207\n",
            "Epoch: 410 | Loss: 0.0026950098108500242\n",
            "Epoch: 411 | Loss: 0.002682067919522524\n",
            "Epoch: 412 | Loss: 0.0026691951788961887\n",
            "Epoch: 413 | Loss: 0.0026563776191323996\n",
            "Epoch: 414 | Loss: 0.0026436219923198223\n",
            "Epoch: 415 | Loss: 0.002630916191264987\n",
            "Epoch: 416 | Loss: 0.002618297003209591\n",
            "Epoch: 417 | Loss: 0.0026057192590087652\n",
            "Epoch: 418 | Loss: 0.002593209035694599\n",
            "Epoch: 419 | Loss: 0.002580758184194565\n",
            "Epoch: 420 | Loss: 0.002568357391282916\n",
            "Epoch: 421 | Loss: 0.00255604088306427\n",
            "Epoch: 422 | Loss: 0.0025437658187001944\n",
            "Epoch: 423 | Loss: 0.002531548961997032\n",
            "Epoch: 424 | Loss: 0.0025193816982209682\n",
            "Epoch: 425 | Loss: 0.0025072842836380005\n",
            "Epoch: 426 | Loss: 0.002495227614417672\n",
            "Epoch: 427 | Loss: 0.0024832619819790125\n",
            "Epoch: 428 | Loss: 0.0024713249877095222\n",
            "Epoch: 429 | Loss: 0.0024594650603830814\n",
            "Epoch: 430 | Loss: 0.002447648672387004\n",
            "Epoch: 431 | Loss: 0.0024358988739550114\n",
            "Epoch: 432 | Loss: 0.0024242005310952663\n",
            "Epoch: 433 | Loss: 0.0024125531781464815\n",
            "Epoch: 434 | Loss: 0.00240098824724555\n",
            "Epoch: 435 | Loss: 0.0023894445039331913\n",
            "Epoch: 436 | Loss: 0.002377969678491354\n",
            "Epoch: 437 | Loss: 0.0023665581829845905\n",
            "Epoch: 438 | Loss: 0.0023551841732114553\n",
            "Epoch: 439 | Loss: 0.002343866741284728\n",
            "Epoch: 440 | Loss: 0.002332620322704315\n",
            "Epoch: 441 | Loss: 0.0023214281536638737\n",
            "Epoch: 442 | Loss: 0.002310285810381174\n",
            "Epoch: 443 | Loss: 0.0022991816513240337\n",
            "Epoch: 444 | Loss: 0.0022881394252181053\n",
            "Epoch: 445 | Loss: 0.002277158899232745\n",
            "Epoch: 446 | Loss: 0.002266207942739129\n",
            "Epoch: 447 | Loss: 0.0022553298622369766\n",
            "Epoch: 448 | Loss: 0.0022445032373070717\n",
            "Epoch: 449 | Loss: 0.002233723411336541\n",
            "Epoch: 450 | Loss: 0.002222997834905982\n",
            "Epoch: 451 | Loss: 0.0022123283706605434\n",
            "Epoch: 452 | Loss: 0.002201698487624526\n",
            "Epoch: 453 | Loss: 0.002191130304709077\n",
            "Epoch: 454 | Loss: 0.0021806098520755768\n",
            "Epoch: 455 | Loss: 0.002170123625546694\n",
            "Epoch: 456 | Loss: 0.0021597142331302166\n",
            "Epoch: 457 | Loss: 0.0021493379026651382\n",
            "Epoch: 458 | Loss: 0.0021390237379819155\n",
            "Epoch: 459 | Loss: 0.00212874379940331\n",
            "Epoch: 460 | Loss: 0.002118523931130767\n",
            "Epoch: 461 | Loss: 0.002108355052769184\n",
            "Epoch: 462 | Loss: 0.0020982290152460337\n",
            "Epoch: 463 | Loss: 0.0020881532691419125\n",
            "Epoch: 464 | Loss: 0.002078112680464983\n",
            "Epoch: 465 | Loss: 0.0020681528840214014\n",
            "Epoch: 466 | Loss: 0.002058215206488967\n",
            "Epoch: 467 | Loss: 0.0020483287516981363\n",
            "Epoch: 468 | Loss: 0.0020384935196489096\n",
            "Epoch: 469 | Loss: 0.0020287109073251486\n",
            "Epoch: 470 | Loss: 0.00201895902864635\n",
            "Epoch: 471 | Loss: 0.0020092669874429703\n",
            "Epoch: 472 | Loss: 0.001999617787078023\n",
            "Epoch: 473 | Loss: 0.001990014221519232\n",
            "Epoch: 474 | Loss: 0.0019804590847343206\n",
            "Epoch: 475 | Loss: 0.0019709598273038864\n",
            "Epoch: 476 | Loss: 0.0019614938646554947\n",
            "Epoch: 477 | Loss: 0.0019520726054906845\n",
            "Epoch: 478 | Loss: 0.0019427052466198802\n",
            "Epoch: 479 | Loss: 0.0019333651289343834\n",
            "Epoch: 480 | Loss: 0.0019240856636315584\n",
            "Epoch: 481 | Loss: 0.0019148432184010744\n",
            "Epoch: 482 | Loss: 0.0019056450109928846\n",
            "Epoch: 483 | Loss: 0.0018965037306770682\n",
            "Epoch: 484 | Loss: 0.0018873920198529959\n",
            "Epoch: 485 | Loss: 0.0018783225677907467\n",
            "Epoch: 486 | Loss: 0.001869311323389411\n",
            "Epoch: 487 | Loss: 0.0018603303469717503\n",
            "Epoch: 488 | Loss: 0.001851382665336132\n",
            "Epoch: 489 | Loss: 0.001842498080804944\n",
            "Epoch: 490 | Loss: 0.001833656569942832\n",
            "Epoch: 491 | Loss: 0.0018248393898829818\n",
            "Epoch: 492 | Loss: 0.0018160961335524917\n",
            "Epoch: 493 | Loss: 0.0018073581159114838\n",
            "Epoch: 494 | Loss: 0.001798677141778171\n",
            "Epoch: 495 | Loss: 0.0017900591483339667\n",
            "Epoch: 496 | Loss: 0.0017814537277445197\n",
            "Epoch: 497 | Loss: 0.0017728888196870685\n",
            "Epoch: 498 | Loss: 0.00176438398193568\n",
            "Epoch: 499 | Loss: 0.0017559122061356902\n",
            "Prediction (after training): 4 7.9159613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06_logistic_regression.py"
      ],
      "metadata": {
        "id": "JpunUVTmIUhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# dataset\n",
        "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = tensor([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    instantiate nn.Linear Module\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = nn.Linear(1, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "\n",
        "# out model\n",
        "model = Model()\n",
        "\n",
        "# loss function & optimizer contruction\n",
        "# call to model.parameters()\n",
        "criterion = nn.BCELoss(reduction = 'mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "\n",
        "#Traning loop\n",
        "for epoch in range(1000):\n",
        "  # forward pass : compute predicted y by passing x to the model\n",
        "  y_pred = model(x_data)\n",
        "\n",
        "  # compute and print loss\n",
        "  loss = criterion(y_pred, y_data)\n",
        "  print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
        "\n",
        "  #zero gradient, perform a backward pass, update the weights\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "#After training\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(tensor([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(tensor([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
      ],
      "metadata": {
        "id": "kyqRLp6dxuwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ac89b1-b603-4e06-9032-abcc1197aeb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000 | Loss: 0.7630\n",
            "Epoch 2/1000 | Loss: 0.7591\n",
            "Epoch 3/1000 | Loss: 0.7552\n",
            "Epoch 4/1000 | Loss: 0.7514\n",
            "Epoch 5/1000 | Loss: 0.7477\n",
            "Epoch 6/1000 | Loss: 0.7439\n",
            "Epoch 7/1000 | Loss: 0.7403\n",
            "Epoch 8/1000 | Loss: 0.7366\n",
            "Epoch 9/1000 | Loss: 0.7330\n",
            "Epoch 10/1000 | Loss: 0.7295\n",
            "Epoch 11/1000 | Loss: 0.7259\n",
            "Epoch 12/1000 | Loss: 0.7225\n",
            "Epoch 13/1000 | Loss: 0.7190\n",
            "Epoch 14/1000 | Loss: 0.7156\n",
            "Epoch 15/1000 | Loss: 0.7122\n",
            "Epoch 16/1000 | Loss: 0.7089\n",
            "Epoch 17/1000 | Loss: 0.7056\n",
            "Epoch 18/1000 | Loss: 0.7024\n",
            "Epoch 19/1000 | Loss: 0.6992\n",
            "Epoch 20/1000 | Loss: 0.6960\n",
            "Epoch 21/1000 | Loss: 0.6929\n",
            "Epoch 22/1000 | Loss: 0.6898\n",
            "Epoch 23/1000 | Loss: 0.6868\n",
            "Epoch 24/1000 | Loss: 0.6838\n",
            "Epoch 25/1000 | Loss: 0.6808\n",
            "Epoch 26/1000 | Loss: 0.6779\n",
            "Epoch 27/1000 | Loss: 0.6750\n",
            "Epoch 28/1000 | Loss: 0.6721\n",
            "Epoch 29/1000 | Loss: 0.6693\n",
            "Epoch 30/1000 | Loss: 0.6666\n",
            "Epoch 31/1000 | Loss: 0.6638\n",
            "Epoch 32/1000 | Loss: 0.6611\n",
            "Epoch 33/1000 | Loss: 0.6585\n",
            "Epoch 34/1000 | Loss: 0.6559\n",
            "Epoch 35/1000 | Loss: 0.6533\n",
            "Epoch 36/1000 | Loss: 0.6507\n",
            "Epoch 37/1000 | Loss: 0.6482\n",
            "Epoch 38/1000 | Loss: 0.6458\n",
            "Epoch 39/1000 | Loss: 0.6433\n",
            "Epoch 40/1000 | Loss: 0.6409\n",
            "Epoch 41/1000 | Loss: 0.6386\n",
            "Epoch 42/1000 | Loss: 0.6363\n",
            "Epoch 43/1000 | Loss: 0.6340\n",
            "Epoch 44/1000 | Loss: 0.6317\n",
            "Epoch 45/1000 | Loss: 0.6295\n",
            "Epoch 46/1000 | Loss: 0.6273\n",
            "Epoch 47/1000 | Loss: 0.6252\n",
            "Epoch 48/1000 | Loss: 0.6231\n",
            "Epoch 49/1000 | Loss: 0.6210\n",
            "Epoch 50/1000 | Loss: 0.6190\n",
            "Epoch 51/1000 | Loss: 0.6169\n",
            "Epoch 52/1000 | Loss: 0.6150\n",
            "Epoch 53/1000 | Loss: 0.6130\n",
            "Epoch 54/1000 | Loss: 0.6111\n",
            "Epoch 55/1000 | Loss: 0.6092\n",
            "Epoch 56/1000 | Loss: 0.6074\n",
            "Epoch 57/1000 | Loss: 0.6056\n",
            "Epoch 58/1000 | Loss: 0.6038\n",
            "Epoch 59/1000 | Loss: 0.6020\n",
            "Epoch 60/1000 | Loss: 0.6003\n",
            "Epoch 61/1000 | Loss: 0.5986\n",
            "Epoch 62/1000 | Loss: 0.5970\n",
            "Epoch 63/1000 | Loss: 0.5953\n",
            "Epoch 64/1000 | Loss: 0.5937\n",
            "Epoch 65/1000 | Loss: 0.5921\n",
            "Epoch 66/1000 | Loss: 0.5906\n",
            "Epoch 67/1000 | Loss: 0.5891\n",
            "Epoch 68/1000 | Loss: 0.5876\n",
            "Epoch 69/1000 | Loss: 0.5861\n",
            "Epoch 70/1000 | Loss: 0.5847\n",
            "Epoch 71/1000 | Loss: 0.5833\n",
            "Epoch 72/1000 | Loss: 0.5819\n",
            "Epoch 73/1000 | Loss: 0.5805\n",
            "Epoch 74/1000 | Loss: 0.5792\n",
            "Epoch 75/1000 | Loss: 0.5779\n",
            "Epoch 76/1000 | Loss: 0.5766\n",
            "Epoch 77/1000 | Loss: 0.5754\n",
            "Epoch 78/1000 | Loss: 0.5741\n",
            "Epoch 79/1000 | Loss: 0.5729\n",
            "Epoch 80/1000 | Loss: 0.5717\n",
            "Epoch 81/1000 | Loss: 0.5706\n",
            "Epoch 82/1000 | Loss: 0.5694\n",
            "Epoch 83/1000 | Loss: 0.5683\n",
            "Epoch 84/1000 | Loss: 0.5672\n",
            "Epoch 85/1000 | Loss: 0.5661\n",
            "Epoch 86/1000 | Loss: 0.5650\n",
            "Epoch 87/1000 | Loss: 0.5640\n",
            "Epoch 88/1000 | Loss: 0.5630\n",
            "Epoch 89/1000 | Loss: 0.5620\n",
            "Epoch 90/1000 | Loss: 0.5610\n",
            "Epoch 91/1000 | Loss: 0.5601\n",
            "Epoch 92/1000 | Loss: 0.5591\n",
            "Epoch 93/1000 | Loss: 0.5582\n",
            "Epoch 94/1000 | Loss: 0.5573\n",
            "Epoch 95/1000 | Loss: 0.5564\n",
            "Epoch 96/1000 | Loss: 0.5555\n",
            "Epoch 97/1000 | Loss: 0.5547\n",
            "Epoch 98/1000 | Loss: 0.5538\n",
            "Epoch 99/1000 | Loss: 0.5530\n",
            "Epoch 100/1000 | Loss: 0.5522\n",
            "Epoch 101/1000 | Loss: 0.5514\n",
            "Epoch 102/1000 | Loss: 0.5506\n",
            "Epoch 103/1000 | Loss: 0.5499\n",
            "Epoch 104/1000 | Loss: 0.5491\n",
            "Epoch 105/1000 | Loss: 0.5484\n",
            "Epoch 106/1000 | Loss: 0.5477\n",
            "Epoch 107/1000 | Loss: 0.5470\n",
            "Epoch 108/1000 | Loss: 0.5463\n",
            "Epoch 109/1000 | Loss: 0.5456\n",
            "Epoch 110/1000 | Loss: 0.5450\n",
            "Epoch 111/1000 | Loss: 0.5443\n",
            "Epoch 112/1000 | Loss: 0.5437\n",
            "Epoch 113/1000 | Loss: 0.5430\n",
            "Epoch 114/1000 | Loss: 0.5424\n",
            "Epoch 115/1000 | Loss: 0.5418\n",
            "Epoch 116/1000 | Loss: 0.5412\n",
            "Epoch 117/1000 | Loss: 0.5407\n",
            "Epoch 118/1000 | Loss: 0.5401\n",
            "Epoch 119/1000 | Loss: 0.5395\n",
            "Epoch 120/1000 | Loss: 0.5390\n",
            "Epoch 121/1000 | Loss: 0.5384\n",
            "Epoch 122/1000 | Loss: 0.5379\n",
            "Epoch 123/1000 | Loss: 0.5374\n",
            "Epoch 124/1000 | Loss: 0.5369\n",
            "Epoch 125/1000 | Loss: 0.5364\n",
            "Epoch 126/1000 | Loss: 0.5359\n",
            "Epoch 127/1000 | Loss: 0.5354\n",
            "Epoch 128/1000 | Loss: 0.5349\n",
            "Epoch 129/1000 | Loss: 0.5345\n",
            "Epoch 130/1000 | Loss: 0.5340\n",
            "Epoch 131/1000 | Loss: 0.5335\n",
            "Epoch 132/1000 | Loss: 0.5331\n",
            "Epoch 133/1000 | Loss: 0.5327\n",
            "Epoch 134/1000 | Loss: 0.5322\n",
            "Epoch 135/1000 | Loss: 0.5318\n",
            "Epoch 136/1000 | Loss: 0.5314\n",
            "Epoch 137/1000 | Loss: 0.5310\n",
            "Epoch 138/1000 | Loss: 0.5306\n",
            "Epoch 139/1000 | Loss: 0.5302\n",
            "Epoch 140/1000 | Loss: 0.5298\n",
            "Epoch 141/1000 | Loss: 0.5294\n",
            "Epoch 142/1000 | Loss: 0.5290\n",
            "Epoch 143/1000 | Loss: 0.5287\n",
            "Epoch 144/1000 | Loss: 0.5283\n",
            "Epoch 145/1000 | Loss: 0.5279\n",
            "Epoch 146/1000 | Loss: 0.5276\n",
            "Epoch 147/1000 | Loss: 0.5272\n",
            "Epoch 148/1000 | Loss: 0.5269\n",
            "Epoch 149/1000 | Loss: 0.5265\n",
            "Epoch 150/1000 | Loss: 0.5262\n",
            "Epoch 151/1000 | Loss: 0.5259\n",
            "Epoch 152/1000 | Loss: 0.5255\n",
            "Epoch 153/1000 | Loss: 0.5252\n",
            "Epoch 154/1000 | Loss: 0.5249\n",
            "Epoch 155/1000 | Loss: 0.5246\n",
            "Epoch 156/1000 | Loss: 0.5243\n",
            "Epoch 157/1000 | Loss: 0.5240\n",
            "Epoch 158/1000 | Loss: 0.5237\n",
            "Epoch 159/1000 | Loss: 0.5234\n",
            "Epoch 160/1000 | Loss: 0.5231\n",
            "Epoch 161/1000 | Loss: 0.5228\n",
            "Epoch 162/1000 | Loss: 0.5225\n",
            "Epoch 163/1000 | Loss: 0.5222\n",
            "Epoch 164/1000 | Loss: 0.5219\n",
            "Epoch 165/1000 | Loss: 0.5216\n",
            "Epoch 166/1000 | Loss: 0.5214\n",
            "Epoch 167/1000 | Loss: 0.5211\n",
            "Epoch 168/1000 | Loss: 0.5208\n",
            "Epoch 169/1000 | Loss: 0.5206\n",
            "Epoch 170/1000 | Loss: 0.5203\n",
            "Epoch 171/1000 | Loss: 0.5200\n",
            "Epoch 172/1000 | Loss: 0.5198\n",
            "Epoch 173/1000 | Loss: 0.5195\n",
            "Epoch 174/1000 | Loss: 0.5193\n",
            "Epoch 175/1000 | Loss: 0.5190\n",
            "Epoch 176/1000 | Loss: 0.5188\n",
            "Epoch 177/1000 | Loss: 0.5185\n",
            "Epoch 178/1000 | Loss: 0.5183\n",
            "Epoch 179/1000 | Loss: 0.5180\n",
            "Epoch 180/1000 | Loss: 0.5178\n",
            "Epoch 181/1000 | Loss: 0.5175\n",
            "Epoch 182/1000 | Loss: 0.5173\n",
            "Epoch 183/1000 | Loss: 0.5171\n",
            "Epoch 184/1000 | Loss: 0.5168\n",
            "Epoch 185/1000 | Loss: 0.5166\n",
            "Epoch 186/1000 | Loss: 0.5164\n",
            "Epoch 187/1000 | Loss: 0.5162\n",
            "Epoch 188/1000 | Loss: 0.5159\n",
            "Epoch 189/1000 | Loss: 0.5157\n",
            "Epoch 190/1000 | Loss: 0.5155\n",
            "Epoch 191/1000 | Loss: 0.5153\n",
            "Epoch 192/1000 | Loss: 0.5150\n",
            "Epoch 193/1000 | Loss: 0.5148\n",
            "Epoch 194/1000 | Loss: 0.5146\n",
            "Epoch 195/1000 | Loss: 0.5144\n",
            "Epoch 196/1000 | Loss: 0.5142\n",
            "Epoch 197/1000 | Loss: 0.5140\n",
            "Epoch 198/1000 | Loss: 0.5137\n",
            "Epoch 199/1000 | Loss: 0.5135\n",
            "Epoch 200/1000 | Loss: 0.5133\n",
            "Epoch 201/1000 | Loss: 0.5131\n",
            "Epoch 202/1000 | Loss: 0.5129\n",
            "Epoch 203/1000 | Loss: 0.5127\n",
            "Epoch 204/1000 | Loss: 0.5125\n",
            "Epoch 205/1000 | Loss: 0.5123\n",
            "Epoch 206/1000 | Loss: 0.5121\n",
            "Epoch 207/1000 | Loss: 0.5119\n",
            "Epoch 208/1000 | Loss: 0.5117\n",
            "Epoch 209/1000 | Loss: 0.5115\n",
            "Epoch 210/1000 | Loss: 0.5113\n",
            "Epoch 211/1000 | Loss: 0.5111\n",
            "Epoch 212/1000 | Loss: 0.5109\n",
            "Epoch 213/1000 | Loss: 0.5107\n",
            "Epoch 214/1000 | Loss: 0.5105\n",
            "Epoch 215/1000 | Loss: 0.5103\n",
            "Epoch 216/1000 | Loss: 0.5101\n",
            "Epoch 217/1000 | Loss: 0.5099\n",
            "Epoch 218/1000 | Loss: 0.5097\n",
            "Epoch 219/1000 | Loss: 0.5095\n",
            "Epoch 220/1000 | Loss: 0.5093\n",
            "Epoch 221/1000 | Loss: 0.5092\n",
            "Epoch 222/1000 | Loss: 0.5090\n",
            "Epoch 223/1000 | Loss: 0.5088\n",
            "Epoch 224/1000 | Loss: 0.5086\n",
            "Epoch 225/1000 | Loss: 0.5084\n",
            "Epoch 226/1000 | Loss: 0.5082\n",
            "Epoch 227/1000 | Loss: 0.5080\n",
            "Epoch 228/1000 | Loss: 0.5078\n",
            "Epoch 229/1000 | Loss: 0.5077\n",
            "Epoch 230/1000 | Loss: 0.5075\n",
            "Epoch 231/1000 | Loss: 0.5073\n",
            "Epoch 232/1000 | Loss: 0.5071\n",
            "Epoch 233/1000 | Loss: 0.5069\n",
            "Epoch 234/1000 | Loss: 0.5067\n",
            "Epoch 235/1000 | Loss: 0.5066\n",
            "Epoch 236/1000 | Loss: 0.5064\n",
            "Epoch 237/1000 | Loss: 0.5062\n",
            "Epoch 238/1000 | Loss: 0.5060\n",
            "Epoch 239/1000 | Loss: 0.5058\n",
            "Epoch 240/1000 | Loss: 0.5056\n",
            "Epoch 241/1000 | Loss: 0.5055\n",
            "Epoch 242/1000 | Loss: 0.5053\n",
            "Epoch 243/1000 | Loss: 0.5051\n",
            "Epoch 244/1000 | Loss: 0.5049\n",
            "Epoch 245/1000 | Loss: 0.5047\n",
            "Epoch 246/1000 | Loss: 0.5046\n",
            "Epoch 247/1000 | Loss: 0.5044\n",
            "Epoch 248/1000 | Loss: 0.5042\n",
            "Epoch 249/1000 | Loss: 0.5040\n",
            "Epoch 250/1000 | Loss: 0.5039\n",
            "Epoch 251/1000 | Loss: 0.5037\n",
            "Epoch 252/1000 | Loss: 0.5035\n",
            "Epoch 253/1000 | Loss: 0.5033\n",
            "Epoch 254/1000 | Loss: 0.5032\n",
            "Epoch 255/1000 | Loss: 0.5030\n",
            "Epoch 256/1000 | Loss: 0.5028\n",
            "Epoch 257/1000 | Loss: 0.5026\n",
            "Epoch 258/1000 | Loss: 0.5025\n",
            "Epoch 259/1000 | Loss: 0.5023\n",
            "Epoch 260/1000 | Loss: 0.5021\n",
            "Epoch 261/1000 | Loss: 0.5019\n",
            "Epoch 262/1000 | Loss: 0.5018\n",
            "Epoch 263/1000 | Loss: 0.5016\n",
            "Epoch 264/1000 | Loss: 0.5014\n",
            "Epoch 265/1000 | Loss: 0.5012\n",
            "Epoch 266/1000 | Loss: 0.5011\n",
            "Epoch 267/1000 | Loss: 0.5009\n",
            "Epoch 268/1000 | Loss: 0.5007\n",
            "Epoch 269/1000 | Loss: 0.5006\n",
            "Epoch 270/1000 | Loss: 0.5004\n",
            "Epoch 271/1000 | Loss: 0.5002\n",
            "Epoch 272/1000 | Loss: 0.5000\n",
            "Epoch 273/1000 | Loss: 0.4999\n",
            "Epoch 274/1000 | Loss: 0.4997\n",
            "Epoch 275/1000 | Loss: 0.4995\n",
            "Epoch 276/1000 | Loss: 0.4994\n",
            "Epoch 277/1000 | Loss: 0.4992\n",
            "Epoch 278/1000 | Loss: 0.4990\n",
            "Epoch 279/1000 | Loss: 0.4989\n",
            "Epoch 280/1000 | Loss: 0.4987\n",
            "Epoch 281/1000 | Loss: 0.4985\n",
            "Epoch 282/1000 | Loss: 0.4983\n",
            "Epoch 283/1000 | Loss: 0.4982\n",
            "Epoch 284/1000 | Loss: 0.4980\n",
            "Epoch 285/1000 | Loss: 0.4978\n",
            "Epoch 286/1000 | Loss: 0.4977\n",
            "Epoch 287/1000 | Loss: 0.4975\n",
            "Epoch 288/1000 | Loss: 0.4973\n",
            "Epoch 289/1000 | Loss: 0.4972\n",
            "Epoch 290/1000 | Loss: 0.4970\n",
            "Epoch 291/1000 | Loss: 0.4968\n",
            "Epoch 292/1000 | Loss: 0.4967\n",
            "Epoch 293/1000 | Loss: 0.4965\n",
            "Epoch 294/1000 | Loss: 0.4963\n",
            "Epoch 295/1000 | Loss: 0.4962\n",
            "Epoch 296/1000 | Loss: 0.4960\n",
            "Epoch 297/1000 | Loss: 0.4958\n",
            "Epoch 298/1000 | Loss: 0.4957\n",
            "Epoch 299/1000 | Loss: 0.4955\n",
            "Epoch 300/1000 | Loss: 0.4953\n",
            "Epoch 301/1000 | Loss: 0.4952\n",
            "Epoch 302/1000 | Loss: 0.4950\n",
            "Epoch 303/1000 | Loss: 0.4948\n",
            "Epoch 304/1000 | Loss: 0.4947\n",
            "Epoch 305/1000 | Loss: 0.4945\n",
            "Epoch 306/1000 | Loss: 0.4943\n",
            "Epoch 307/1000 | Loss: 0.4942\n",
            "Epoch 308/1000 | Loss: 0.4940\n",
            "Epoch 309/1000 | Loss: 0.4938\n",
            "Epoch 310/1000 | Loss: 0.4937\n",
            "Epoch 311/1000 | Loss: 0.4935\n",
            "Epoch 312/1000 | Loss: 0.4934\n",
            "Epoch 313/1000 | Loss: 0.4932\n",
            "Epoch 314/1000 | Loss: 0.4930\n",
            "Epoch 315/1000 | Loss: 0.4929\n",
            "Epoch 316/1000 | Loss: 0.4927\n",
            "Epoch 317/1000 | Loss: 0.4925\n",
            "Epoch 318/1000 | Loss: 0.4924\n",
            "Epoch 319/1000 | Loss: 0.4922\n",
            "Epoch 320/1000 | Loss: 0.4920\n",
            "Epoch 321/1000 | Loss: 0.4919\n",
            "Epoch 322/1000 | Loss: 0.4917\n",
            "Epoch 323/1000 | Loss: 0.4916\n",
            "Epoch 324/1000 | Loss: 0.4914\n",
            "Epoch 325/1000 | Loss: 0.4912\n",
            "Epoch 326/1000 | Loss: 0.4911\n",
            "Epoch 327/1000 | Loss: 0.4909\n",
            "Epoch 328/1000 | Loss: 0.4908\n",
            "Epoch 329/1000 | Loss: 0.4906\n",
            "Epoch 330/1000 | Loss: 0.4904\n",
            "Epoch 331/1000 | Loss: 0.4903\n",
            "Epoch 332/1000 | Loss: 0.4901\n",
            "Epoch 333/1000 | Loss: 0.4899\n",
            "Epoch 334/1000 | Loss: 0.4898\n",
            "Epoch 335/1000 | Loss: 0.4896\n",
            "Epoch 336/1000 | Loss: 0.4895\n",
            "Epoch 337/1000 | Loss: 0.4893\n",
            "Epoch 338/1000 | Loss: 0.4891\n",
            "Epoch 339/1000 | Loss: 0.4890\n",
            "Epoch 340/1000 | Loss: 0.4888\n",
            "Epoch 341/1000 | Loss: 0.4887\n",
            "Epoch 342/1000 | Loss: 0.4885\n",
            "Epoch 343/1000 | Loss: 0.4883\n",
            "Epoch 344/1000 | Loss: 0.4882\n",
            "Epoch 345/1000 | Loss: 0.4880\n",
            "Epoch 346/1000 | Loss: 0.4879\n",
            "Epoch 347/1000 | Loss: 0.4877\n",
            "Epoch 348/1000 | Loss: 0.4875\n",
            "Epoch 349/1000 | Loss: 0.4874\n",
            "Epoch 350/1000 | Loss: 0.4872\n",
            "Epoch 351/1000 | Loss: 0.4871\n",
            "Epoch 352/1000 | Loss: 0.4869\n",
            "Epoch 353/1000 | Loss: 0.4867\n",
            "Epoch 354/1000 | Loss: 0.4866\n",
            "Epoch 355/1000 | Loss: 0.4864\n",
            "Epoch 356/1000 | Loss: 0.4863\n",
            "Epoch 357/1000 | Loss: 0.4861\n",
            "Epoch 358/1000 | Loss: 0.4859\n",
            "Epoch 359/1000 | Loss: 0.4858\n",
            "Epoch 360/1000 | Loss: 0.4856\n",
            "Epoch 361/1000 | Loss: 0.4855\n",
            "Epoch 362/1000 | Loss: 0.4853\n",
            "Epoch 363/1000 | Loss: 0.4852\n",
            "Epoch 364/1000 | Loss: 0.4850\n",
            "Epoch 365/1000 | Loss: 0.4848\n",
            "Epoch 366/1000 | Loss: 0.4847\n",
            "Epoch 367/1000 | Loss: 0.4845\n",
            "Epoch 368/1000 | Loss: 0.4844\n",
            "Epoch 369/1000 | Loss: 0.4842\n",
            "Epoch 370/1000 | Loss: 0.4841\n",
            "Epoch 371/1000 | Loss: 0.4839\n",
            "Epoch 372/1000 | Loss: 0.4837\n",
            "Epoch 373/1000 | Loss: 0.4836\n",
            "Epoch 374/1000 | Loss: 0.4834\n",
            "Epoch 375/1000 | Loss: 0.4833\n",
            "Epoch 376/1000 | Loss: 0.4831\n",
            "Epoch 377/1000 | Loss: 0.4830\n",
            "Epoch 378/1000 | Loss: 0.4828\n",
            "Epoch 379/1000 | Loss: 0.4826\n",
            "Epoch 380/1000 | Loss: 0.4825\n",
            "Epoch 381/1000 | Loss: 0.4823\n",
            "Epoch 382/1000 | Loss: 0.4822\n",
            "Epoch 383/1000 | Loss: 0.4820\n",
            "Epoch 384/1000 | Loss: 0.4819\n",
            "Epoch 385/1000 | Loss: 0.4817\n",
            "Epoch 386/1000 | Loss: 0.4816\n",
            "Epoch 387/1000 | Loss: 0.4814\n",
            "Epoch 388/1000 | Loss: 0.4813\n",
            "Epoch 389/1000 | Loss: 0.4811\n",
            "Epoch 390/1000 | Loss: 0.4809\n",
            "Epoch 391/1000 | Loss: 0.4808\n",
            "Epoch 392/1000 | Loss: 0.4806\n",
            "Epoch 393/1000 | Loss: 0.4805\n",
            "Epoch 394/1000 | Loss: 0.4803\n",
            "Epoch 395/1000 | Loss: 0.4802\n",
            "Epoch 396/1000 | Loss: 0.4800\n",
            "Epoch 397/1000 | Loss: 0.4799\n",
            "Epoch 398/1000 | Loss: 0.4797\n",
            "Epoch 399/1000 | Loss: 0.4796\n",
            "Epoch 400/1000 | Loss: 0.4794\n",
            "Epoch 401/1000 | Loss: 0.4792\n",
            "Epoch 402/1000 | Loss: 0.4791\n",
            "Epoch 403/1000 | Loss: 0.4789\n",
            "Epoch 404/1000 | Loss: 0.4788\n",
            "Epoch 405/1000 | Loss: 0.4786\n",
            "Epoch 406/1000 | Loss: 0.4785\n",
            "Epoch 407/1000 | Loss: 0.4783\n",
            "Epoch 408/1000 | Loss: 0.4782\n",
            "Epoch 409/1000 | Loss: 0.4780\n",
            "Epoch 410/1000 | Loss: 0.4779\n",
            "Epoch 411/1000 | Loss: 0.4777\n",
            "Epoch 412/1000 | Loss: 0.4776\n",
            "Epoch 413/1000 | Loss: 0.4774\n",
            "Epoch 414/1000 | Loss: 0.4773\n",
            "Epoch 415/1000 | Loss: 0.4771\n",
            "Epoch 416/1000 | Loss: 0.4769\n",
            "Epoch 417/1000 | Loss: 0.4768\n",
            "Epoch 418/1000 | Loss: 0.4766\n",
            "Epoch 419/1000 | Loss: 0.4765\n",
            "Epoch 420/1000 | Loss: 0.4763\n",
            "Epoch 421/1000 | Loss: 0.4762\n",
            "Epoch 422/1000 | Loss: 0.4760\n",
            "Epoch 423/1000 | Loss: 0.4759\n",
            "Epoch 424/1000 | Loss: 0.4757\n",
            "Epoch 425/1000 | Loss: 0.4756\n",
            "Epoch 426/1000 | Loss: 0.4754\n",
            "Epoch 427/1000 | Loss: 0.4753\n",
            "Epoch 428/1000 | Loss: 0.4751\n",
            "Epoch 429/1000 | Loss: 0.4750\n",
            "Epoch 430/1000 | Loss: 0.4748\n",
            "Epoch 431/1000 | Loss: 0.4747\n",
            "Epoch 432/1000 | Loss: 0.4745\n",
            "Epoch 433/1000 | Loss: 0.4744\n",
            "Epoch 434/1000 | Loss: 0.4742\n",
            "Epoch 435/1000 | Loss: 0.4741\n",
            "Epoch 436/1000 | Loss: 0.4739\n",
            "Epoch 437/1000 | Loss: 0.4738\n",
            "Epoch 438/1000 | Loss: 0.4736\n",
            "Epoch 439/1000 | Loss: 0.4735\n",
            "Epoch 440/1000 | Loss: 0.4733\n",
            "Epoch 441/1000 | Loss: 0.4732\n",
            "Epoch 442/1000 | Loss: 0.4730\n",
            "Epoch 443/1000 | Loss: 0.4729\n",
            "Epoch 444/1000 | Loss: 0.4727\n",
            "Epoch 445/1000 | Loss: 0.4726\n",
            "Epoch 446/1000 | Loss: 0.4724\n",
            "Epoch 447/1000 | Loss: 0.4723\n",
            "Epoch 448/1000 | Loss: 0.4721\n",
            "Epoch 449/1000 | Loss: 0.4720\n",
            "Epoch 450/1000 | Loss: 0.4718\n",
            "Epoch 451/1000 | Loss: 0.4717\n",
            "Epoch 452/1000 | Loss: 0.4715\n",
            "Epoch 453/1000 | Loss: 0.4714\n",
            "Epoch 454/1000 | Loss: 0.4712\n",
            "Epoch 455/1000 | Loss: 0.4711\n",
            "Epoch 456/1000 | Loss: 0.4709\n",
            "Epoch 457/1000 | Loss: 0.4708\n",
            "Epoch 458/1000 | Loss: 0.4706\n",
            "Epoch 459/1000 | Loss: 0.4705\n",
            "Epoch 460/1000 | Loss: 0.4704\n",
            "Epoch 461/1000 | Loss: 0.4702\n",
            "Epoch 462/1000 | Loss: 0.4701\n",
            "Epoch 463/1000 | Loss: 0.4699\n",
            "Epoch 464/1000 | Loss: 0.4698\n",
            "Epoch 465/1000 | Loss: 0.4696\n",
            "Epoch 466/1000 | Loss: 0.4695\n",
            "Epoch 467/1000 | Loss: 0.4693\n",
            "Epoch 468/1000 | Loss: 0.4692\n",
            "Epoch 469/1000 | Loss: 0.4690\n",
            "Epoch 470/1000 | Loss: 0.4689\n",
            "Epoch 471/1000 | Loss: 0.4687\n",
            "Epoch 472/1000 | Loss: 0.4686\n",
            "Epoch 473/1000 | Loss: 0.4684\n",
            "Epoch 474/1000 | Loss: 0.4683\n",
            "Epoch 475/1000 | Loss: 0.4681\n",
            "Epoch 476/1000 | Loss: 0.4680\n",
            "Epoch 477/1000 | Loss: 0.4679\n",
            "Epoch 478/1000 | Loss: 0.4677\n",
            "Epoch 479/1000 | Loss: 0.4676\n",
            "Epoch 480/1000 | Loss: 0.4674\n",
            "Epoch 481/1000 | Loss: 0.4673\n",
            "Epoch 482/1000 | Loss: 0.4671\n",
            "Epoch 483/1000 | Loss: 0.4670\n",
            "Epoch 484/1000 | Loss: 0.4668\n",
            "Epoch 485/1000 | Loss: 0.4667\n",
            "Epoch 486/1000 | Loss: 0.4665\n",
            "Epoch 487/1000 | Loss: 0.4664\n",
            "Epoch 488/1000 | Loss: 0.4663\n",
            "Epoch 489/1000 | Loss: 0.4661\n",
            "Epoch 490/1000 | Loss: 0.4660\n",
            "Epoch 491/1000 | Loss: 0.4658\n",
            "Epoch 492/1000 | Loss: 0.4657\n",
            "Epoch 493/1000 | Loss: 0.4655\n",
            "Epoch 494/1000 | Loss: 0.4654\n",
            "Epoch 495/1000 | Loss: 0.4652\n",
            "Epoch 496/1000 | Loss: 0.4651\n",
            "Epoch 497/1000 | Loss: 0.4650\n",
            "Epoch 498/1000 | Loss: 0.4648\n",
            "Epoch 499/1000 | Loss: 0.4647\n",
            "Epoch 500/1000 | Loss: 0.4645\n",
            "Epoch 501/1000 | Loss: 0.4644\n",
            "Epoch 502/1000 | Loss: 0.4642\n",
            "Epoch 503/1000 | Loss: 0.4641\n",
            "Epoch 504/1000 | Loss: 0.4639\n",
            "Epoch 505/1000 | Loss: 0.4638\n",
            "Epoch 506/1000 | Loss: 0.4637\n",
            "Epoch 507/1000 | Loss: 0.4635\n",
            "Epoch 508/1000 | Loss: 0.4634\n",
            "Epoch 509/1000 | Loss: 0.4632\n",
            "Epoch 510/1000 | Loss: 0.4631\n",
            "Epoch 511/1000 | Loss: 0.4629\n",
            "Epoch 512/1000 | Loss: 0.4628\n",
            "Epoch 513/1000 | Loss: 0.4627\n",
            "Epoch 514/1000 | Loss: 0.4625\n",
            "Epoch 515/1000 | Loss: 0.4624\n",
            "Epoch 516/1000 | Loss: 0.4622\n",
            "Epoch 517/1000 | Loss: 0.4621\n",
            "Epoch 518/1000 | Loss: 0.4619\n",
            "Epoch 519/1000 | Loss: 0.4618\n",
            "Epoch 520/1000 | Loss: 0.4617\n",
            "Epoch 521/1000 | Loss: 0.4615\n",
            "Epoch 522/1000 | Loss: 0.4614\n",
            "Epoch 523/1000 | Loss: 0.4612\n",
            "Epoch 524/1000 | Loss: 0.4611\n",
            "Epoch 525/1000 | Loss: 0.4609\n",
            "Epoch 526/1000 | Loss: 0.4608\n",
            "Epoch 527/1000 | Loss: 0.4607\n",
            "Epoch 528/1000 | Loss: 0.4605\n",
            "Epoch 529/1000 | Loss: 0.4604\n",
            "Epoch 530/1000 | Loss: 0.4602\n",
            "Epoch 531/1000 | Loss: 0.4601\n",
            "Epoch 532/1000 | Loss: 0.4600\n",
            "Epoch 533/1000 | Loss: 0.4598\n",
            "Epoch 534/1000 | Loss: 0.4597\n",
            "Epoch 535/1000 | Loss: 0.4595\n",
            "Epoch 536/1000 | Loss: 0.4594\n",
            "Epoch 537/1000 | Loss: 0.4593\n",
            "Epoch 538/1000 | Loss: 0.4591\n",
            "Epoch 539/1000 | Loss: 0.4590\n",
            "Epoch 540/1000 | Loss: 0.4588\n",
            "Epoch 541/1000 | Loss: 0.4587\n",
            "Epoch 542/1000 | Loss: 0.4586\n",
            "Epoch 543/1000 | Loss: 0.4584\n",
            "Epoch 544/1000 | Loss: 0.4583\n",
            "Epoch 545/1000 | Loss: 0.4581\n",
            "Epoch 546/1000 | Loss: 0.4580\n",
            "Epoch 547/1000 | Loss: 0.4579\n",
            "Epoch 548/1000 | Loss: 0.4577\n",
            "Epoch 549/1000 | Loss: 0.4576\n",
            "Epoch 550/1000 | Loss: 0.4574\n",
            "Epoch 551/1000 | Loss: 0.4573\n",
            "Epoch 552/1000 | Loss: 0.4572\n",
            "Epoch 553/1000 | Loss: 0.4570\n",
            "Epoch 554/1000 | Loss: 0.4569\n",
            "Epoch 555/1000 | Loss: 0.4567\n",
            "Epoch 556/1000 | Loss: 0.4566\n",
            "Epoch 557/1000 | Loss: 0.4565\n",
            "Epoch 558/1000 | Loss: 0.4563\n",
            "Epoch 559/1000 | Loss: 0.4562\n",
            "Epoch 560/1000 | Loss: 0.4560\n",
            "Epoch 561/1000 | Loss: 0.4559\n",
            "Epoch 562/1000 | Loss: 0.4558\n",
            "Epoch 563/1000 | Loss: 0.4556\n",
            "Epoch 564/1000 | Loss: 0.4555\n",
            "Epoch 565/1000 | Loss: 0.4554\n",
            "Epoch 566/1000 | Loss: 0.4552\n",
            "Epoch 567/1000 | Loss: 0.4551\n",
            "Epoch 568/1000 | Loss: 0.4549\n",
            "Epoch 569/1000 | Loss: 0.4548\n",
            "Epoch 570/1000 | Loss: 0.4547\n",
            "Epoch 571/1000 | Loss: 0.4545\n",
            "Epoch 572/1000 | Loss: 0.4544\n",
            "Epoch 573/1000 | Loss: 0.4543\n",
            "Epoch 574/1000 | Loss: 0.4541\n",
            "Epoch 575/1000 | Loss: 0.4540\n",
            "Epoch 576/1000 | Loss: 0.4538\n",
            "Epoch 577/1000 | Loss: 0.4537\n",
            "Epoch 578/1000 | Loss: 0.4536\n",
            "Epoch 579/1000 | Loss: 0.4534\n",
            "Epoch 580/1000 | Loss: 0.4533\n",
            "Epoch 581/1000 | Loss: 0.4532\n",
            "Epoch 582/1000 | Loss: 0.4530\n",
            "Epoch 583/1000 | Loss: 0.4529\n",
            "Epoch 584/1000 | Loss: 0.4528\n",
            "Epoch 585/1000 | Loss: 0.4526\n",
            "Epoch 586/1000 | Loss: 0.4525\n",
            "Epoch 587/1000 | Loss: 0.4523\n",
            "Epoch 588/1000 | Loss: 0.4522\n",
            "Epoch 589/1000 | Loss: 0.4521\n",
            "Epoch 590/1000 | Loss: 0.4519\n",
            "Epoch 591/1000 | Loss: 0.4518\n",
            "Epoch 592/1000 | Loss: 0.4517\n",
            "Epoch 593/1000 | Loss: 0.4515\n",
            "Epoch 594/1000 | Loss: 0.4514\n",
            "Epoch 595/1000 | Loss: 0.4513\n",
            "Epoch 596/1000 | Loss: 0.4511\n",
            "Epoch 597/1000 | Loss: 0.4510\n",
            "Epoch 598/1000 | Loss: 0.4509\n",
            "Epoch 599/1000 | Loss: 0.4507\n",
            "Epoch 600/1000 | Loss: 0.4506\n",
            "Epoch 601/1000 | Loss: 0.4504\n",
            "Epoch 602/1000 | Loss: 0.4503\n",
            "Epoch 603/1000 | Loss: 0.4502\n",
            "Epoch 604/1000 | Loss: 0.4500\n",
            "Epoch 605/1000 | Loss: 0.4499\n",
            "Epoch 606/1000 | Loss: 0.4498\n",
            "Epoch 607/1000 | Loss: 0.4496\n",
            "Epoch 608/1000 | Loss: 0.4495\n",
            "Epoch 609/1000 | Loss: 0.4494\n",
            "Epoch 610/1000 | Loss: 0.4492\n",
            "Epoch 611/1000 | Loss: 0.4491\n",
            "Epoch 612/1000 | Loss: 0.4490\n",
            "Epoch 613/1000 | Loss: 0.4488\n",
            "Epoch 614/1000 | Loss: 0.4487\n",
            "Epoch 615/1000 | Loss: 0.4486\n",
            "Epoch 616/1000 | Loss: 0.4484\n",
            "Epoch 617/1000 | Loss: 0.4483\n",
            "Epoch 618/1000 | Loss: 0.4482\n",
            "Epoch 619/1000 | Loss: 0.4480\n",
            "Epoch 620/1000 | Loss: 0.4479\n",
            "Epoch 621/1000 | Loss: 0.4478\n",
            "Epoch 622/1000 | Loss: 0.4476\n",
            "Epoch 623/1000 | Loss: 0.4475\n",
            "Epoch 624/1000 | Loss: 0.4474\n",
            "Epoch 625/1000 | Loss: 0.4472\n",
            "Epoch 626/1000 | Loss: 0.4471\n",
            "Epoch 627/1000 | Loss: 0.4470\n",
            "Epoch 628/1000 | Loss: 0.4468\n",
            "Epoch 629/1000 | Loss: 0.4467\n",
            "Epoch 630/1000 | Loss: 0.4466\n",
            "Epoch 631/1000 | Loss: 0.4464\n",
            "Epoch 632/1000 | Loss: 0.4463\n",
            "Epoch 633/1000 | Loss: 0.4462\n",
            "Epoch 634/1000 | Loss: 0.4460\n",
            "Epoch 635/1000 | Loss: 0.4459\n",
            "Epoch 636/1000 | Loss: 0.4458\n",
            "Epoch 637/1000 | Loss: 0.4456\n",
            "Epoch 638/1000 | Loss: 0.4455\n",
            "Epoch 639/1000 | Loss: 0.4454\n",
            "Epoch 640/1000 | Loss: 0.4453\n",
            "Epoch 641/1000 | Loss: 0.4451\n",
            "Epoch 642/1000 | Loss: 0.4450\n",
            "Epoch 643/1000 | Loss: 0.4449\n",
            "Epoch 644/1000 | Loss: 0.4447\n",
            "Epoch 645/1000 | Loss: 0.4446\n",
            "Epoch 646/1000 | Loss: 0.4445\n",
            "Epoch 647/1000 | Loss: 0.4443\n",
            "Epoch 648/1000 | Loss: 0.4442\n",
            "Epoch 649/1000 | Loss: 0.4441\n",
            "Epoch 650/1000 | Loss: 0.4439\n",
            "Epoch 651/1000 | Loss: 0.4438\n",
            "Epoch 652/1000 | Loss: 0.4437\n",
            "Epoch 653/1000 | Loss: 0.4436\n",
            "Epoch 654/1000 | Loss: 0.4434\n",
            "Epoch 655/1000 | Loss: 0.4433\n",
            "Epoch 656/1000 | Loss: 0.4432\n",
            "Epoch 657/1000 | Loss: 0.4430\n",
            "Epoch 658/1000 | Loss: 0.4429\n",
            "Epoch 659/1000 | Loss: 0.4428\n",
            "Epoch 660/1000 | Loss: 0.4426\n",
            "Epoch 661/1000 | Loss: 0.4425\n",
            "Epoch 662/1000 | Loss: 0.4424\n",
            "Epoch 663/1000 | Loss: 0.4423\n",
            "Epoch 664/1000 | Loss: 0.4421\n",
            "Epoch 665/1000 | Loss: 0.4420\n",
            "Epoch 666/1000 | Loss: 0.4419\n",
            "Epoch 667/1000 | Loss: 0.4417\n",
            "Epoch 668/1000 | Loss: 0.4416\n",
            "Epoch 669/1000 | Loss: 0.4415\n",
            "Epoch 670/1000 | Loss: 0.4413\n",
            "Epoch 671/1000 | Loss: 0.4412\n",
            "Epoch 672/1000 | Loss: 0.4411\n",
            "Epoch 673/1000 | Loss: 0.4410\n",
            "Epoch 674/1000 | Loss: 0.4408\n",
            "Epoch 675/1000 | Loss: 0.4407\n",
            "Epoch 676/1000 | Loss: 0.4406\n",
            "Epoch 677/1000 | Loss: 0.4404\n",
            "Epoch 678/1000 | Loss: 0.4403\n",
            "Epoch 679/1000 | Loss: 0.4402\n",
            "Epoch 680/1000 | Loss: 0.4401\n",
            "Epoch 681/1000 | Loss: 0.4399\n",
            "Epoch 682/1000 | Loss: 0.4398\n",
            "Epoch 683/1000 | Loss: 0.4397\n",
            "Epoch 684/1000 | Loss: 0.4395\n",
            "Epoch 685/1000 | Loss: 0.4394\n",
            "Epoch 686/1000 | Loss: 0.4393\n",
            "Epoch 687/1000 | Loss: 0.4392\n",
            "Epoch 688/1000 | Loss: 0.4390\n",
            "Epoch 689/1000 | Loss: 0.4389\n",
            "Epoch 690/1000 | Loss: 0.4388\n",
            "Epoch 691/1000 | Loss: 0.4387\n",
            "Epoch 692/1000 | Loss: 0.4385\n",
            "Epoch 693/1000 | Loss: 0.4384\n",
            "Epoch 694/1000 | Loss: 0.4383\n",
            "Epoch 695/1000 | Loss: 0.4381\n",
            "Epoch 696/1000 | Loss: 0.4380\n",
            "Epoch 697/1000 | Loss: 0.4379\n",
            "Epoch 698/1000 | Loss: 0.4378\n",
            "Epoch 699/1000 | Loss: 0.4376\n",
            "Epoch 700/1000 | Loss: 0.4375\n",
            "Epoch 701/1000 | Loss: 0.4374\n",
            "Epoch 702/1000 | Loss: 0.4373\n",
            "Epoch 703/1000 | Loss: 0.4371\n",
            "Epoch 704/1000 | Loss: 0.4370\n",
            "Epoch 705/1000 | Loss: 0.4369\n",
            "Epoch 706/1000 | Loss: 0.4368\n",
            "Epoch 707/1000 | Loss: 0.4366\n",
            "Epoch 708/1000 | Loss: 0.4365\n",
            "Epoch 709/1000 | Loss: 0.4364\n",
            "Epoch 710/1000 | Loss: 0.4363\n",
            "Epoch 711/1000 | Loss: 0.4361\n",
            "Epoch 712/1000 | Loss: 0.4360\n",
            "Epoch 713/1000 | Loss: 0.4359\n",
            "Epoch 714/1000 | Loss: 0.4357\n",
            "Epoch 715/1000 | Loss: 0.4356\n",
            "Epoch 716/1000 | Loss: 0.4355\n",
            "Epoch 717/1000 | Loss: 0.4354\n",
            "Epoch 718/1000 | Loss: 0.4352\n",
            "Epoch 719/1000 | Loss: 0.4351\n",
            "Epoch 720/1000 | Loss: 0.4350\n",
            "Epoch 721/1000 | Loss: 0.4349\n",
            "Epoch 722/1000 | Loss: 0.4347\n",
            "Epoch 723/1000 | Loss: 0.4346\n",
            "Epoch 724/1000 | Loss: 0.4345\n",
            "Epoch 725/1000 | Loss: 0.4344\n",
            "Epoch 726/1000 | Loss: 0.4342\n",
            "Epoch 727/1000 | Loss: 0.4341\n",
            "Epoch 728/1000 | Loss: 0.4340\n",
            "Epoch 729/1000 | Loss: 0.4339\n",
            "Epoch 730/1000 | Loss: 0.4338\n",
            "Epoch 731/1000 | Loss: 0.4336\n",
            "Epoch 732/1000 | Loss: 0.4335\n",
            "Epoch 733/1000 | Loss: 0.4334\n",
            "Epoch 734/1000 | Loss: 0.4333\n",
            "Epoch 735/1000 | Loss: 0.4331\n",
            "Epoch 736/1000 | Loss: 0.4330\n",
            "Epoch 737/1000 | Loss: 0.4329\n",
            "Epoch 738/1000 | Loss: 0.4328\n",
            "Epoch 739/1000 | Loss: 0.4326\n",
            "Epoch 740/1000 | Loss: 0.4325\n",
            "Epoch 741/1000 | Loss: 0.4324\n",
            "Epoch 742/1000 | Loss: 0.4323\n",
            "Epoch 743/1000 | Loss: 0.4321\n",
            "Epoch 744/1000 | Loss: 0.4320\n",
            "Epoch 745/1000 | Loss: 0.4319\n",
            "Epoch 746/1000 | Loss: 0.4318\n",
            "Epoch 747/1000 | Loss: 0.4316\n",
            "Epoch 748/1000 | Loss: 0.4315\n",
            "Epoch 749/1000 | Loss: 0.4314\n",
            "Epoch 750/1000 | Loss: 0.4313\n",
            "Epoch 751/1000 | Loss: 0.4312\n",
            "Epoch 752/1000 | Loss: 0.4310\n",
            "Epoch 753/1000 | Loss: 0.4309\n",
            "Epoch 754/1000 | Loss: 0.4308\n",
            "Epoch 755/1000 | Loss: 0.4307\n",
            "Epoch 756/1000 | Loss: 0.4305\n",
            "Epoch 757/1000 | Loss: 0.4304\n",
            "Epoch 758/1000 | Loss: 0.4303\n",
            "Epoch 759/1000 | Loss: 0.4302\n",
            "Epoch 760/1000 | Loss: 0.4301\n",
            "Epoch 761/1000 | Loss: 0.4299\n",
            "Epoch 762/1000 | Loss: 0.4298\n",
            "Epoch 763/1000 | Loss: 0.4297\n",
            "Epoch 764/1000 | Loss: 0.4296\n",
            "Epoch 765/1000 | Loss: 0.4294\n",
            "Epoch 766/1000 | Loss: 0.4293\n",
            "Epoch 767/1000 | Loss: 0.4292\n",
            "Epoch 768/1000 | Loss: 0.4291\n",
            "Epoch 769/1000 | Loss: 0.4290\n",
            "Epoch 770/1000 | Loss: 0.4288\n",
            "Epoch 771/1000 | Loss: 0.4287\n",
            "Epoch 772/1000 | Loss: 0.4286\n",
            "Epoch 773/1000 | Loss: 0.4285\n",
            "Epoch 774/1000 | Loss: 0.4284\n",
            "Epoch 775/1000 | Loss: 0.4282\n",
            "Epoch 776/1000 | Loss: 0.4281\n",
            "Epoch 777/1000 | Loss: 0.4280\n",
            "Epoch 778/1000 | Loss: 0.4279\n",
            "Epoch 779/1000 | Loss: 0.4278\n",
            "Epoch 780/1000 | Loss: 0.4276\n",
            "Epoch 781/1000 | Loss: 0.4275\n",
            "Epoch 782/1000 | Loss: 0.4274\n",
            "Epoch 783/1000 | Loss: 0.4273\n",
            "Epoch 784/1000 | Loss: 0.4272\n",
            "Epoch 785/1000 | Loss: 0.4270\n",
            "Epoch 786/1000 | Loss: 0.4269\n",
            "Epoch 787/1000 | Loss: 0.4268\n",
            "Epoch 788/1000 | Loss: 0.4267\n",
            "Epoch 789/1000 | Loss: 0.4266\n",
            "Epoch 790/1000 | Loss: 0.4264\n",
            "Epoch 791/1000 | Loss: 0.4263\n",
            "Epoch 792/1000 | Loss: 0.4262\n",
            "Epoch 793/1000 | Loss: 0.4261\n",
            "Epoch 794/1000 | Loss: 0.4260\n",
            "Epoch 795/1000 | Loss: 0.4258\n",
            "Epoch 796/1000 | Loss: 0.4257\n",
            "Epoch 797/1000 | Loss: 0.4256\n",
            "Epoch 798/1000 | Loss: 0.4255\n",
            "Epoch 799/1000 | Loss: 0.4254\n",
            "Epoch 800/1000 | Loss: 0.4252\n",
            "Epoch 801/1000 | Loss: 0.4251\n",
            "Epoch 802/1000 | Loss: 0.4250\n",
            "Epoch 803/1000 | Loss: 0.4249\n",
            "Epoch 804/1000 | Loss: 0.4248\n",
            "Epoch 805/1000 | Loss: 0.4246\n",
            "Epoch 806/1000 | Loss: 0.4245\n",
            "Epoch 807/1000 | Loss: 0.4244\n",
            "Epoch 808/1000 | Loss: 0.4243\n",
            "Epoch 809/1000 | Loss: 0.4242\n",
            "Epoch 810/1000 | Loss: 0.4241\n",
            "Epoch 811/1000 | Loss: 0.4239\n",
            "Epoch 812/1000 | Loss: 0.4238\n",
            "Epoch 813/1000 | Loss: 0.4237\n",
            "Epoch 814/1000 | Loss: 0.4236\n",
            "Epoch 815/1000 | Loss: 0.4235\n",
            "Epoch 816/1000 | Loss: 0.4233\n",
            "Epoch 817/1000 | Loss: 0.4232\n",
            "Epoch 818/1000 | Loss: 0.4231\n",
            "Epoch 819/1000 | Loss: 0.4230\n",
            "Epoch 820/1000 | Loss: 0.4229\n",
            "Epoch 821/1000 | Loss: 0.4228\n",
            "Epoch 822/1000 | Loss: 0.4226\n",
            "Epoch 823/1000 | Loss: 0.4225\n",
            "Epoch 824/1000 | Loss: 0.4224\n",
            "Epoch 825/1000 | Loss: 0.4223\n",
            "Epoch 826/1000 | Loss: 0.4222\n",
            "Epoch 827/1000 | Loss: 0.4221\n",
            "Epoch 828/1000 | Loss: 0.4219\n",
            "Epoch 829/1000 | Loss: 0.4218\n",
            "Epoch 830/1000 | Loss: 0.4217\n",
            "Epoch 831/1000 | Loss: 0.4216\n",
            "Epoch 832/1000 | Loss: 0.4215\n",
            "Epoch 833/1000 | Loss: 0.4214\n",
            "Epoch 834/1000 | Loss: 0.4212\n",
            "Epoch 835/1000 | Loss: 0.4211\n",
            "Epoch 836/1000 | Loss: 0.4210\n",
            "Epoch 837/1000 | Loss: 0.4209\n",
            "Epoch 838/1000 | Loss: 0.4208\n",
            "Epoch 839/1000 | Loss: 0.4207\n",
            "Epoch 840/1000 | Loss: 0.4205\n",
            "Epoch 841/1000 | Loss: 0.4204\n",
            "Epoch 842/1000 | Loss: 0.4203\n",
            "Epoch 843/1000 | Loss: 0.4202\n",
            "Epoch 844/1000 | Loss: 0.4201\n",
            "Epoch 845/1000 | Loss: 0.4200\n",
            "Epoch 846/1000 | Loss: 0.4198\n",
            "Epoch 847/1000 | Loss: 0.4197\n",
            "Epoch 848/1000 | Loss: 0.4196\n",
            "Epoch 849/1000 | Loss: 0.4195\n",
            "Epoch 850/1000 | Loss: 0.4194\n",
            "Epoch 851/1000 | Loss: 0.4193\n",
            "Epoch 852/1000 | Loss: 0.4192\n",
            "Epoch 853/1000 | Loss: 0.4190\n",
            "Epoch 854/1000 | Loss: 0.4189\n",
            "Epoch 855/1000 | Loss: 0.4188\n",
            "Epoch 856/1000 | Loss: 0.4187\n",
            "Epoch 857/1000 | Loss: 0.4186\n",
            "Epoch 858/1000 | Loss: 0.4185\n",
            "Epoch 859/1000 | Loss: 0.4183\n",
            "Epoch 860/1000 | Loss: 0.4182\n",
            "Epoch 861/1000 | Loss: 0.4181\n",
            "Epoch 862/1000 | Loss: 0.4180\n",
            "Epoch 863/1000 | Loss: 0.4179\n",
            "Epoch 864/1000 | Loss: 0.4178\n",
            "Epoch 865/1000 | Loss: 0.4177\n",
            "Epoch 866/1000 | Loss: 0.4175\n",
            "Epoch 867/1000 | Loss: 0.4174\n",
            "Epoch 868/1000 | Loss: 0.4173\n",
            "Epoch 869/1000 | Loss: 0.4172\n",
            "Epoch 870/1000 | Loss: 0.4171\n",
            "Epoch 871/1000 | Loss: 0.4170\n",
            "Epoch 872/1000 | Loss: 0.4169\n",
            "Epoch 873/1000 | Loss: 0.4168\n",
            "Epoch 874/1000 | Loss: 0.4166\n",
            "Epoch 875/1000 | Loss: 0.4165\n",
            "Epoch 876/1000 | Loss: 0.4164\n",
            "Epoch 877/1000 | Loss: 0.4163\n",
            "Epoch 878/1000 | Loss: 0.4162\n",
            "Epoch 879/1000 | Loss: 0.4161\n",
            "Epoch 880/1000 | Loss: 0.4160\n",
            "Epoch 881/1000 | Loss: 0.4158\n",
            "Epoch 882/1000 | Loss: 0.4157\n",
            "Epoch 883/1000 | Loss: 0.4156\n",
            "Epoch 884/1000 | Loss: 0.4155\n",
            "Epoch 885/1000 | Loss: 0.4154\n",
            "Epoch 886/1000 | Loss: 0.4153\n",
            "Epoch 887/1000 | Loss: 0.4152\n",
            "Epoch 888/1000 | Loss: 0.4151\n",
            "Epoch 889/1000 | Loss: 0.4149\n",
            "Epoch 890/1000 | Loss: 0.4148\n",
            "Epoch 891/1000 | Loss: 0.4147\n",
            "Epoch 892/1000 | Loss: 0.4146\n",
            "Epoch 893/1000 | Loss: 0.4145\n",
            "Epoch 894/1000 | Loss: 0.4144\n",
            "Epoch 895/1000 | Loss: 0.4143\n",
            "Epoch 896/1000 | Loss: 0.4142\n",
            "Epoch 897/1000 | Loss: 0.4140\n",
            "Epoch 898/1000 | Loss: 0.4139\n",
            "Epoch 899/1000 | Loss: 0.4138\n",
            "Epoch 900/1000 | Loss: 0.4137\n",
            "Epoch 901/1000 | Loss: 0.4136\n",
            "Epoch 902/1000 | Loss: 0.4135\n",
            "Epoch 903/1000 | Loss: 0.4134\n",
            "Epoch 904/1000 | Loss: 0.4133\n",
            "Epoch 905/1000 | Loss: 0.4131\n",
            "Epoch 906/1000 | Loss: 0.4130\n",
            "Epoch 907/1000 | Loss: 0.4129\n",
            "Epoch 908/1000 | Loss: 0.4128\n",
            "Epoch 909/1000 | Loss: 0.4127\n",
            "Epoch 910/1000 | Loss: 0.4126\n",
            "Epoch 911/1000 | Loss: 0.4125\n",
            "Epoch 912/1000 | Loss: 0.4124\n",
            "Epoch 913/1000 | Loss: 0.4123\n",
            "Epoch 914/1000 | Loss: 0.4121\n",
            "Epoch 915/1000 | Loss: 0.4120\n",
            "Epoch 916/1000 | Loss: 0.4119\n",
            "Epoch 917/1000 | Loss: 0.4118\n",
            "Epoch 918/1000 | Loss: 0.4117\n",
            "Epoch 919/1000 | Loss: 0.4116\n",
            "Epoch 920/1000 | Loss: 0.4115\n",
            "Epoch 921/1000 | Loss: 0.4114\n",
            "Epoch 922/1000 | Loss: 0.4113\n",
            "Epoch 923/1000 | Loss: 0.4112\n",
            "Epoch 924/1000 | Loss: 0.4110\n",
            "Epoch 925/1000 | Loss: 0.4109\n",
            "Epoch 926/1000 | Loss: 0.4108\n",
            "Epoch 927/1000 | Loss: 0.4107\n",
            "Epoch 928/1000 | Loss: 0.4106\n",
            "Epoch 929/1000 | Loss: 0.4105\n",
            "Epoch 930/1000 | Loss: 0.4104\n",
            "Epoch 931/1000 | Loss: 0.4103\n",
            "Epoch 932/1000 | Loss: 0.4102\n",
            "Epoch 933/1000 | Loss: 0.4101\n",
            "Epoch 934/1000 | Loss: 0.4099\n",
            "Epoch 935/1000 | Loss: 0.4098\n",
            "Epoch 936/1000 | Loss: 0.4097\n",
            "Epoch 937/1000 | Loss: 0.4096\n",
            "Epoch 938/1000 | Loss: 0.4095\n",
            "Epoch 939/1000 | Loss: 0.4094\n",
            "Epoch 940/1000 | Loss: 0.4093\n",
            "Epoch 941/1000 | Loss: 0.4092\n",
            "Epoch 942/1000 | Loss: 0.4091\n",
            "Epoch 943/1000 | Loss: 0.4090\n",
            "Epoch 944/1000 | Loss: 0.4088\n",
            "Epoch 945/1000 | Loss: 0.4087\n",
            "Epoch 946/1000 | Loss: 0.4086\n",
            "Epoch 947/1000 | Loss: 0.4085\n",
            "Epoch 948/1000 | Loss: 0.4084\n",
            "Epoch 949/1000 | Loss: 0.4083\n",
            "Epoch 950/1000 | Loss: 0.4082\n",
            "Epoch 951/1000 | Loss: 0.4081\n",
            "Epoch 952/1000 | Loss: 0.4080\n",
            "Epoch 953/1000 | Loss: 0.4079\n",
            "Epoch 954/1000 | Loss: 0.4078\n",
            "Epoch 955/1000 | Loss: 0.4077\n",
            "Epoch 956/1000 | Loss: 0.4075\n",
            "Epoch 957/1000 | Loss: 0.4074\n",
            "Epoch 958/1000 | Loss: 0.4073\n",
            "Epoch 959/1000 | Loss: 0.4072\n",
            "Epoch 960/1000 | Loss: 0.4071\n",
            "Epoch 961/1000 | Loss: 0.4070\n",
            "Epoch 962/1000 | Loss: 0.4069\n",
            "Epoch 963/1000 | Loss: 0.4068\n",
            "Epoch 964/1000 | Loss: 0.4067\n",
            "Epoch 965/1000 | Loss: 0.4066\n",
            "Epoch 966/1000 | Loss: 0.4065\n",
            "Epoch 967/1000 | Loss: 0.4064\n",
            "Epoch 968/1000 | Loss: 0.4063\n",
            "Epoch 969/1000 | Loss: 0.4061\n",
            "Epoch 970/1000 | Loss: 0.4060\n",
            "Epoch 971/1000 | Loss: 0.4059\n",
            "Epoch 972/1000 | Loss: 0.4058\n",
            "Epoch 973/1000 | Loss: 0.4057\n",
            "Epoch 974/1000 | Loss: 0.4056\n",
            "Epoch 975/1000 | Loss: 0.4055\n",
            "Epoch 976/1000 | Loss: 0.4054\n",
            "Epoch 977/1000 | Loss: 0.4053\n",
            "Epoch 978/1000 | Loss: 0.4052\n",
            "Epoch 979/1000 | Loss: 0.4051\n",
            "Epoch 980/1000 | Loss: 0.4050\n",
            "Epoch 981/1000 | Loss: 0.4049\n",
            "Epoch 982/1000 | Loss: 0.4048\n",
            "Epoch 983/1000 | Loss: 0.4046\n",
            "Epoch 984/1000 | Loss: 0.4045\n",
            "Epoch 985/1000 | Loss: 0.4044\n",
            "Epoch 986/1000 | Loss: 0.4043\n",
            "Epoch 987/1000 | Loss: 0.4042\n",
            "Epoch 988/1000 | Loss: 0.4041\n",
            "Epoch 989/1000 | Loss: 0.4040\n",
            "Epoch 990/1000 | Loss: 0.4039\n",
            "Epoch 991/1000 | Loss: 0.4038\n",
            "Epoch 992/1000 | Loss: 0.4037\n",
            "Epoch 993/1000 | Loss: 0.4036\n",
            "Epoch 994/1000 | Loss: 0.4035\n",
            "Epoch 995/1000 | Loss: 0.4034\n",
            "Epoch 996/1000 | Loss: 0.4033\n",
            "Epoch 997/1000 | Loss: 0.4032\n",
            "Epoch 998/1000 | Loss: 0.4031\n",
            "Epoch 999/1000 | Loss: 0.4030\n",
            "Epoch 1000/1000 | Loss: 0.4028\n",
            "\n",
            "Let's predict the hours need to score above 50%\n",
            "==================================================\n",
            "Prediction after 1 hour of training: 0.3155 | Above 50%: False\n",
            "Prediction after 7 hours of training: 0.9853 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax\n"
      ],
      "metadata": {
        "id": "Twe-cP48R9hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit\n",
        "import optax\n",
        "\n",
        "x_data = jnp.array([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = jnp.array([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "# defind model\n",
        "def model(params, x): # 이 부분에서, init_params 에서 받은 값들을\n",
        "  return jax.nn.sigmoid(jnp.dot(x, params['weight']) + params['bias'])\n",
        "\n",
        "def loss(params, x, y):\n",
        "  y_pred = model(params, x)\n",
        "  return optax.sigmoid_binary_cross_entropy(y_pred, y).mean()\n",
        "\n",
        "# value and gradient computation\n",
        "\n",
        "\n",
        "# Initialize parameters\n",
        "key = jax.random.PRNGKey(0)  # Define random key\n",
        "init_params = {\n",
        "    'weight': jax.random.normal(key, (1, 1)),\n",
        "    'bias': jax.random.normal(key, (1,))\n",
        "}\n",
        "optimizer = optax.sgd(learning_rate = 0.01)\n",
        "# optax.sgd 를 활용해 optimizer 를 초기화 할때, params 가 jax 배열이 아니기때문에 오류가 발생\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "num_epochs = 1000\n",
        "params = init_params\n",
        "\n",
        "@jit\n",
        "# def value_and_grad(params, x, y):\n",
        "#     return jax.value_and_grad(loss)(params, x, y)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # jit으로 선언한 함수를 사용할 떄\n",
        "  # loss_val, grads = value_and_grad(params,x_data, y_data)\n",
        "\n",
        "  loss_val, grads = jax.value_and_grad(loss)(params,x_data, y_data)\n",
        "  #print(params)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  #print(params)\n",
        "  print(f'Epoch {epoch + 1}/{num_epochs} | loss : {loss_val}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "HoffS53dIqqF",
        "outputId": "0f151738-1104-452c-91f0-e8bcae03cfd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-80-cb8e6c0d82b8>, line 37)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-80-cb8e6c0d82b8>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    for epoch in range(num_epochs):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07_diabets_logistic.py"
      ],
      "metadata": {
        "id": "N_gPZ2TogL4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "\n",
        "xy = np.loadtxt('/content/drive/MyDrive/google_jax/data/diabetes.csv.gz', delimiter=',', dtype = np.float32)\n",
        "x_data = from_numpy(xy[:, 0: -1])\n",
        "y_data = from_numpy(xy[:, [-1]])\n",
        "\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mATnN4tuc9TC",
        "outputId": "cf194cf0-7950-4152-94bf-fe2308447fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import jax\n",
        "\n",
        "# define model using flax\n",
        "import flax.linen as nn\n",
        "\n",
        "@nn.compact\n",
        "class Model(nn.Module):\n",
        "    def setup(self):\n",
        "        self.l1 = nn.Dense(6)\n",
        "        self.l2 = nn.Dense(4)\n",
        "        self.l3 = nn.Dense(1)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        out1 = nn.sigmoid(self.l1(x))\n",
        "        out2 = nn.sigmoid(self.l2(out1))\n",
        "        y_pred = nn.sigmoid(self.l3(out2))\n",
        "        return y_pred\n",
        "\n",
        "model = Model()\n"
      ],
      "metadata": {
        "id": "hO19ys8T867l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "vLCYY0Ys9bL-",
        "outputId": "760a63bb-6ee0-4c7a-ad08-a6e8fcbecbc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model()"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function\n",
        "def loss_fn(params, model, batch):\n",
        "    inputs, targets = batch\n",
        "    logits = model.apply({\"params\": params}, inputs)\n",
        "    return jnp.mean(jax.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=targets))\n",
        "\n",
        "# Initialize model parameters\n",
        "rng = jax.random.PRNGKey(0)\n",
        "params = model.init(rng, jnp.ones((1, 8), dtype=jnp.float32))"
      ],
      "metadata": {
        "id": "b1FVrlKb9eid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "id": "8NMhOjVj9frb",
        "outputId": "ad8af015-04f0-4357-ac85-f946f4884f00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    params: {\n",
              "        l1: {\n",
              "            kernel: DeviceArray([[-0.36620265, -0.28757483,  0.5535811 , -0.13558859,\n",
              "                          -0.41631603,  0.54938847],\n",
              "                         [ 0.4399298 ,  0.3729127 ,  0.47254488,  0.08311932,\n",
              "                           0.20160902, -0.03585215],\n",
              "                         [-0.4550131 , -0.16402927,  0.46358654, -0.2197457 ,\n",
              "                          -0.07501189,  0.10366119],\n",
              "                         [ 0.554131  ,  0.6856542 ,  0.40352252, -0.44767907,\n",
              "                           0.16142297,  0.23764141],\n",
              "                         [ 0.26231918, -0.03894472, -0.42013368,  0.1577048 ,\n",
              "                          -0.320385  , -0.25536203],\n",
              "                         [ 0.29287338, -0.50183195,  0.07120508,  0.25253683,\n",
              "                          -0.32684764,  0.4308775 ],\n",
              "                         [ 0.35276017,  0.15336749,  0.4077243 ,  0.49453962,\n",
              "                          -0.33912134, -0.09362802],\n",
              "                         [-0.0650667 , -0.45902833,  0.76266474,  0.03806336,\n",
              "                          -0.10498474,  0.25202912]], dtype=float32),\n",
              "            bias: DeviceArray([0., 0., 0., 0., 0., 0.], dtype=float32),\n",
              "        },\n",
              "        l2: {\n",
              "            kernel: DeviceArray([[-0.22741623,  0.01351821,  0.26589125, -0.19545685],\n",
              "                         [ 0.27252594, -0.476836  ,  0.07989179, -0.16153733],\n",
              "                         [-0.44305292, -0.08097568, -0.73746544, -0.3998755 ],\n",
              "                         [ 0.15250306,  0.5116453 ,  0.44199452,  0.10331115],\n",
              "                         [ 0.8089616 , -0.8524552 ,  0.13858955, -0.33914313],\n",
              "                         [ 0.20628472,  0.4168406 , -0.54414105, -0.04667921]],            dtype=float32),\n",
              "            bias: DeviceArray([0., 0., 0., 0.], dtype=float32),\n",
              "        },\n",
              "        l3: {\n",
              "            kernel: DeviceArray([[ 0.41349557],\n",
              "                         [ 0.08869311],\n",
              "                         [-0.11149999],\n",
              "                         [-0.68320346]], dtype=float32),\n",
              "            bias: DeviceArray([0.], dtype=float32),\n",
              "        },\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i\n",
        "\n",
        "\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optax.sgd(learning_rate=0.1)\n",
        "optimizer_state = optimizer.init(params)\n",
        "\n",
        "# Define training step\n",
        "@jax.jit\n",
        "def train_step(optimizer_state, batch):\n",
        "    params = optimizer_state\n",
        "    grads = jax.grad(loss_fn)(params, model, batch)\n",
        "    updates, new_optimizer_state = optimizer.update(grads, optimizer_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_optimizer_state\n",
        "\n",
        "# Define data loader function\n",
        "def data_loader(x_data, y_data, batch_size=32, shuffle=True):\n",
        "    num_samples = len(x_data)\n",
        "    indices = jax.random.permutation(rng, num_samples)\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        batch_indices = indices[i:i+batch_size]\n",
        "        yield x_data[batch_indices], y_data[batch_indices]\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in data_loader(x_data, y_data, batch_size=batch_size):\n",
        "        optimizer_state, _ = train_step(optimizer_state, batch)\n"
      ],
      "metadata": {
        "id": "wKfT99yfhcjf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "4fb7f370-c9e8-42a8-8846-4432107bb856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ApplyScopeInvalidVariablesTypeError",
          "evalue": "The first argument passed to an apply function should be a dictionary of collections. Each collection should be a dictionary with string keys. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ApplyScopeInvalidVariablesTypeError)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApplyScopeInvalidVariablesTypeError\u001b[0m       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-23806569e47b>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-23806569e47b>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(optimizer_state, batch)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_optimizer_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mnew_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-23806569e47b>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(params, model, batch)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/core/scope.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(variables, rngs, mutable, flags)\u001b[0m\n\u001b[1;32m    871\u001b[0m   \"\"\"\n\u001b[1;32m    872\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyScopeInvalidVariablesTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mrngs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_rngs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     raise errors.InvalidRngError(\n",
            "\u001b[0;31mApplyScopeInvalidVariablesTypeError\u001b[0m: The first argument passed to an apply function should be a dictionary of collections. Each collection should be a dictionary with string keys. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ApplyScopeInvalidVariablesTypeError)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7AdREOKhiKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "pytorch_to_jax",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}