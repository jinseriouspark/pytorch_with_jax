{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinseriouspark/pytorch_with_jax/blob/main/pytorch_to_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform pytorch to jax\n",
        "\n",
        "- 활용자료 : https://github.com/hunkim/PyTorchZeroToAll"
      ],
      "metadata": {
        "id": "ynp9olWoHBz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hunkim/PyTorchZeroToAll.git"
      ],
      "metadata": {
        "id": "VZeCuLmERFmh",
        "outputId": "57ed514e-d6af-4ed3-c9bb-5bc35020bca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PyTorchZeroToAll'...\n",
            "remote: Enumerating objects: 598, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 598 (delta 3), reused 4 (delta 0), pack-reused 589\u001b[K\n",
            "Receiving objects: 100% (598/598), 52.77 MiB | 28.44 MiB/s, done.\n",
            "Resolving deltas: 100% (389/389), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CosN6ukkgYbf",
        "outputId": "7333aae9-cadc-4afa-fc78-a46fd06f566d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform pytorch to jax"
      ],
      "metadata": {
        "id": "WPcCYoxuG22g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/sample_data/california_housing_train.csv', nrows = 100)\n",
        "feature_col = 'median_income'\n",
        "target_col = 'median_house_value'"
      ],
      "metadata": {
        "id": "FLUkG_f6h6Zs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01_basic.py"
      ],
      "metadata": {
        "id": "tzo4-YL3qfAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_basic.py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#x_data = data[feature_col].values\n",
        "#y_data = data[target_col].values\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) **2\n",
        "\n",
        "# list of weights/mean square Error (MSE) for each input\n",
        "w_list = []\n",
        "mse_list = []\n",
        "\n",
        "for w in np.arange(0.0, 4.0, 1.0):\n",
        "  l_sum = 0\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    y_pred_val = forward(x_val)\n",
        "    l = loss(x_val, y_val)\n",
        "    l_sum += l\n",
        "\n",
        "    print('\\t', x_val, y_val, y_pred_val, l)\n",
        "  print('MSE=', l_sum/ len(x_data)) # 직접 평균 계산\n",
        "  w_list.append(w)\n",
        "  mse_list.append(l_sum / len(x_data))\n",
        "# Plot it all\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wokSxfHZHOlU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "outputId": "bebf9c04-1673-43ee-df02-b5d96246559a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01_basic with jax"
      ],
      "metadata": {
        "id": "O12vMT2Fqhur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_basic with jax\n",
        "## jax.numpy as jnp , from jax import grad, jnp.mean() 등을 사용\n",
        "\n",
        "import datetime\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "# 데이터 정의\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "# forward pass\n",
        "def forward(x, w):\n",
        "  return x * w\n",
        "# loss\n",
        "def loss(w, x, y):\n",
        "  y_pred = forward(x, w)\n",
        "  return jnp.mean((y_pred - y) **2)\n",
        "\n",
        "# grad를 계산하는 함수 생성\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "# 초기 w 값 설정\n",
        "w = 1.0\n",
        "\n",
        "# w업데이트하면서 손실감소\n",
        "num_epochs = 100\n",
        "losses = []\n",
        "lr = 0.01\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  grad_w = grad_loss(w, x_data, y_data)\n",
        "  w -= lr * grad_w\n",
        "  loss_val = loss(w, x_data, y_data)\n",
        "  losses.append(loss_val)\n",
        "  if epoch % 10 == 0:\n",
        "      print(f'Epoch {epoch + 1}, Loss {loss_val}')\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9lydl-tGHAY1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "741ea65d-e5bb-4128-bab8-acfd185e19ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 3.836207151412964\n",
            "Epoch 11, Loss 0.5405738353729248\n",
            "Epoch 21, Loss 0.0761740654706955\n",
            "Epoch 31, Loss 0.01073399931192398\n",
            "Epoch 41, Loss 0.0015125819481909275\n",
            "Epoch 51, Loss 0.00021314274636097252\n",
            "Epoch 61, Loss 3.003445999638643e-05\n",
            "Epoch 71, Loss 4.233250365359709e-06\n",
            "Epoch 81, Loss 5.96372842665005e-07\n",
            "Epoch 91, Loss 8.396483508477104e-08\n",
            "0:00:02.029153\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02_manual_gradient.py"
      ],
      "metadata": {
        "id": "6bateGiDivo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = 1.0\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y) # 오차제곱\n",
        "\n",
        "# compute gradient\n",
        "def gradient(x, y):\n",
        "  return 2 * x * (x * w - y)\n",
        "\n",
        "# before training\n",
        "print('prediction (before training)', 4, forward(4))\n",
        "\n",
        "# training loop\n",
        "for epoch in range(10):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad = gradient(x_val, y_val)\n",
        "    w = w - 0.01 * grad\n",
        "    print('\\tgrad: ', x_val, y_val, round(grad, 2))\n",
        "    l = loss(x_val, y_val) # 오차 제곱을 데이터셋 별로 더하여 계산\n",
        "  print('progress:', epoch, 'w=', round(w, 2), 'loss=', round(1, 2))\n",
        "\n",
        "#after training\n",
        "print('predicted score (after training)', 4, forward(4))"
      ],
      "metadata": {
        "id": "Vi2OAKCRlrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "MVEsUBeOqkZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "w = 1.0\n",
        "\n",
        "def forward(x, w):\n",
        "  return x * w\n",
        "\n",
        "def loss(w, x, y):\n",
        "  y_pred = forward(x, w)\n",
        "  return jnp.mean((y_pred - y) ** 2)\n",
        "\n",
        "# gradient 계산,loss 함수를 입력으로 받아들여 그래디언트 계산하는 함수로 반환\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "print('prediction (before_training)', forward(4, w))\n",
        "\n",
        "# training\n",
        "learning_rate= 0.01\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad_w = grad_loss(w, x_val, y_val)\n",
        "    w -= learning_rate * grad_w\n",
        "    print('\\tgrad:', x_val, y_val, round(grad_w, 2))\n",
        "    loss_val = loss(w, x_val, y_val)\n",
        "  print('progress:', epoch, 'w=', round(w, 2), 'loss=', round(loss_val, 2))\n",
        "\n",
        "# 학습 후 예측\n",
        "print('predicted score (after training):', forward(4, w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXDrwd8rnDUU",
        "outputId": "c528c930-b82f-4fbc-ceaf-d202455038df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before_training) 4.0\n",
            "\tgrad: 1.0 2.0 -2.0\n",
            "\tgrad: 2.0 4.0 -7.8399997\n",
            "\tgrad: 3.0 6.0 -16.23\n",
            "progress: 0 w= 1.26 loss= 4.92\n",
            "\tgrad: 1.0 2.0 -1.48\n",
            "\tgrad: 2.0 4.0 -5.7999997\n",
            "\tgrad: 3.0 6.0 -12.0\n",
            "progress: 1 w= 1.4499999 loss= 2.69\n",
            "\tgrad: 1.0 2.0 -1.09\n",
            "\tgrad: 2.0 4.0 -4.29\n",
            "\tgrad: 3.0 6.0 -8.87\n",
            "progress: 2 w= 1.5999999 loss= 1.4699999\n",
            "\tgrad: 1.0 2.0 -0.81\n",
            "\tgrad: 2.0 4.0 -3.1699998\n",
            "\tgrad: 3.0 6.0 -6.56\n",
            "progress: 3 w= 1.6999999 loss= 0.79999995\n",
            "\tgrad: 1.0 2.0 -0.59999996\n",
            "\tgrad: 2.0 4.0 -2.34\n",
            "\tgrad: 3.0 6.0 -4.85\n",
            "progress: 4 w= 1.78 loss= 0.44\n",
            "\tgrad: 1.0 2.0 -0.44\n",
            "\tgrad: 2.0 4.0 -1.73\n",
            "\tgrad: 3.0 6.0 -3.58\n",
            "progress: 5 w= 1.8399999 loss= 0.24\n",
            "\tgrad: 1.0 2.0 -0.32999998\n",
            "\tgrad: 2.0 4.0 -1.28\n",
            "\tgrad: 3.0 6.0 -2.6499999\n",
            "progress: 6 w= 1.88 loss= 0.13\n",
            "\tgrad: 1.0 2.0 -0.24\n",
            "\tgrad: 2.0 4.0 -0.95\n",
            "\tgrad: 3.0 6.0 -1.9599999\n",
            "progress: 7 w= 1.91 loss= 0.07\n",
            "\tgrad: 1.0 2.0 -0.17999999\n",
            "\tgrad: 2.0 4.0 -0.7\n",
            "\tgrad: 3.0 6.0 -1.4499999\n",
            "progress: 8 w= 1.93 loss= 0.04\n",
            "\tgrad: 1.0 2.0 -0.13\n",
            "\tgrad: 2.0 4.0 -0.52\n",
            "\tgrad: 3.0 6.0 -1.0699999\n",
            "progress: 9 w= 1.9499999 loss= 0.02\n",
            "predicted score (after training): 7.8048644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 manual gradient 계산할 때는 loss 계산에서 틀린 것이 없었다."
      ],
      "metadata": {
        "id": "kPHee6J6i1Ir"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_w, loss_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zjYLusNpLwt",
        "outputId": "8e16d93c-dc56-4557-e63e-2e3b8bb8361d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array(-1.0708666, dtype=float32, weak_type=True),\n",
              " Array(0.02141885, dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03_auto_gradient.py"
      ],
      "metadata": {
        "id": "UDkcoyF_qV4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pdb\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "w = torch.tensor([1.0], requires_grad = True)\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(y_pred, y_val):\n",
        "  return (y_pred - y_val) **2\n",
        "\n",
        "# before training\n",
        "print('prediction (before training)', 4, forward(4).item())\n",
        "\n",
        "# training loop\n",
        "for epoch in range(1):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    y_pred = forward(x_val) # 1) forward pass\n",
        "    l = loss(y_pred, y_val) # 2) compute loss\n",
        "    print(f'{y_pred},{y_val} loss : {l.item()}')\n",
        "    l.backward() # 3) backpropagation to update weights\n",
        "    #print('\\t grad:', x_val, y_val, w.grad.item())\n",
        "    w.data = w.data - 0.01 * w.grad.item()\n",
        "\n",
        "    w.grad.data.zero_()\n",
        "  print(f'Epoch: {epoch} | loss : {l.item()}')\n",
        "\n",
        "# after training\n",
        "print('prediction (after training)', 4, forward(4).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqTUzucjpVbX",
        "outputId": "c579efc9-c73b-4412-9a07-e984a4070509"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "tensor([1.], grad_fn=<MulBackward0>),2.0 loss : 1.0\n",
            "tensor([2.0400], grad_fn=<MulBackward0>),4.0 loss : 3.841600179672241\n",
            "tensor([3.2952], grad_fn=<MulBackward0>),6.0 loss : 7.315943717956543\n",
            "Epoch: 0 | loss : 7.315943717956543\n",
            "prediction (after training) 4 5.042752265930176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax\n",
        "- torch 구현보다 예측력이 떨어진다!?"
      ],
      "metadata": {
        "id": "kSwVz9EdqnPf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IdiTnoPKro1K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수정 전\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "import jax\n",
        "\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "w = jnp.array([1.0]) # 당연하게 requires_grad = True 는 없다\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "def loss(y_pred, y_val):\n",
        "  return (y_pred - y_val)**2\n",
        "\n",
        "learning_rate = 0.01\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "print('prediction (before training)', 4, forward(4)[0])\n",
        "\n",
        "for epoch in range(20):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    # y_pred = forward(x_val) # 1) forward pass\n",
        "    #l = loss(y_pred, y_val) # 2) compute loss\n",
        "    #l.backward() # 3) backpropagation to update weights\n",
        "    #print('\\t grad:', x_val, y_val, w.grad.item())\n",
        "    y_pred = forward(x_val)\n",
        "    grad_w = grad_loss(y_pred[0],y_val)\n",
        "    w -= learning_rate * grad_w\n",
        "    print('\\tgrad: ', x_val, y_val, grad_w)\n",
        "\n",
        "  print(f'Epoch: {epoch} | Loss: {loss(forward(x_data), y_data)}')\n",
        "\n",
        "print('prediction (after training)', 4, forward(4)[0], '-> 잘못된 grad() 사용으로 인한 결과물 오류 ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTRvGqLtqm7X",
        "outputId": "fdf0ee0c-c350-45e0-c839-96477701e43e"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -3.92\n",
            "\tgrad:  3.0 6.0 -5.6448\n",
            "Epoch: 0 | Loss: [0.7820786 3.1283145 7.038707 ]\n",
            "\tgrad:  1.0 2.0 -1.7687042\n",
            "\tgrad:  2.0 4.0 -3.46666\n",
            "\tgrad:  3.0 6.0 -4.99199\n",
            "Epoch: 1 | Loss: [0.6116468 2.446587  5.5048213]\n",
            "\tgrad:  1.0 2.0 -1.564157\n",
            "\tgrad:  2.0 4.0 -3.0657477\n",
            "\tgrad:  3.0 6.0 -4.4146767\n",
            "Epoch: 2 | Loss: [0.4783557 1.9134228 4.3052006]\n",
            "\tgrad:  1.0 2.0 -1.3832653\n",
            "\tgrad:  2.0 4.0 -2.7111998\n",
            "\tgrad:  3.0 6.0 -3.9041271\n",
            "Epoch: 3 | Loss: [0.37411162 1.4964465  3.3670046 ]\n",
            "\tgrad:  1.0 2.0 -1.2232933\n",
            "\tgrad:  2.0 4.0 -2.397655\n",
            "\tgrad:  3.0 6.0 -3.4526234\n",
            "Epoch: 4 | Loss: [0.29258466 1.1703386  2.6332629 ]\n",
            "\tgrad:  1.0 2.0 -1.0818219\n",
            "\tgrad:  2.0 4.0 -2.1203709\n",
            "\tgrad:  3.0 6.0 -3.0533333\n",
            "Epoch: 5 | Loss: [0.22882412 0.9152965  2.0594177 ]\n",
            "\tgrad:  1.0 2.0 -0.9567113\n",
            "\tgrad:  2.0 4.0 -1.875154\n",
            "\tgrad:  3.0 6.0 -2.700222\n",
            "Epoch: 6 | Loss: [0.17895843 0.7158337  1.6106262 ]\n",
            "\tgrad:  1.0 2.0 -0.8460696\n",
            "\tgrad:  2.0 4.0 -1.6582966\n",
            "\tgrad:  3.0 6.0 -2.387947\n",
            "Epoch: 7 | Loss: [0.13995953 0.5598381  1.2596358 ]\n",
            "\tgrad:  1.0 2.0 -0.7482233\n",
            "\tgrad:  2.0 4.0 -1.4665174\n",
            "\tgrad:  3.0 6.0 -2.111786\n",
            "Epoch: 8 | Loss: [0.10945936 0.43783745 0.9851345 ]\n",
            "\tgrad:  1.0 2.0 -0.66169286\n",
            "\tgrad:  2.0 4.0 -1.2969179\n",
            "\tgrad:  3.0 6.0 -1.8675623\n",
            "Epoch: 9 | Loss: [0.08560585 0.3424234  0.77045244]\n",
            "\tgrad:  1.0 2.0 -0.58516955\n",
            "\tgrad:  2.0 4.0 -1.1469321\n",
            "\tgrad:  3.0 6.0 -1.6515818\n",
            "Epoch: 10 | Loss: [0.06695043 0.26780173 0.60255355]\n",
            "\tgrad:  1.0 2.0 -0.51749563\n",
            "\tgrad:  2.0 4.0 -1.0142913\n",
            "\tgrad:  3.0 6.0 -1.4605789\n",
            "Epoch: 11 | Loss: [0.05236049 0.20944194 0.47124436]\n",
            "\tgrad:  1.0 2.0 -0.45764828\n",
            "\tgrad:  2.0 4.0 -0.8969908\n",
            "\tgrad:  3.0 6.0 -1.291667\n",
            "Epoch: 12 | Loss: [0.04095002 0.16380008 0.36855015]\n",
            "\tgrad:  1.0 2.0 -0.4047222\n",
            "\tgrad:  2.0 4.0 -0.7932553\n",
            "\tgrad:  3.0 6.0 -1.1422882\n",
            "Epoch: 13 | Loss: [0.03202612 0.12810446 0.28823504]\n",
            "\tgrad:  1.0 2.0 -0.35791683\n",
            "\tgrad:  2.0 4.0 -0.7015171\n",
            "\tgrad:  3.0 6.0 -1.0101843\n",
            "Epoch: 14 | Loss: [0.02504694 0.10018776 0.22542247]\n",
            "\tgrad:  1.0 2.0 -0.3165245\n",
            "\tgrad:  2.0 4.0 -0.62038803\n",
            "\tgrad:  3.0 6.0 -0.89335823\n",
            "Epoch: 15 | Loss: [0.01958868 0.07835473 0.17629834]\n",
            "\tgrad:  1.0 2.0 -0.27991915\n",
            "\tgrad:  2.0 4.0 -0.5486417\n",
            "\tgrad:  3.0 6.0 -0.7900448\n",
            "Epoch: 16 | Loss: [0.0153199  0.06127959 0.13787907]\n",
            "\tgrad:  1.0 2.0 -0.24754715\n",
            "\tgrad:  2.0 4.0 -0.4851923\n",
            "\tgrad:  3.0 6.0 -0.69867706\n",
            "Epoch: 17 | Loss: [0.01198136 0.04792544 0.10783225]\n",
            "\tgrad:  1.0 2.0 -0.2189188\n",
            "\tgrad:  2.0 4.0 -0.42908096\n",
            "\tgrad:  3.0 6.0 -0.61787605\n",
            "Epoch: 18 | Loss: [0.00937037 0.03748149 0.08433329]\n",
            "\tgrad:  1.0 2.0 -0.19360137\n",
            "\tgrad:  2.0 4.0 -0.3794589\n",
            "\tgrad:  3.0 6.0 -0.54642105\n",
            "Epoch: 19 | Loss: [0.00732838 0.02931353 0.06595539]\n",
            "prediction (after training) 4 7.657576 -> 잘못된 grad() 사용으로 인한 결과물 오류 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 오류 해결:  torch w.grad.data.zeros_() 와 jax.grad(, argnums = (0,1)) 를 비교"
      ],
      "metadata": {
        "id": "5liyTR9e2FIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch, jax 간 같은 loss 함수를 사용함에도 불구하고 사용법에 따라 문제가 발생하는 이유\n",
        "# grad 를 계산하는 방식이 다름\n",
        "## torch\n",
        "# l = loss(y_pred, y_true)\n",
        "# l.backward()\n",
        "\n",
        "## jax\n",
        "# grad_loss=grad(loss)\n",
        "# grad_loss(y_pred, y_true)\n",
        "\n",
        "\n",
        "w_torch = torch.tensor([1.0], requires_grad = True)\n",
        "w_jax = jnp.array([1.0]) # 당연하게 requires_grad = True 는 없다\n",
        "def forward_torch(x):\n",
        "    return x * w_torch\n",
        "def forward_jax(w_jax, x):\n",
        "    return x * w_jax\n",
        "\n",
        "def loss(y_pred, y_true):\n",
        "    return (y_pred - y_true) **2\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "    return jnp.mean((forward_jax(params, x) - y) **2)\n",
        "\n",
        "\n",
        "x_data_torch = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\n",
        "y_data_torch = torch.tensor([2.0, 4.0, 6.0], requires_grad=False)\n",
        "\n",
        "x_data_jax = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data_jax = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "for x_val_t, y_val_t, x_val_j, y_val_j in zip(x_data_torch, y_data_torch, x_data_jax, y_data_jax):\n",
        "\n",
        "    # torch\n",
        "    y_pred_torch = forward_torch(x_val_t)\n",
        "    l = loss(y_pred_torch, y_val_t)\n",
        "    l.backward()\n",
        "    print(f'y_pred : {y_pred_torch.item()}, y_true:{y_val_t}')\n",
        "    print(f'torch loss: {l.item()}, w grad : {w_torch.grad.item()}') # w에 대한 gradient 였음\n",
        "    w_torch.grad.data.zero_()\n",
        "\n",
        "    # jax\n",
        "    y_pred_jax = forward_jax(w_jax, x_val_j)\n",
        "   #print(f'y_pred : {y_pred_jax[0]}, y_true:{y_val}')\n",
        "    jax_loss = loss(y_pred_jax[0], y_val_j)\n",
        "    grad_ = grad_loss(y_pred_jax[0], y_val_j) # 이런 방식으로 gradient 를 구한다면, y에 대한 gradient 가 출력됨\n",
        "    print(f'jax loss: {jax_loss}, y grad: {grad_}')\n",
        "\n",
        "    grad2 = jax.grad(loss_fn, argnums=(0, 1))(w_jax, x_val_j, y_val_j)\n",
        "    # 위와 같은 방식으로 해야만 제대로 weight 에 대한 gradient 를 계산할 수 있음\n",
        "    print(f'jax loss: {jax_loss}, param grad: {grad2[0]}')\n",
        "    print('-' * 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGg1kVUlml2y",
        "outputId": "0660f59e-0ef3-4739-ac23-0006cde548a1"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred : 1.0, y_true:2.0\n",
            "torch loss: 1.0, w grad : -2.0\n",
            "jax loss: 1.0, y grad: -2.0\n",
            "jax loss: 1.0, param grad: [-2.]\n",
            "----------\n",
            "y_pred : 2.0, y_true:4.0\n",
            "torch loss: 4.0, w grad : -8.0\n",
            "jax loss: 4.0, y grad: -4.0\n",
            "jax loss: 4.0, param grad: [-8.]\n",
            "----------\n",
            "y_pred : 3.0, y_true:6.0\n",
            "torch loss: 9.0, w grad : -18.0\n",
            "jax loss: 9.0, y grad: -6.0\n",
            "jax loss: 9.0, param grad: [-18.]\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수정 후\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "w = jnp.array([1.0]) # 당연하게 requires_grad = True 는 없다\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "def loss(y_pred, y_val):\n",
        "  return (y_pred - y_val)**2\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "print('prediction (before training)', 4, forward(4)[0])\n",
        "\n",
        "for epoch in range(20):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    # y_pred = forward(x_val) # 1) forward pass\n",
        "    #l = loss(y_pred, y_val) # 2) compute loss\n",
        "    #l.backward() # 3) backpropagation to update weights\n",
        "    #print('\\t grad:', x_val, y_val, w.grad.item())\n",
        "    y_pred = forward(x_val)\n",
        "    grad_w = jax.grad(loss_fn, argnums=(0, 1))(w, x_val, y_val)\n",
        "    w -= learning_rate * grad_w[0]\n",
        "    print('\\tgrad: ', x_val, y_val, grad_w[0])\n",
        "\n",
        "  print(f'Epoch: {epoch} | Loss: {loss(forward(x_data), y_data)}')\n",
        "\n",
        "print('prediction (after training)', 4, forward(4)[0], '->제대로 학습한 결과물 ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKoR3NMTmnRz",
        "outputId": "7fe66294-39c0-40e9-b5d2-22c58182aeda"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 [-2.]\n",
            "\tgrad:  2.0 4.0 [-7.84]\n",
            "\tgrad:  3.0 6.0 [-16.228802]\n",
            "Epoch: 0 | Loss: [0.54658216 2.1863286  4.919239  ]\n",
            "\tgrad:  1.0 2.0 [-1.4786239]\n",
            "\tgrad:  2.0 4.0 [-5.7962055]\n",
            "\tgrad:  3.0 6.0 [-11.998146]\n",
            "Epoch: 1 | Loss: [0.29875213 1.1950085  2.688769  ]\n",
            "\tgrad:  1.0 2.0 [-1.0931644]\n",
            "\tgrad:  2.0 4.0 [-4.285205]\n",
            "\tgrad:  3.0 6.0 [-8.870373]\n",
            "Epoch: 2 | Loss: [0.16329262 0.65317047 1.4696338 ]\n",
            "\tgrad:  1.0 2.0 [-0.80818963]\n",
            "\tgrad:  2.0 4.0 [-3.1681032]\n",
            "\tgrad:  3.0 6.0 [-6.557974]\n",
            "Epoch: 3 | Loss: [0.0892528  0.3570112  0.80327564]\n",
            "\tgrad:  1.0 2.0 [-0.59750414]\n",
            "\tgrad:  2.0 4.0 [-2.3422165]\n",
            "\tgrad:  3.0 6.0 [-4.8483896]\n",
            "Epoch: 4 | Loss: [0.04878404 0.19513616 0.43905652]\n",
            "\tgrad:  1.0 2.0 [-0.44174218]\n",
            "\tgrad:  2.0 4.0 [-1.7316294]\n",
            "\tgrad:  3.0 6.0 [-3.5844727]\n",
            "Epoch: 5 | Loss: [0.02666449 0.10665795 0.23998016]\n",
            "\tgrad:  1.0 2.0 [-0.3265853]\n",
            "\tgrad:  2.0 4.0 [-1.2802143]\n",
            "\tgrad:  3.0 6.0 [-2.6500454]\n",
            "Epoch: 6 | Loss: [0.01457433 0.05829733 0.13116899]\n",
            "\tgrad:  1.0 2.0 [-0.2414484]\n",
            "\tgrad:  2.0 4.0 [-0.9464779]\n",
            "\tgrad:  3.0 6.0 [-1.9592113]\n",
            "Epoch: 7 | Loss: [0.00796607 0.03186427 0.07169455]\n",
            "\tgrad:  1.0 2.0 [-0.17850566]\n",
            "\tgrad:  2.0 4.0 [-0.6997423]\n",
            "\tgrad:  3.0 6.0 [-1.4484673]\n",
            "Epoch: 8 | Loss: [0.00435411 0.01741644 0.03918699]\n",
            "\tgrad:  1.0 2.0 [-0.13197136]\n",
            "\tgrad:  2.0 4.0 [-0.5173273]\n",
            "\tgrad:  3.0 6.0 [-1.0708666]\n",
            "Epoch: 9 | Loss: [0.00237987 0.00951947 0.02141885]\n",
            "\tgrad:  1.0 2.0 [-0.0975678]\n",
            "\tgrad:  2.0 4.0 [-0.38246536]\n",
            "\tgrad:  3.0 6.0 [-0.7917023]\n",
            "Epoch: 10 | Loss: [0.00130079 0.00520314 0.01170705]\n",
            "\tgrad:  1.0 2.0 [-0.07213283]\n",
            "\tgrad:  2.0 4.0 [-0.28276062]\n",
            "\tgrad:  3.0 6.0 [-0.5853138]\n",
            "Epoch: 11 | Loss: [0.00071098 0.00284393 0.00639884]\n",
            "\tgrad:  1.0 2.0 [-0.05332851]\n",
            "\tgrad:  2.0 4.0 [-0.20904732]\n",
            "\tgrad:  3.0 6.0 [-0.43272972]\n",
            "Epoch: 12 | Loss: [0.00038861 0.00155444 0.00349745]\n",
            "\tgrad:  1.0 2.0 [-0.03942633]\n",
            "\tgrad:  2.0 4.0 [-0.1545515]\n",
            "\tgrad:  3.0 6.0 [-0.3199196]\n",
            "Epoch: 13 | Loss: [0.00021241 0.00084963 0.00191167]\n",
            "\tgrad:  1.0 2.0 [-0.02914834]\n",
            "\tgrad:  2.0 4.0 [-0.11426163]\n",
            "\tgrad:  3.0 6.0 [-0.23652077]\n",
            "Epoch: 14 | Loss: [0.0001161  0.00046439 0.00104489]\n",
            "\tgrad:  1.0 2.0 [-0.0215497]\n",
            "\tgrad:  2.0 4.0 [-0.08447456]\n",
            "\tgrad:  3.0 6.0 [-0.17486286]\n",
            "Epoch: 15 | Loss: [6.3455918e-05 2.5382367e-04 5.7109760e-04]\n",
            "\tgrad:  1.0 2.0 [-0.01593184]\n",
            "\tgrad:  2.0 4.0 [-0.06245327]\n",
            "\tgrad:  3.0 6.0 [-0.12927818]\n",
            "Epoch: 16 | Loss: [3.4683813e-05 1.3873525e-04 3.1215011e-04]\n",
            "\tgrad:  1.0 2.0 [-0.01177859]\n",
            "\tgrad:  2.0 4.0 [-0.04617214]\n",
            "\tgrad:  3.0 6.0 [-0.09557533]\n",
            "Epoch: 17 | Loss: [1.8958355e-05 7.5833421e-05 1.7062831e-04]\n",
            "\tgrad:  1.0 2.0 [-0.00870824]\n",
            "\tgrad:  2.0 4.0 [-0.03413582]\n",
            "\tgrad:  3.0 6.0 [-0.07066154]\n",
            "Epoch: 18 | Loss: [1.0361248e-05 4.1444993e-05 9.3255832e-05]\n",
            "\tgrad:  1.0 2.0 [-0.00643778]\n",
            "\tgrad:  2.0 4.0 [-0.02523613]\n",
            "\tgrad:  3.0 6.0 [-0.05223942]\n",
            "Epoch: 19 | Loss: [5.6633294e-06 2.2653318e-05 5.0968261e-05]\n",
            "prediction (after training) 4 7.990481 ->제대로 학습한 결과물 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05_linear_regression.py"
      ],
      "metadata": {
        "id": "hyGPMymMsn74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1) # one in and one out , 인풋 1개에 대한 아웃풋 1개\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred\n",
        "\n",
        "model = Model()\n",
        "# 가중치 초기화\n",
        "nn.init.constant_(model.linear.weight, 1.0) # 참고 코드엔 없는 내역\n",
        "nn.init.constant_(model.linear.bias, 0.0) # 참고 코드엔 없는 내역\n",
        "\n",
        "# loss function & optimizer\n",
        "criterion = torch.nn.MSELoss(reduction = 'sum') # data point 별로 발생한 loss를 합칠 때 sum, 평균을 낼때는 mean\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "# training loop\n",
        "hour_var = tensor([[4.0]])\n",
        "print('prediction (before training)', 4,  model(hour_var).data[0][0].item())\n",
        "for epoch in range(500):\n",
        "    # 1) forward pass : compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()}')\n",
        "\n",
        "    optimizer.zero_grad() # w.grad.data.zero_() 와 같은 기능\n",
        "    loss.backward() # perform backward pass\n",
        "    optimizer.step() # update the weights\n",
        "\n",
        "# after training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(f'prediction (after training)', 4, model(hour_var).data[0][0].item())"
      ],
      "metadata": {
        "id": "5w1GKtijsTJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax\n",
        "- jax 는 pytorch와 다르게 함수형 프로그램을 강조하고 있음 (class 가 필요없음)\n",
        "- 모델과 관련된 파라미터와 계산을 함수로 구현하여 사용하는 것이 일반적임\n",
        "\n",
        "\n",
        "### 변경사항\n",
        "  - loss 를 optax 내장함수로 바꾸어줌\n",
        "  - opimizer 가 2줄짜리로 변경 : opt_state, params 업데이트 필요\n",
        "\n",
        "### optax 를 사용한 최적화 코드\n",
        "\n",
        "[참고자료](https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.sgd)\n",
        "\n",
        "```\n",
        "# optax.sgd 를 활용해 optimizer 를 초기화 할때, params 가 jax 배열이 아니기때문에 오류가 발생\n",
        "optimizer = optax.sgd(learning_rate = 0.01)\n",
        "\n",
        "# 아래와 같이 한번 init 해주어야함\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "\n",
        "# optimizer 를 활용해 업데이트를 하면 opt_state 가 업데이트 되며,\n",
        "# 이를 다시 apply_updates 로 처리해서 실제 업데이트도 진행\n",
        "updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "params = optax.apply_updates(params, updates)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Q. jit 을 gradient 계산하는 부분에 적용하기 위해서는 어떻게 해야 하는가?\n",
        "\n",
        "\n",
        "chat gpt 설명\n",
        "```\n",
        "@jit\n",
        "def value_and_grad(params, x, y):\n",
        "    return jax.value_and_grad(loss)(params, x, y)\n",
        "```\n"
      ],
      "metadata": {
        "id": "5t0YGypVuwGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit\n",
        "import optax\n",
        "\n",
        "x_data = jnp.array([[1.0], [2.0], [3.0]])\n",
        "y_data = jnp.array([[2.0], [4.0], [6.0]])\n",
        "\n",
        "def model(params, x):\n",
        "    return jnp.dot(x, params['weight']) + params['bias']\n",
        "\n",
        "def loss(params, x, y):\n",
        "    y_pred = model(params, x)\n",
        "    return jnp.sum((y_pred - y) ** 2)\n",
        "\n",
        "init_params = {'weight' : jnp.array([[1.0]]), 'bias': jnp.array([0.0])}\n",
        "optimizer = optax.sgd(learning_rate = 0.01)\n",
        "opt_state = optimizer.init(init_params)\n",
        "\n",
        "@jit\n",
        "def update(params, opt_state, x, y):\n",
        "    grads = jax.grad(loss)(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates) # optimizer.step()\n",
        "    return new_params, opt_state\n",
        "\n",
        "num_epochs= 500\n",
        "params = init_params\n",
        "\n",
        "#\n",
        "for epoch in range(num_epochs):\n",
        "    loss_val = loss(params, x_data, y_data)\n",
        "    params, opt_state = update(params, opt_state, x_data, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss_val}')\n",
        "\n",
        "# After training\n",
        "hour_var = jnp.array([[4.0]])\n",
        "y_pred = model(params, hour_var)\n",
        "print(\"Prediction (after training):\", 4, y_pred[0][0])\n"
      ],
      "metadata": {
        "id": "dE2U8mH47OPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962b5a66-623e-41df-9203-9159a27beb01"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 14.0\n",
            "Epoch: 1 | Loss: 6.26400089263916\n",
            "Epoch: 2 | Loss: 2.8196990489959717\n",
            "Epoch: 3 | Loss: 1.2859458923339844\n",
            "Epoch: 4 | Loss: 0.6027218103408813\n",
            "Epoch: 5 | Loss: 0.2981351613998413\n",
            "Epoch: 6 | Loss: 0.16211287677288055\n",
            "Epoch: 7 | Loss: 0.10113726556301117\n",
            "Epoch: 8 | Loss: 0.07357636839151382\n",
            "Epoch: 9 | Loss: 0.06089683622121811\n",
            "Epoch: 10 | Loss: 0.0548475943505764\n",
            "Epoch: 11 | Loss: 0.0517561249434948\n",
            "Epoch: 12 | Loss: 0.04998693987727165\n",
            "Epoch: 13 | Loss: 0.04881208762526512\n",
            "Epoch: 14 | Loss: 0.04790737107396126\n",
            "Epoch: 15 | Loss: 0.047128405421972275\n",
            "Epoch: 16 | Loss: 0.04641082137823105\n",
            "Epoch: 17 | Loss: 0.04572590813040733\n",
            "Epoch: 18 | Loss: 0.045060817152261734\n",
            "Epoch: 19 | Loss: 0.04440966248512268\n",
            "Epoch: 20 | Loss: 0.04376979172229767\n",
            "Epoch: 21 | Loss: 0.04314004257321358\n",
            "Epoch: 22 | Loss: 0.042519811540842056\n",
            "Epoch: 23 | Loss: 0.0419086329638958\n",
            "Epoch: 24 | Loss: 0.041306257247924805\n",
            "Epoch: 25 | Loss: 0.040712546557188034\n",
            "Epoch: 26 | Loss: 0.04012734442949295\n",
            "Epoch: 27 | Loss: 0.03955063596367836\n",
            "Epoch: 28 | Loss: 0.03898235410451889\n",
            "Epoch: 29 | Loss: 0.03842207416892052\n",
            "Epoch: 30 | Loss: 0.03786984458565712\n",
            "Epoch: 31 | Loss: 0.03732563927769661\n",
            "Epoch: 32 | Loss: 0.0367891751229763\n",
            "Epoch: 33 | Loss: 0.03626055642962456\n",
            "Epoch: 34 | Loss: 0.03573933616280556\n",
            "Epoch: 35 | Loss: 0.03522577881813049\n",
            "Epoch: 36 | Loss: 0.03471941128373146\n",
            "Epoch: 37 | Loss: 0.03422047942876816\n",
            "Epoch: 38 | Loss: 0.033728763461112976\n",
            "Epoch: 39 | Loss: 0.033244065940380096\n",
            "Epoch: 40 | Loss: 0.03276626020669937\n",
            "Epoch: 41 | Loss: 0.03229529410600662\n",
            "Epoch: 42 | Loss: 0.03183121234178543\n",
            "Epoch: 43 | Loss: 0.03137364983558655\n",
            "Epoch: 44 | Loss: 0.030922820791602135\n",
            "Epoch: 45 | Loss: 0.030478371307253838\n",
            "Epoch: 46 | Loss: 0.030040377750992775\n",
            "Epoch: 47 | Loss: 0.029608706012368202\n",
            "Epoch: 48 | Loss: 0.02918318472802639\n",
            "Epoch: 49 | Loss: 0.028763676062226295\n",
            "Epoch: 50 | Loss: 0.02835029363632202\n",
            "Epoch: 51 | Loss: 0.027942977845668793\n",
            "Epoch: 52 | Loss: 0.027541376650333405\n",
            "Epoch: 53 | Loss: 0.027145519852638245\n",
            "Epoch: 54 | Loss: 0.026755426079034805\n",
            "Epoch: 55 | Loss: 0.026370886713266373\n",
            "Epoch: 56 | Loss: 0.025991875678300858\n",
            "Epoch: 57 | Loss: 0.025618383660912514\n",
            "Epoch: 58 | Loss: 0.02525017224252224\n",
            "Epoch: 59 | Loss: 0.024887269362807274\n",
            "Epoch: 60 | Loss: 0.0245295949280262\n",
            "Epoch: 61 | Loss: 0.02417709492146969\n",
            "Epoch: 62 | Loss: 0.02382965385913849\n",
            "Epoch: 63 | Loss: 0.023487165570259094\n",
            "Epoch: 64 | Loss: 0.02314964309334755\n",
            "Epoch: 65 | Loss: 0.022816916927695274\n",
            "Epoch: 66 | Loss: 0.022489039227366447\n",
            "Epoch: 67 | Loss: 0.02216583862900734\n",
            "Epoch: 68 | Loss: 0.021847248077392578\n",
            "Epoch: 69 | Loss: 0.021533232182264328\n",
            "Epoch: 70 | Loss: 0.021223774179816246\n",
            "Epoch: 71 | Loss: 0.02091877907514572\n",
            "Epoch: 72 | Loss: 0.020618150010704994\n",
            "Epoch: 73 | Loss: 0.020321808755397797\n",
            "Epoch: 74 | Loss: 0.020029770210385323\n",
            "Epoch: 75 | Loss: 0.019741924479603767\n",
            "Epoch: 76 | Loss: 0.019458137452602386\n",
            "Epoch: 77 | Loss: 0.019178541377186775\n",
            "Epoch: 78 | Loss: 0.018902920186519623\n",
            "Epoch: 79 | Loss: 0.018631258979439735\n",
            "Epoch: 80 | Loss: 0.01836351305246353\n",
            "Epoch: 81 | Loss: 0.018099557608366013\n",
            "Epoch: 82 | Loss: 0.017839515581727028\n",
            "Epoch: 83 | Loss: 0.017583046108484268\n",
            "Epoch: 84 | Loss: 0.017330406233668327\n",
            "Epoch: 85 | Loss: 0.01708134450018406\n",
            "Epoch: 86 | Loss: 0.01683584228157997\n",
            "Epoch: 87 | Loss: 0.016593940556049347\n",
            "Epoch: 88 | Loss: 0.016355380415916443\n",
            "Epoch: 89 | Loss: 0.016120336949825287\n",
            "Epoch: 90 | Loss: 0.015888718888163567\n",
            "Epoch: 91 | Loss: 0.015660325065255165\n",
            "Epoch: 92 | Loss: 0.015435283072292805\n",
            "Epoch: 93 | Loss: 0.015213397331535816\n",
            "Epoch: 94 | Loss: 0.014994784258306026\n",
            "Epoch: 95 | Loss: 0.014779286459088326\n",
            "Epoch: 96 | Loss: 0.014566915109753609\n",
            "Epoch: 97 | Loss: 0.014357549138367176\n",
            "Epoch: 98 | Loss: 0.0141511932015419\n",
            "Epoch: 99 | Loss: 0.0139478063210845\n",
            "Epoch: 100 | Loss: 0.013747386634349823\n",
            "Epoch: 101 | Loss: 0.013549769297242165\n",
            "Epoch: 102 | Loss: 0.013355089351534843\n",
            "Epoch: 103 | Loss: 0.013163124211132526\n",
            "Epoch: 104 | Loss: 0.012973926961421967\n",
            "Epoch: 105 | Loss: 0.012787539511919022\n",
            "Epoch: 106 | Loss: 0.012603693641722202\n",
            "Epoch: 107 | Loss: 0.012422600761055946\n",
            "Epoch: 108 | Loss: 0.01224406436085701\n",
            "Epoch: 109 | Loss: 0.012068149633705616\n",
            "Epoch: 110 | Loss: 0.011894656345248222\n",
            "Epoch: 111 | Loss: 0.011723739095032215\n",
            "Epoch: 112 | Loss: 0.011555266566574574\n",
            "Epoch: 113 | Loss: 0.011389167048037052\n",
            "Epoch: 114 | Loss: 0.01122551504522562\n",
            "Epoch: 115 | Loss: 0.011064141988754272\n",
            "Epoch: 116 | Loss: 0.010905174538493156\n",
            "Epoch: 117 | Loss: 0.010748437605798244\n",
            "Epoch: 118 | Loss: 0.010593948885798454\n",
            "Epoch: 119 | Loss: 0.010441717691719532\n",
            "Epoch: 120 | Loss: 0.010291616432368755\n",
            "Epoch: 121 | Loss: 0.010143695399165154\n",
            "Epoch: 122 | Loss: 0.00999793317168951\n",
            "Epoch: 123 | Loss: 0.00985425803810358\n",
            "Epoch: 124 | Loss: 0.00971265509724617\n",
            "Epoch: 125 | Loss: 0.00957304984331131\n",
            "Epoch: 126 | Loss: 0.009435472078621387\n",
            "Epoch: 127 | Loss: 0.009299837052822113\n",
            "Epoch: 128 | Loss: 0.00916619598865509\n",
            "Epoch: 129 | Loss: 0.009034479968249798\n",
            "Epoch: 130 | Loss: 0.008904600515961647\n",
            "Epoch: 131 | Loss: 0.008776652626693249\n",
            "Epoch: 132 | Loss: 0.008650511503219604\n",
            "Epoch: 133 | Loss: 0.00852624885737896\n",
            "Epoch: 134 | Loss: 0.008403655141592026\n",
            "Epoch: 135 | Loss: 0.008282926864922047\n",
            "Epoch: 136 | Loss: 0.008163896389305592\n",
            "Epoch: 137 | Loss: 0.008046537637710571\n",
            "Epoch: 138 | Loss: 0.007930917665362358\n",
            "Epoch: 139 | Loss: 0.0078169209882617\n",
            "Epoch: 140 | Loss: 0.007704576943069696\n",
            "Epoch: 141 | Loss: 0.007593845948576927\n",
            "Epoch: 142 | Loss: 0.007484696805477142\n",
            "Epoch: 143 | Loss: 0.0073771364986896515\n",
            "Epoch: 144 | Loss: 0.007271123584359884\n",
            "Epoch: 145 | Loss: 0.007166639901697636\n",
            "Epoch: 146 | Loss: 0.007063587661832571\n",
            "Epoch: 147 | Loss: 0.006962144747376442\n",
            "Epoch: 148 | Loss: 0.006862071808427572\n",
            "Epoch: 149 | Loss: 0.006763416342437267\n",
            "Epoch: 150 | Loss: 0.006666216067969799\n",
            "Epoch: 151 | Loss: 0.006570430938154459\n",
            "Epoch: 152 | Loss: 0.0064759887754917145\n",
            "Epoch: 153 | Loss: 0.006382969208061695\n",
            "Epoch: 154 | Loss: 0.006291213445365429\n",
            "Epoch: 155 | Loss: 0.006200794596225023\n",
            "Epoch: 156 | Loss: 0.0061116707511246204\n",
            "Epoch: 157 | Loss: 0.006023851223289967\n",
            "Epoch: 158 | Loss: 0.005937259178608656\n",
            "Epoch: 159 | Loss: 0.005851956084370613\n",
            "Epoch: 160 | Loss: 0.005767835304141045\n",
            "Epoch: 161 | Loss: 0.0056849513202905655\n",
            "Epoch: 162 | Loss: 0.005603244993835688\n",
            "Epoch: 163 | Loss: 0.005522718653082848\n",
            "Epoch: 164 | Loss: 0.0054433452896773815\n",
            "Epoch: 165 | Loss: 0.005365096032619476\n",
            "Epoch: 166 | Loss: 0.005287996027618647\n",
            "Epoch: 167 | Loss: 0.005212034098803997\n",
            "Epoch: 168 | Loss: 0.005137084051966667\n",
            "Epoch: 169 | Loss: 0.005063260439783335\n",
            "Epoch: 170 | Loss: 0.004990500397980213\n",
            "Epoch: 171 | Loss: 0.004918789491057396\n",
            "Epoch: 172 | Loss: 0.0048480951227247715\n",
            "Epoch: 173 | Loss: 0.004778435453772545\n",
            "Epoch: 174 | Loss: 0.004709746688604355\n",
            "Epoch: 175 | Loss: 0.004642066080123186\n",
            "Epoch: 176 | Loss: 0.004575342871248722\n",
            "Epoch: 177 | Loss: 0.0045096068643033504\n",
            "Epoch: 178 | Loss: 0.004444803576916456\n",
            "Epoch: 179 | Loss: 0.004380927421152592\n",
            "Epoch: 180 | Loss: 0.004317964427173138\n",
            "Epoch: 181 | Loss: 0.004255920182913542\n",
            "Epoch: 182 | Loss: 0.004194723907858133\n",
            "Epoch: 183 | Loss: 0.004134460352361202\n",
            "Epoch: 184 | Loss: 0.004075000993907452\n",
            "Epoch: 185 | Loss: 0.004016476683318615\n",
            "Epoch: 186 | Loss: 0.003958716057240963\n",
            "Epoch: 187 | Loss: 0.003901816438883543\n",
            "Epoch: 188 | Loss: 0.003845775965601206\n",
            "Epoch: 189 | Loss: 0.003790502902120352\n",
            "Epoch: 190 | Loss: 0.0037360202986747026\n",
            "Epoch: 191 | Loss: 0.0036823309492319822\n",
            "Epoch: 192 | Loss: 0.003629387589171529\n",
            "Epoch: 193 | Loss: 0.0035772761330008507\n",
            "Epoch: 194 | Loss: 0.0035258487332612276\n",
            "Epoch: 195 | Loss: 0.0034751573111861944\n",
            "Epoch: 196 | Loss: 0.0034252172335982323\n",
            "Epoch: 197 | Loss: 0.0033760133665055037\n",
            "Epoch: 198 | Loss: 0.0033274642191827297\n",
            "Epoch: 199 | Loss: 0.003279662923887372\n",
            "Epoch: 200 | Loss: 0.0032325314823538065\n",
            "Epoch: 201 | Loss: 0.0031860528979450464\n",
            "Epoch: 202 | Loss: 0.00314028887078166\n",
            "Epoch: 203 | Loss: 0.0030951567459851503\n",
            "Epoch: 204 | Loss: 0.0030506467446684837\n",
            "Epoch: 205 | Loss: 0.0030068003106862307\n",
            "Epoch: 206 | Loss: 0.002963609527796507\n",
            "Epoch: 207 | Loss: 0.002921030391007662\n",
            "Epoch: 208 | Loss: 0.0028790521901100874\n",
            "Epoch: 209 | Loss: 0.0028376751579344273\n",
            "Epoch: 210 | Loss: 0.0027968839276582003\n",
            "Epoch: 211 | Loss: 0.0027566992212086916\n",
            "Epoch: 212 | Loss: 0.0027170598041266203\n",
            "Epoch: 213 | Loss: 0.00267801433801651\n",
            "Epoch: 214 | Loss: 0.0026395502500236034\n",
            "Epoch: 215 | Loss: 0.0026015988551080227\n",
            "Epoch: 216 | Loss: 0.0025642195250838995\n",
            "Epoch: 217 | Loss: 0.0025273384526371956\n",
            "Epoch: 218 | Loss: 0.002491023624315858\n",
            "Epoch: 219 | Loss: 0.0024552568793296814\n",
            "Epoch: 220 | Loss: 0.0024199490435421467\n",
            "Epoch: 221 | Loss: 0.0023851722944527864\n",
            "Epoch: 222 | Loss: 0.002350872615352273\n",
            "Epoch: 223 | Loss: 0.002317101461812854\n",
            "Epoch: 224 | Loss: 0.0022838024888187647\n",
            "Epoch: 225 | Loss: 0.002250984311103821\n",
            "Epoch: 226 | Loss: 0.0022186364512890577\n",
            "Epoch: 227 | Loss: 0.002186740282922983\n",
            "Epoch: 228 | Loss: 0.002155304653570056\n",
            "Epoch: 229 | Loss: 0.002124351216480136\n",
            "Epoch: 230 | Loss: 0.0020937970839440823\n",
            "Epoch: 231 | Loss: 0.0020637046545743942\n",
            "Epoch: 232 | Loss: 0.002034052973613143\n",
            "Epoch: 233 | Loss: 0.0020048164296895266\n",
            "Epoch: 234 | Loss: 0.0019759980496019125\n",
            "Epoch: 235 | Loss: 0.0019476065644994378\n",
            "Epoch: 236 | Loss: 0.001919639646075666\n",
            "Epoch: 237 | Loss: 0.0018920368747785687\n",
            "Epoch: 238 | Loss: 0.0018648473778739572\n",
            "Epoch: 239 | Loss: 0.001838034251704812\n",
            "Epoch: 240 | Loss: 0.0018116403371095657\n",
            "Epoch: 241 | Loss: 0.0017856091726571321\n",
            "Epoch: 242 | Loss: 0.0017599178245291114\n",
            "Epoch: 243 | Loss: 0.001734637888148427\n",
            "Epoch: 244 | Loss: 0.0017097019590437412\n",
            "Epoch: 245 | Loss: 0.00168513972312212\n",
            "Epoch: 246 | Loss: 0.0016609163722023368\n",
            "Epoch: 247 | Loss: 0.001637061359360814\n",
            "Epoch: 248 | Loss: 0.0016135071637108922\n",
            "Epoch: 249 | Loss: 0.0015903395833447576\n",
            "Epoch: 250 | Loss: 0.0015674843452870846\n",
            "Epoch: 251 | Loss: 0.0015449614729732275\n",
            "Epoch: 252 | Loss: 0.001522753620520234\n",
            "Epoch: 253 | Loss: 0.001500859740190208\n",
            "Epoch: 254 | Loss: 0.0014793032314628363\n",
            "Epoch: 255 | Loss: 0.001458026934415102\n",
            "Epoch: 256 | Loss: 0.0014370810240507126\n",
            "Epoch: 257 | Loss: 0.0014164356980472803\n",
            "Epoch: 258 | Loss: 0.0013960677897557616\n",
            "Epoch: 259 | Loss: 0.001376024098135531\n",
            "Epoch: 260 | Loss: 0.0013562433887273073\n",
            "Epoch: 261 | Loss: 0.0013367440551519394\n",
            "Epoch: 262 | Loss: 0.0013175250496715307\n",
            "Epoch: 263 | Loss: 0.0012986023211851716\n",
            "Epoch: 264 | Loss: 0.0012799331452697515\n",
            "Epoch: 265 | Loss: 0.0012615270679816604\n",
            "Epoch: 266 | Loss: 0.0012433999218046665\n",
            "Epoch: 267 | Loss: 0.001225542277097702\n",
            "Epoch: 268 | Loss: 0.0012079108273610473\n",
            "Epoch: 269 | Loss: 0.0011905636638402939\n",
            "Epoch: 270 | Loss: 0.0011734634172171354\n",
            "Epoch: 271 | Loss: 0.00115658575668931\n",
            "Epoch: 272 | Loss: 0.001139973639510572\n",
            "Epoch: 273 | Loss: 0.001123575260862708\n",
            "Epoch: 274 | Loss: 0.0011074234498664737\n",
            "Epoch: 275 | Loss: 0.0010915192542597651\n",
            "Epoch: 276 | Loss: 0.0010758405551314354\n",
            "Epoch: 277 | Loss: 0.0010603814153000712\n",
            "Epoch: 278 | Loss: 0.0010451374109834433\n",
            "Epoch: 279 | Loss: 0.0010301047004759312\n",
            "Epoch: 280 | Loss: 0.0010153104085475206\n",
            "Epoch: 281 | Loss: 0.001000713324174285\n",
            "Epoch: 282 | Loss: 0.0009863314917311072\n",
            "Epoch: 283 | Loss: 0.0009721643873490393\n",
            "Epoch: 284 | Loss: 0.0009581951308064163\n",
            "Epoch: 285 | Loss: 0.0009444163297303021\n",
            "Epoch: 286 | Loss: 0.0009308473090641201\n",
            "Epoch: 287 | Loss: 0.0009174680453725159\n",
            "Epoch: 288 | Loss: 0.0009042852325364947\n",
            "Epoch: 289 | Loss: 0.0008912811754271388\n",
            "Epoch: 290 | Loss: 0.0008784712990745902\n",
            "Epoch: 291 | Loss: 0.0008658362203277647\n",
            "Epoch: 292 | Loss: 0.0008534013759344816\n",
            "Epoch: 293 | Loss: 0.000841132365167141\n",
            "Epoch: 294 | Loss: 0.0008290560217574239\n",
            "Epoch: 295 | Loss: 0.0008171384106390178\n",
            "Epoch: 296 | Loss: 0.0008053945493884385\n",
            "Epoch: 297 | Loss: 0.0007938192575238645\n",
            "Epoch: 298 | Loss: 0.0007824156200513244\n",
            "Epoch: 299 | Loss: 0.0007711492944508791\n",
            "Epoch: 300 | Loss: 0.0007600897806696594\n",
            "Epoch: 301 | Loss: 0.000749157858081162\n",
            "Epoch: 302 | Loss: 0.000738385715521872\n",
            "Epoch: 303 | Loss: 0.0007277887780219316\n",
            "Epoch: 304 | Loss: 0.0007173230405896902\n",
            "Epoch: 305 | Loss: 0.0007070149295032024\n",
            "Epoch: 306 | Loss: 0.0006968490197323263\n",
            "Epoch: 307 | Loss: 0.0006868389900773764\n",
            "Epoch: 308 | Loss: 0.0006769543397240341\n",
            "Epoch: 309 | Loss: 0.0006672344752587378\n",
            "Epoch: 310 | Loss: 0.0006576415034942329\n",
            "Epoch: 311 | Loss: 0.0006482008611783385\n",
            "Epoch: 312 | Loss: 0.0006388785550370812\n",
            "Epoch: 313 | Loss: 0.0006297083455137908\n",
            "Epoch: 314 | Loss: 0.0006206565885804594\n",
            "Epoch: 315 | Loss: 0.0006117264274507761\n",
            "Epoch: 316 | Loss: 0.000602938060183078\n",
            "Epoch: 317 | Loss: 0.0005942629068158567\n",
            "Epoch: 318 | Loss: 0.0005857336800545454\n",
            "Epoch: 319 | Loss: 0.0005773093434982002\n",
            "Epoch: 320 | Loss: 0.0005690209800377488\n",
            "Epoch: 321 | Loss: 0.0005608421633951366\n",
            "Epoch: 322 | Loss: 0.0005527825560420752\n",
            "Epoch: 323 | Loss: 0.0005448437877930701\n",
            "Epoch: 324 | Loss: 0.0005370001308619976\n",
            "Epoch: 325 | Loss: 0.0005292869755066931\n",
            "Epoch: 326 | Loss: 0.0005216760910116136\n",
            "Epoch: 327 | Loss: 0.0005141822621226311\n",
            "Epoch: 328 | Loss: 0.0005067884922027588\n",
            "Epoch: 329 | Loss: 0.0004995002527721226\n",
            "Epoch: 330 | Loss: 0.0004923305823467672\n",
            "Epoch: 331 | Loss: 0.0004852486017625779\n",
            "Epoch: 332 | Loss: 0.0004782903997693211\n",
            "Epoch: 333 | Loss: 0.00047141523100435734\n",
            "Epoch: 334 | Loss: 0.00046463878243230283\n",
            "Epoch: 335 | Loss: 0.00045795616460964084\n",
            "Epoch: 336 | Loss: 0.0004513664171099663\n",
            "Epoch: 337 | Loss: 0.0004448943363968283\n",
            "Epoch: 338 | Loss: 0.0004384909407235682\n",
            "Epoch: 339 | Loss: 0.00043219071812927723\n",
            "Epoch: 340 | Loss: 0.00042597929132170975\n",
            "Epoch: 341 | Loss: 0.0004198593378532678\n",
            "Epoch: 342 | Loss: 0.00041382620111107826\n",
            "Epoch: 343 | Loss: 0.0004078743513673544\n",
            "Epoch: 344 | Loss: 0.00040202500531449914\n",
            "Epoch: 345 | Loss: 0.00039623299380764365\n",
            "Epoch: 346 | Loss: 0.00039053699583746493\n",
            "Epoch: 347 | Loss: 0.0003849358472507447\n",
            "Epoch: 348 | Loss: 0.00037939558387733996\n",
            "Epoch: 349 | Loss: 0.0003739392850548029\n",
            "Epoch: 350 | Loss: 0.00036857501254417\n",
            "Epoch: 351 | Loss: 0.00036327826092019677\n",
            "Epoch: 352 | Loss: 0.00035805729567073286\n",
            "Epoch: 353 | Loss: 0.00035290251253172755\n",
            "Epoch: 354 | Loss: 0.00034783605951815844\n",
            "Epoch: 355 | Loss: 0.0003428396303206682\n",
            "Epoch: 356 | Loss: 0.00033790816087275743\n",
            "Epoch: 357 | Loss: 0.0003330559702590108\n",
            "Epoch: 358 | Loss: 0.00032826728420332074\n",
            "Epoch: 359 | Loss: 0.0003235457115806639\n",
            "Epoch: 360 | Loss: 0.0003188936389051378\n",
            "Epoch: 361 | Loss: 0.00031431030947715044\n",
            "Epoch: 362 | Loss: 0.00030979799339547753\n",
            "Epoch: 363 | Loss: 0.0003053467662539333\n",
            "Epoch: 364 | Loss: 0.00030096006230451167\n",
            "Epoch: 365 | Loss: 0.0002966340980492532\n",
            "Epoch: 366 | Loss: 0.00029236829141154885\n",
            "Epoch: 367 | Loss: 0.00028816398116759956\n",
            "Epoch: 368 | Loss: 0.00028402736643329263\n",
            "Epoch: 369 | Loss: 0.0002799401117954403\n",
            "Epoch: 370 | Loss: 0.0002759210765361786\n",
            "Epoch: 371 | Loss: 0.00027195896836929023\n",
            "Epoch: 372 | Loss: 0.0002680454635992646\n",
            "Epoch: 373 | Loss: 0.0002641981409396976\n",
            "Epoch: 374 | Loss: 0.00026039828662760556\n",
            "Epoch: 375 | Loss: 0.0002566576877143234\n",
            "Epoch: 376 | Loss: 0.00025296909734606743\n",
            "Epoch: 377 | Loss: 0.0002493264910299331\n",
            "Epoch: 378 | Loss: 0.0002457503869663924\n",
            "Epoch: 379 | Loss: 0.00024221553758252412\n",
            "Epoch: 380 | Loss: 0.0002387395070400089\n",
            "Epoch: 381 | Loss: 0.00023530020553153008\n",
            "Epoch: 382 | Loss: 0.00023192568914964795\n",
            "Epoch: 383 | Loss: 0.0002285896334797144\n",
            "Epoch: 384 | Loss: 0.00022530391288455576\n",
            "Epoch: 385 | Loss: 0.00022206884750630707\n",
            "Epoch: 386 | Loss: 0.0002188743674196303\n",
            "Epoch: 387 | Loss: 0.00021572950936388224\n",
            "Epoch: 388 | Loss: 0.00021262775408104062\n",
            "Epoch: 389 | Loss: 0.0002095770905725658\n",
            "Epoch: 390 | Loss: 0.0002065626613330096\n",
            "Epoch: 391 | Loss: 0.0002035941433859989\n",
            "Epoch: 392 | Loss: 0.00020066769502591342\n",
            "Epoch: 393 | Loss: 0.00019778532441705465\n",
            "Epoch: 394 | Loss: 0.00019494249136187136\n",
            "Epoch: 395 | Loss: 0.00019213714404031634\n",
            "Epoch: 396 | Loss: 0.00018938173889182508\n",
            "Epoch: 397 | Loss: 0.00018665259995032102\n",
            "Epoch: 398 | Loss: 0.00018397322855889797\n",
            "Epoch: 399 | Loss: 0.00018133751291316003\n",
            "Epoch: 400 | Loss: 0.00017872624448500574\n",
            "Epoch: 401 | Loss: 0.00017615704564377666\n",
            "Epoch: 402 | Loss: 0.00017362258222419769\n",
            "Epoch: 403 | Loss: 0.0001711284858174622\n",
            "Epoch: 404 | Loss: 0.00016867060912773013\n",
            "Epoch: 405 | Loss: 0.0001662417344050482\n",
            "Epoch: 406 | Loss: 0.0001638542889850214\n",
            "Epoch: 407 | Loss: 0.00016149814473465085\n",
            "Epoch: 408 | Loss: 0.0001591818145243451\n",
            "Epoch: 409 | Loss: 0.00015689314750488847\n",
            "Epoch: 410 | Loss: 0.00015463694580830634\n",
            "Epoch: 411 | Loss: 0.00015242006338667125\n",
            "Epoch: 412 | Loss: 0.00015022345178294927\n",
            "Epoch: 413 | Loss: 0.00014806189574301243\n",
            "Epoch: 414 | Loss: 0.00014594057574868202\n",
            "Epoch: 415 | Loss: 0.00014383667439687997\n",
            "Epoch: 416 | Loss: 0.0001417764142388478\n",
            "Epoch: 417 | Loss: 0.00013973447494208813\n",
            "Epoch: 418 | Loss: 0.00013772780948784202\n",
            "Epoch: 419 | Loss: 0.0001357525761704892\n",
            "Epoch: 420 | Loss: 0.00013380104792304337\n",
            "Epoch: 421 | Loss: 0.0001318729919148609\n",
            "Epoch: 422 | Loss: 0.0001299834402743727\n",
            "Epoch: 423 | Loss: 0.0001281121512874961\n",
            "Epoch: 424 | Loss: 0.00012626878742594272\n",
            "Epoch: 425 | Loss: 0.00012445435277186334\n",
            "Epoch: 426 | Loss: 0.00012266852718312293\n",
            "Epoch: 427 | Loss: 0.00012090388918295503\n",
            "Epoch: 428 | Loss: 0.00011916537187062204\n",
            "Epoch: 429 | Loss: 0.0001174596109194681\n",
            "Epoch: 430 | Loss: 0.0001157705337391235\n",
            "Epoch: 431 | Loss: 0.00011410238221287727\n",
            "Epoch: 432 | Loss: 0.0001124666741816327\n",
            "Epoch: 433 | Loss: 0.00011084462312282994\n",
            "Epoch: 434 | Loss: 0.00010925441165454686\n",
            "Epoch: 435 | Loss: 0.0001076841217582114\n",
            "Epoch: 436 | Loss: 0.00010613360063871369\n",
            "Epoch: 437 | Loss: 0.00010461394413141534\n",
            "Epoch: 438 | Loss: 0.0001031070132739842\n",
            "Epoch: 439 | Loss: 0.00010163035767618567\n",
            "Epoch: 440 | Loss: 0.00010016377927968279\n",
            "Epoch: 441 | Loss: 9.872634109342471e-05\n",
            "Epoch: 442 | Loss: 9.730616875458509e-05\n",
            "Epoch: 443 | Loss: 9.590996342012659e-05\n",
            "Epoch: 444 | Loss: 9.453231905354187e-05\n",
            "Epoch: 445 | Loss: 9.31713730096817e-05\n",
            "Epoch: 446 | Loss: 9.18353398446925e-05\n",
            "Epoch: 447 | Loss: 9.051505185198039e-05\n",
            "Epoch: 448 | Loss: 8.921419794205576e-05\n",
            "Epoch: 449 | Loss: 8.793039160082117e-05\n",
            "Epoch: 450 | Loss: 8.66634800331667e-05\n",
            "Epoch: 451 | Loss: 8.541919669369236e-05\n",
            "Epoch: 452 | Loss: 8.41945584397763e-05\n",
            "Epoch: 453 | Loss: 8.298196189571172e-05\n",
            "Epoch: 454 | Loss: 8.178708958439529e-05\n",
            "Epoch: 455 | Loss: 8.061654807534069e-05\n",
            "Epoch: 456 | Loss: 7.94580701040104e-05\n",
            "Epoch: 457 | Loss: 7.831672701286152e-05\n",
            "Epoch: 458 | Loss: 7.718568667769432e-05\n",
            "Epoch: 459 | Loss: 7.607959560118616e-05\n",
            "Epoch: 460 | Loss: 7.498854392906651e-05\n",
            "Epoch: 461 | Loss: 7.390737300738692e-05\n",
            "Epoch: 462 | Loss: 7.284794264705852e-05\n",
            "Epoch: 463 | Loss: 7.179813837865368e-05\n",
            "Epoch: 464 | Loss: 7.076769543346018e-05\n",
            "Epoch: 465 | Loss: 6.975100404815748e-05\n",
            "Epoch: 466 | Loss: 6.874553218949586e-05\n",
            "Epoch: 467 | Loss: 6.775883957743645e-05\n",
            "Epoch: 468 | Loss: 6.678546196781099e-05\n",
            "Epoch: 469 | Loss: 6.582619243999943e-05\n",
            "Epoch: 470 | Loss: 6.488231156254187e-05\n",
            "Epoch: 471 | Loss: 6.394430238287896e-05\n",
            "Epoch: 472 | Loss: 6.302974361460656e-05\n",
            "Epoch: 473 | Loss: 6.212222797330469e-05\n",
            "Epoch: 474 | Loss: 6.123130879132077e-05\n",
            "Epoch: 475 | Loss: 6.0351343563525006e-05\n",
            "Epoch: 476 | Loss: 5.948402395006269e-05\n",
            "Epoch: 477 | Loss: 5.8625202655093744e-05\n",
            "Epoch: 478 | Loss: 5.778324339189567e-05\n",
            "Epoch: 479 | Loss: 5.6954384490381926e-05\n",
            "Epoch: 480 | Loss: 5.6137610954465345e-05\n",
            "Epoch: 481 | Loss: 5.5329339375020936e-05\n",
            "Epoch: 482 | Loss: 5.4534655646421015e-05\n",
            "Epoch: 483 | Loss: 5.375083128456026e-05\n",
            "Epoch: 484 | Loss: 5.2975647122366354e-05\n",
            "Epoch: 485 | Loss: 5.221659375820309e-05\n",
            "Epoch: 486 | Loss: 5.14655293955002e-05\n",
            "Epoch: 487 | Loss: 5.072652129456401e-05\n",
            "Epoch: 488 | Loss: 4.999944576411508e-05\n",
            "Epoch: 489 | Loss: 4.927964982925914e-05\n",
            "Epoch: 490 | Loss: 4.857196472585201e-05\n",
            "Epoch: 491 | Loss: 4.7872221330180764e-05\n",
            "Epoch: 492 | Loss: 4.718354102806188e-05\n",
            "Epoch: 493 | Loss: 4.6507375373039395e-05\n",
            "Epoch: 494 | Loss: 4.584003545460291e-05\n",
            "Epoch: 495 | Loss: 4.518142668530345e-05\n",
            "Epoch: 496 | Loss: 4.453223300515674e-05\n",
            "Epoch: 497 | Loss: 4.389121022541076e-05\n",
            "Epoch: 498 | Loss: 4.3259424273855984e-05\n",
            "Epoch: 499 | Loss: 4.2639050661819056e-05\n",
            "Prediction (after training): 4 7.992494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06_logistic_regression.py"
      ],
      "metadata": {
        "id": "JpunUVTmIUhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# dataset\n",
        "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = tensor([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    instantiate nn.Linear Module\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = nn.Linear(1, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "\n",
        "# out model\n",
        "model = Model()\n",
        "\n",
        "# 가중치 초기화\n",
        "nn.init.constant_(model.linear.weight, 1.0)\n",
        "nn.init.constant_(model.linear.bias, 0.0)\n",
        "\n",
        "# loss function & optimizer contruction\n",
        "# call to model.parameters()\n",
        "criterion = nn.BCELoss(reduction = 'mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "\n",
        "#Traning loop\n",
        "for epoch in range(1000):\n",
        "  # forward pass : compute predicted y by passing x to the model\n",
        "  y_pred = model(x_data)\n",
        "\n",
        "  # compute and print loss\n",
        "  loss = criterion(y_pred, y_data)\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
        "\n",
        "  #zero gradient, perform a backward pass, update the weights\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "#After training\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(tensor([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(tensor([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
      ],
      "metadata": {
        "id": "kyqRLp6dxuwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "vMa71RQ-JLhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit\n",
        "import optax\n",
        "\n",
        "x_data = jnp.array([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = jnp.array([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "# 모델 정의하기\n",
        "def model(params, x):\n",
        "    return jax.nn.sigmoid(jnp.dot(x, params['weight']) + params['bias'])\n",
        "\n",
        "# 가중치 초기화\n",
        "# # 랜덤 초기화\n",
        "# key = jax.random.PRNGKey(0)\n",
        "# init_params = {\n",
        "#     'weight': jax.random.normal(key, (1, 1)),\n",
        "#     'bias': jax.random.normal(key, (1, ))\n",
        "# }\n",
        "\n",
        "# 지정값으로 초기화\n",
        "init_params = {'weight': jnp.array([[1.0]]), 'bias': jnp.array([0.0])}\n",
        "\n",
        "# 손실함수 및 최적화기 정의하기\n",
        "def binary_cross_entropy(y_hat, y):\n",
        "    bce = y * jnp.log(y_hat) + (1 - y) * jnp.log(1 - y_hat)\n",
        "    return jnp.mean(-bce)\n",
        "\n",
        "def loss(params, x, y):\n",
        "    y_pred = model(params, x)\n",
        "    #return optax.sigmoid_binary_cross_entropy(y_pred, y).mean()\n",
        "    return binary_cross_entropy(y_pred, y).mean()\n",
        "\n",
        "optimizer = optax.sgd(learning_rate = 0.01)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# def update(params, opt_state, x, y):\n",
        "#     grads = jax.grad(loss)(params, x, y)\n",
        "#     updates, opt_state = optimizer.update(grad, opt_state)\n",
        "#     new_params =\n",
        "\n",
        "num_epochs = 1000\n",
        "params = init_params\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss_val, grads = jax.value_and_grad(loss)(params, x_data, y_data)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params) # optax.sgd~  의 update 는 x, y 데이터셋을 필요로 하지 않음\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch : {epoch + 1} / {num_epochs}| loss: {loss_val}')\n",
        "\n",
        "\n",
        "#After training\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(params, jnp.array([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(params, jnp.array([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
      ],
      "metadata": {
        "id": "Zf_zIyPlDkLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 07_diabets_logistic.py"
      ],
      "metadata": {
        "id": "N_gPZ2TogL4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "xy = np.loadtxt('/content/PyTorchZeroToAll/data/diabetes.csv.gz', delimiter=',', dtype = np.float32)\n",
        "x_data = from_numpy(xy[:, 0: -1])\n",
        "y_data = from_numpy(xy[:, [-1]])\n",
        "\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mATnN4tuc9TC",
        "outputId": "d9ac01cb-b781-4cd4-a8c7-82d78742a1eb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.15, stratify = y_data, random_state= 329)"
      ],
      "metadata": {
        "id": "qG7DI8yq5FbN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1= self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = nn.BCELoss(reduction = 'mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
        "\n",
        "for epoch in range(100):\n",
        "    # forward pass\n",
        "    y_pred = model(X_train)\n",
        "    # compute and print loss\n",
        "    loss = criterion(y_pred, y_train)\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch + 1} / 100 | Loss : {loss.item():.4f}')\n",
        "\n",
        "    # zero gradients, perform a backward pass, update weights\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step() # 100회 이후로는 학습되는 부분이 없음\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5eIknoJK324",
        "outputId": "0b692f4e-2935-465b-e2ba-332fb76eca64"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 100 | Loss : 0.6485\n",
            "Epoch 11 / 100 | Loss : 0.6462\n",
            "Epoch 21 / 100 | Loss : 0.6452\n",
            "Epoch 31 / 100 | Loss : 0.6448\n",
            "Epoch 41 / 100 | Loss : 0.6447\n",
            "Epoch 51 / 100 | Loss : 0.6446\n",
            "Epoch 61 / 100 | Loss : 0.6446\n",
            "Epoch 71 / 100 | Loss : 0.6446\n",
            "Epoch 81 / 100 | Loss : 0.6445\n",
            "Epoch 91 / 100 | Loss : 0.6445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_test)\n",
        "\n",
        "y_pred_binary = [int(i> 0.5) for i in y_pred.detach().numpy()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylK3Q3rj5EaX",
        "outputId": "7c8b0a8b-0b82-49fe-c6bd-903e7ad8e9fd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-2e2587a3a53d>:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  y_pred_binary = [int(i> 0.5) for i in y_pred.detach().numpy()]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict = classification_report(y_pred_binary, y_test, output_dict = True)\n",
        "pd.DataFrame(result_dict).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "2lNFyRDV5odp",
        "outputId": "08106630-7e48-4da8-dc34-991da95ef2c4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              precision    recall  f1-score     support\n",
              "0.0            0.000000  0.000000  0.000000    0.000000\n",
              "1.0            1.000000  0.649123  0.787234  114.000000\n",
              "accuracy       0.649123  0.649123  0.649123    0.649123\n",
              "macro avg      0.500000  0.324561  0.393617  114.000000\n",
              "weighted avg   1.000000  0.649123  0.787234  114.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f23eeb11-b8be-4d6a-89d2-26a28ee86e39\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.649123</td>\n",
              "      <td>0.787234</td>\n",
              "      <td>114.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.649123</td>\n",
              "      <td>0.649123</td>\n",
              "      <td>0.649123</td>\n",
              "      <td>0.649123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macro avg</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.324561</td>\n",
              "      <td>0.393617</td>\n",
              "      <td>114.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted avg</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.649123</td>\n",
              "      <td>0.787234</td>\n",
              "      <td>114.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f23eeb11-b8be-4d6a-89d2-26a28ee86e39')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f23eeb11-b8be-4d6a-89d2-26a28ee86e39 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f23eeb11-b8be-4d6a-89d2-26a28ee86e39');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5bd50b64-8670-460b-b693-d6ccb5c42226\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5bd50b64-8670-460b-b693-d6ccb5c42226')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5bd50b64-8670-460b-b693-d6ccb5c42226 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4147184369710143,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          0.5,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29029654444734115,\n        \"min\": 0.0,\n        \"max\": 0.6491228070175439,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          0.6491228070175439,\n          0.32456140350877194\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1-score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33384659693675633,\n        \"min\": 0.0,\n        \"max\": 0.7872340425531915,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.7872340425531915,\n          0.39361702127659576,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"support\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.26302491915823,\n        \"min\": 0.0,\n        \"max\": 114.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          114.0,\n          0.6491228070175439\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import jax\n",
        "from jax import grad\n",
        "\n",
        "# define model using flax\n",
        "import flax.linen as nn\n",
        "\n",
        "# data preprocessing\n",
        "x_data_jax = jnp.array(x_data)\n",
        "y_data_jax = jnp.array(y_data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_data_jax, y_data_jax, test_size = 0.15, stratify = y_data, random_state= 329)\n",
        "#print(X_train)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "\n",
        "      out1 = nn.sigmoid(nn.Dense(6)(x))\n",
        "      out2 = nn.sigmoid(nn.Dense(4)(out1))\n",
        "      y_pred = nn.sigmoid(nn.Dense(1)(out2))\n",
        "      return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "# Define loss function\n",
        "def loss(params,  x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    #print(logits)\n",
        "    return jnp.mean(optax.sigmoid_binary_cross_entropy(logits=logits, labels=y))\n",
        "\n",
        "# Initialize model parameters\n",
        "rng = jax.random.PRNGKey(0)\n",
        "params = model.init(rng, X_train[:1])  # X_train의 첫 번째 샘플을 사용하여 모델 초기화\n",
        "\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optax.sgd(learning_rate=0.1)\n",
        "optimizer_state = optimizer.init(params)\n",
        "loss_grad_fn = jax.value_and_grad(loss)\n",
        "\n",
        "for i in range(100):\n",
        "  loss_val, grads = loss_grad_fn(params, X_train, y_train)\n",
        "  updates, optimizer_state = optimizer.update(grads, optimizer_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  #break\n",
        "  if i % 10 == 0:\n",
        "\n",
        "   print('Loss step {}: '.format(i + 1), loss_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_pZMIqu-47m",
        "outputId": "827e0fe1-aa9a-4b25-b05a-bfe6084d52c6"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss step 1:  0.6461332\n",
            "Loss step 11:  0.646088\n",
            "Loss step 21:  0.6460452\n",
            "Loss step 31:  0.6460045\n",
            "Loss step 41:  0.64596593\n",
            "Loss step 51:  0.6459294\n",
            "Loss step 61:  0.6458946\n",
            "Loss step 71:  0.64586157\n",
            "Loss step 81:  0.6458302\n",
            "Loss step 91:  0.6458004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(params, x):\n",
        "  return model.apply(params, x)\n",
        "\n",
        "y_pred = predict(params, X_test)\n",
        "y_pred_class = (y_pred > 0.5).astype(int)\n",
        "pd.DataFrame(classification_report(y_pred_class, y_test, output_dict = True)).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "tLrtlviZEICC",
        "outputId": "930c70a2-6d07-4777-b9a4-21231afb2083"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              precision    recall  f1-score     support\n",
              "0.0            0.000000  0.000000  0.000000    0.000000\n",
              "1.0            1.000000  0.649123  0.787234  114.000000\n",
              "accuracy       0.649123  0.649123  0.649123    0.649123\n",
              "macro avg      0.500000  0.324561  0.393617  114.000000\n",
              "weighted avg   1.000000  0.649123  0.787234  114.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4598bcb2-427c-44ba-bf50-b874dc92d6e4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.649123</td>\n",
              "      <td>0.787234</td>\n",
              "      <td>114.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.649123</td>\n",
              "      <td>0.649123</td>\n",
              "      <td>0.649123</td>\n",
              "      <td>0.649123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macro avg</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.324561</td>\n",
              "      <td>0.393617</td>\n",
              "      <td>114.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted avg</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.649123</td>\n",
              "      <td>0.787234</td>\n",
              "      <td>114.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4598bcb2-427c-44ba-bf50-b874dc92d6e4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4598bcb2-427c-44ba-bf50-b874dc92d6e4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4598bcb2-427c-44ba-bf50-b874dc92d6e4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d2f2e5c0-3cc0-4583-85b4-7e189b091b10\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d2f2e5c0-3cc0-4583-85b4-7e189b091b10')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d2f2e5c0-3cc0-4583-85b4-7e189b091b10 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4147184369710143,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          0.5,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29029654444734115,\n        \"min\": 0.0,\n        \"max\": 0.6491228070175439,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          0.6491228070175439,\n          0.32456140350877194\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1-score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33384659693675633,\n        \"min\": 0.0,\n        \"max\": 0.7872340425531915,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.7872340425531915,\n          0.39361702127659576,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"support\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.26302491915823,\n        \"min\": 0.0,\n        \"max\": 114.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          114.0,\n          0.6491228070175439\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 08_1_dataset_loader.py"
      ],
      "metadata": {
        "id": "gKgs1x7JJ0Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ViX42hISEjZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "b4lJ7zY_J2bM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 08_2_dataset_loade_logistic.py"
      ],
      "metadata": {
        "id": "Ih-LpVfMJ4Bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "G8l60JQ-J5xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 09_01_softmax_loss.py"
      ],
      "metadata": {
        "id": "xLav_RkcJ75s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "TvToaJ-MJ9_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 09_2_softmax_mnist.py"
      ],
      "metadata": {
        "id": "gPbAF030J_SG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "Kc6nCi7tKBHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10_1_cnn_mnist.py"
      ],
      "metadata": {
        "id": "R2LGH_duKB-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "FtPt2hf0KDa7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11_1_toy_inception_mnist.py"
      ],
      "metadata": {
        "id": "O4NXyBocKEcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "_s44p2oKKGhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12_1_rnn_basics.py"
      ],
      "metadata": {
        "id": "wZgP5ZwvKI6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "mhaNAZoSKMQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12_2_hello_rnn.py"
      ],
      "metadata": {
        "id": "gJy4ZIXbKNS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "wH6iOSC6KPOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12_3_hello_rnn_seq.py"
      ],
      "metadata": {
        "id": "hBgui7E4KQLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "xTxWr73YKShL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MWY427gCJ2PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12_4_hello_rnn_emb.py"
      ],
      "metadata": {
        "id": "U-gTUJ9qKUnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "ENmCIGWeKd7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13_1_rnn_classification_basics.py"
      ],
      "metadata": {
        "id": "dzXxTIzNKfBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "3FakOCxPKgl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13_2_rnn_classification.py"
      ],
      "metadata": {
        "id": "K7zWVLCtKi6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "KPoBOIxaKj6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13_3_char_rnn.py"
      ],
      "metadata": {
        "id": "XhWuA19xKlQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "FiJofYEzKnMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13_4_pack_pad.py"
      ],
      "metadata": {
        "id": "L7xY_JrmKojE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "hy0sI3wfKqBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14_1_seq2seq.py"
      ],
      "metadata": {
        "id": "dW5jQhhZKsST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "ubtPPVAQKtHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14_2_seq2seq_att.py"
      ],
      "metadata": {
        "id": "hMGApMnOKuBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "koqLEVDlKvUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bBFN1MP_Kk_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZfuoFA4QJ0KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CSdL6wYdJ0II"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uToGsyJZJ0Fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aLaVPY9tJz-a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7AdREOKhiKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "pytorch_to_jax",
      "provenance": [],
      "collapsed_sections": [
        "gKgs1x7JJ0Nj",
        "Ih-LpVfMJ4Bs",
        "xLav_RkcJ75s",
        "gPbAF030J_SG",
        "R2LGH_duKB-a",
        "O4NXyBocKEcf",
        "wZgP5ZwvKI6B",
        "gJy4ZIXbKNS1",
        "hBgui7E4KQLN"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}