{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinseriouspark/pytorch_with_jax/blob/main/pytorch_to_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform pytorch to jax\n",
        "\n",
        "- 활용자료 : https://github.com/hunkim/PyTorchZeroToAll"
      ],
      "metadata": {
        "id": "ynp9olWoHBz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CosN6ukkgYbf",
        "outputId": "95c305b8-55a5-4b9d-aba1-d82631021208"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform pytorch to jax"
      ],
      "metadata": {
        "id": "WPcCYoxuG22g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/sample_data/california_housing_train.csv', nrows = 100)\n",
        "feature_col = 'median_income'\n",
        "target_col = 'median_house_value'"
      ],
      "metadata": {
        "id": "FLUkG_f6h6Zs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01_basic.py"
      ],
      "metadata": {
        "id": "tzo4-YL3qfAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_basic.py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#x_data = data[feature_col].values\n",
        "#y_data = data[target_col].values\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) **2\n",
        "\n",
        "# list of weights/mean square Error (MSE) for each input\n",
        "w_list = []\n",
        "mse_list = []\n",
        "\n",
        "for w in np.arange(0.0, 4.0, 1.0):\n",
        "  l_sum = 0\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    y_pred_val = forward(x_val)\n",
        "    l = loss(x_val, y_val)\n",
        "    l_sum += l\n",
        "\n",
        "    print('\\t', x_val, y_val, y_pred_val, l)\n",
        "  print('MSE=', l_sum/ len(x_data)) # 직접 평균 계산\n",
        "  w_list.append(w)\n",
        "  mse_list.append(l_sum / len(x_data))\n",
        "# Plot it all\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wokSxfHZHOlU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "outputId": "c20819ac-7b49-438a-8dbf-436882ffa253"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01_basic with jax"
      ],
      "metadata": {
        "id": "O12vMT2Fqhur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_basic with jax\n",
        "## jax.numpy as jnp , from jax import grad, jnp.mean() 등을 사용\n",
        "\n",
        "import datetime\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "# 데이터 정의\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "# forward pass\n",
        "def forward(x, w):\n",
        "  return x * w\n",
        "# loss\n",
        "def loss(w, x, y):\n",
        "  y_pred = forward(x, w)\n",
        "  return jnp.mean((y_pred - y) **2)\n",
        "\n",
        "# grad를 계산하는 함수 생성\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "# 초기 w 값 설정\n",
        "w = 1.0\n",
        "\n",
        "# w업데이트하면서 손실감소\n",
        "num_epochs = 100\n",
        "losses = []\n",
        "lr = 0.01\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  grad_w = grad_loss(w, x_data, y_data)\n",
        "  w -= lr * grad_w\n",
        "  loss_val = loss(w, x_data, y_data)\n",
        "  losses.append(loss_val)\n",
        "  print(f'Epoch {epoch + 1}, Loss {loss_val}')\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9lydl-tGHAY1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b8a6614-222e-402f-8a01-b02d285048a5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 3.836207151412964\n",
            "Epoch 2, Loss 3.1535325050354004\n",
            "Epoch 3, Loss 2.592343330383301\n",
            "Epoch 4, Loss 2.131021499633789\n",
            "Epoch 5, Loss 1.7517943382263184\n",
            "Epoch 6, Loss 1.4400527477264404\n",
            "Epoch 7, Loss 1.1837871074676514\n",
            "Epoch 8, Loss 0.9731259346008301\n",
            "Epoch 9, Loss 0.7999526262283325\n",
            "Epoch 10, Loss 0.6575968861579895\n",
            "Epoch 11, Loss 0.5405738353729248\n",
            "Epoch 12, Loss 0.44437557458877563\n",
            "Epoch 13, Loss 0.365296334028244\n",
            "Epoch 14, Loss 0.30028998851776123\n",
            "Epoch 15, Loss 0.24685168266296387\n",
            "Epoch 16, Loss 0.20292294025421143\n",
            "Epoch 17, Loss 0.16681180894374847\n",
            "Epoch 18, Loss 0.13712671399116516\n",
            "Epoch 19, Loss 0.11272414773702621\n",
            "Epoch 20, Loss 0.09266418218612671\n",
            "Epoch 21, Loss 0.0761740654706955\n",
            "Epoch 22, Loss 0.06261858344078064\n",
            "Epoch 23, Loss 0.05147525668144226\n",
            "Epoch 24, Loss 0.04231487959623337\n",
            "Epoch 25, Loss 0.03478473424911499\n",
            "Epoch 26, Loss 0.02859465405344963\n",
            "Epoch 27, Loss 0.02350599691271782\n",
            "Epoch 28, Loss 0.019323039799928665\n",
            "Epoch 29, Loss 0.015884418040513992\n",
            "Epoch 30, Loss 0.013057682663202286\n",
            "Epoch 31, Loss 0.01073399931192398\n",
            "Epoch 32, Loss 0.008823839016258717\n",
            "Epoch 33, Loss 0.007253610994666815\n",
            "Epoch 34, Loss 0.00596278253942728\n",
            "Epoch 35, Loss 0.0049017034471035\n",
            "Epoch 36, Loss 0.004029419273138046\n",
            "Epoch 37, Loss 0.0033123798202723265\n",
            "Epoch 38, Loss 0.0027229059487581253\n",
            "Epoch 39, Loss 0.00223835208453238\n",
            "Epoch 40, Loss 0.0018400289118289948\n",
            "Epoch 41, Loss 0.0015125819481909275\n",
            "Epoch 42, Loss 0.0012434019008651376\n",
            "Epoch 43, Loss 0.0010221307165920734\n",
            "Epoch 44, Loss 0.0008402422536164522\n",
            "Epoch 45, Loss 0.0006907067727297544\n",
            "Epoch 46, Loss 0.0005678010056726635\n",
            "Epoch 47, Loss 0.00046675006160512567\n",
            "Epoch 48, Loss 0.00038369427784346044\n",
            "Epoch 49, Loss 0.00031541852513328195\n",
            "Epoch 50, Loss 0.00025928422110155225\n",
            "Epoch 51, Loss 0.00021314274636097252\n",
            "Epoch 52, Loss 0.00017521600238978863\n",
            "Epoch 53, Loss 0.00014403677778318524\n",
            "Epoch 54, Loss 0.00011840059596579522\n",
            "Epoch 55, Loss 9.73288479144685e-05\n",
            "Epoch 56, Loss 8.000661910045892e-05\n",
            "Epoch 57, Loss 6.576994201168418e-05\n",
            "Epoch 58, Loss 5.406759009929374e-05\n",
            "Epoch 59, Loss 4.444511068868451e-05\n",
            "Epoch 60, Loss 3.6536519473884255e-05\n",
            "Epoch 61, Loss 3.003445999638643e-05\n",
            "Epoch 62, Loss 2.4689192287041806e-05\n",
            "Epoch 63, Loss 2.02947612706339e-05\n",
            "Epoch 64, Loss 1.6683987269061618e-05\n",
            "Epoch 65, Loss 1.3715703971683979e-05\n",
            "Epoch 66, Loss 1.1274603821220808e-05\n",
            "Epoch 67, Loss 9.267815585189965e-06\n",
            "Epoch 68, Loss 7.619353709742427e-06\n",
            "Epoch 69, Loss 6.26398923486704e-06\n",
            "Epoch 70, Loss 5.149629487277707e-06\n",
            "Epoch 71, Loss 4.233250365359709e-06\n",
            "Epoch 72, Loss 3.480036866676528e-06\n",
            "Epoch 73, Loss 2.8608380944206147e-06\n",
            "Epoch 74, Loss 2.3515772227256093e-06\n",
            "Epoch 75, Loss 1.9329436327097937e-06\n",
            "Epoch 76, Loss 1.5888917914708145e-06\n",
            "Epoch 77, Loss 1.3059234333923087e-06\n",
            "Epoch 78, Loss 1.0738483524619369e-06\n",
            "Epoch 79, Loss 8.825445547699928e-07\n",
            "Epoch 80, Loss 7.257014544848062e-07\n",
            "Epoch 81, Loss 5.96372842665005e-07\n",
            "Epoch 82, Loss 4.902041155219194e-07\n",
            "Epoch 83, Loss 4.0302901993527485e-07\n",
            "Epoch 84, Loss 3.3120645070994215e-07\n",
            "Epoch 85, Loss 2.7232630372964195e-07\n",
            "Epoch 86, Loss 2.2384459441582294e-07\n",
            "Epoch 87, Loss 1.8416216107652872e-07\n",
            "Epoch 88, Loss 1.5112431128727621e-07\n",
            "Epoch 89, Loss 1.2432825258201774e-07\n",
            "Epoch 90, Loss 1.0216932366802212e-07\n",
            "Epoch 91, Loss 8.396483508477104e-08\n",
            "Epoch 92, Loss 6.899654181324877e-08\n",
            "Epoch 93, Loss 5.676905345808336e-08\n",
            "Epoch 94, Loss 4.665831454531144e-08\n",
            "Epoch 95, Loss 3.842738749426644e-08\n",
            "Epoch 96, Loss 3.161292028153184e-08\n",
            "Epoch 97, Loss 2.602376980576082e-08\n",
            "Epoch 98, Loss 2.139556087854544e-08\n",
            "Epoch 99, Loss 1.757437928517902e-08\n",
            "Epoch 100, Loss 1.4449810237238125e-08\n",
            "0:00:00.776515\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGzCAYAAADT4Tb9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7+ElEQVR4nO3de3RU9b3//9eeXCYJZCYJmBuEi4VylYsgELDFVhQRLaj1WA4W6vHyRcGD5bQ9jVZrtZ7g8UfVVgtSRW2VolhBD1UBo0CRINcooqAWJBEyCQjJJAEmYWb//kgyOIVgLjOzM5PnY629wuz5zMx79mqd1/rsz8UwTdMUAABAlLBZXQAAAEAwEW4AAEBUIdwAAICoQrgBAABRhXADAACiCuEGAABEFcINAACIKoQbAAAQVQg3AAAgqhBuAABAVIm1uoBG8+fPV15enubOnavHHnusyXbLly/Xvffeqy+++EJ9+/bVww8/rCuvvLLZn+Pz+XTo0CElJyfLMIwgVA4AAELNNE1VVVUpOztbNtu5+2baRbjZunWrnnrqKQ0ZMuSc7TZt2qRp06YpPz9fV111lZYuXaqpU6dqx44dGjx4cLM+69ChQ8rJyQlG2QAAIMxKSkrUvXv3c7YxrN44s7q6WhdeeKH++Mc/6re//a2GDRvWZM/NDTfcoJqaGq1atcp/bsyYMRo2bJgWLVrUrM+rrKxUSkqKSkpK5HA4gvEVAABAiLndbuXk5KiiokJOp/OcbS3vuZk9e7YmT56sCRMm6Le//e052xYWFmrevHkB5yZOnKiVK1c2+RqPxyOPx+N/XFVVJUlyOByEGwAAIkxzhpRYGm6WLVumHTt2aOvWrc1q73K5lJGREXAuIyNDLperydfk5+frN7/5TZvqBAAAkcOy2VIlJSWaO3euXnzxRSUkJITsc/Ly8lRZWek/SkpKQvZZAADAepb13Gzfvl3l5eW68MIL/ee8Xq82bNigJ554Qh6PRzExMQGvyczMVFlZWcC5srIyZWZmNvk5drtddrs9uMUDAIB2y7Kem0svvVS7du1SUVGR/xg5cqSmT5+uoqKiM4KNJOXm5qqgoCDg3Nq1a5WbmxuusgEAQDtnWc9NcnLyGdO3O3XqpC5duvjPz5gxQ926dVN+fr4kae7cuRo/frwWLFigyZMna9myZdq2bZsWL14c9voBAED71K5XKC4uLlZpaan/8dixY7V06VItXrxYQ4cO1SuvvKKVK1c2e40bAAAQ/Sxf5ybc3G63nE6nKisrmQoOAECEaMnvd7vuuQEAAGgpwg0AAIgqhBsAABBVCDcAACCqEG4AAEBUIdwEySmvT2Xukyr+6rjVpQAA0KERboJkyxdHNfp/CnTz883bBBQAAIQG4SZIUpPiJUnHjtdaXAkAAB0b4SZI0jo1hps6dbB1EQEAaFcIN0GSkhQnSfL6TLlPnrK4GgAAOi7CTZDYY2PUKb5+J/MKbk0BAGAZwk0QpTSMuzlaQ7gBAMAqhJsgahx3U3G8zuJKAADouAg3QdQ47oaeGwAArEO4CaLTM6YINwAAWIVwE0SsdQMAgPUIN0F0Otww5gYAAKsQboIotVP9mJtjjLkBAMAyhJsg4rYUAADWI9wEkT/c1HBbCgAAqxBugsh/W4qeGwAALEO4CaKv35Zi80wAAKxBuAmixnBT5zVVU+u1uBoAADomwk0QJcbHKCGu/pIyYwoAAGsQboIsjRlTAABYinATZOwMDgCAtQg3QcbO4AAAWItwE2TsDA4AgLUIN0F2uueGcAMAgBUIN0HmH3NDuAEAwBKEmyBLS2pcpZgxNwAAWIFwE2SpnRr3l6LnBgAAKxBuguz0Fgz03AAAYAVLw83ChQs1ZMgQORwOORwO5ebm6s0332yy/XPPPSfDMAKOhISEMFb8zU7vDE7PDQAAVoi18sO7d++u+fPnq2/fvjJNU88//7ymTJminTt3atCgQWd9jcPh0N69e/2PDcMIV7nN8vWdwU3TbHf1AQAQ7SwNN1dffXXA44ceekgLFy7U5s2bmww3hmEoMzOz2Z/h8Xjk8Xj8j91ud+uKbabGnhvPKZ9O1HmVFG/pJQYAoMNpN2NuvF6vli1bppqaGuXm5jbZrrq6Wj179lROTo6mTJmi3bt3n/N98/Pz5XQ6/UdOTk6wSw+QFB+j+NiGzTMZdwMAQNhZHm527dqlzp07y263a9asWVqxYoUGDhx41rb9+vXTkiVL9Nprr+mFF16Qz+fT2LFj9eWXXzb5/nl5eaqsrPQfJSUlofoqkup7llIbp4Mz7gYAgLCz/J5Jv379VFRUpMrKSr3yyiuaOXOm1q9ff9aAk5ubG9CrM3bsWA0YMEBPPfWUHnzwwbO+v91ul91uD1n9Z5OaFK8yt4edwQEAsIDl4SY+Pl59+vSRJI0YMUJbt27V448/rqeeeuobXxsXF6fhw4fr888/D3WZLZLKzuAAAFjG8ttS/8rn8wUMAD4Xr9erXbt2KSsrK8RVtQw7gwMAYB1Le27y8vI0adIk9ejRQ1VVVVq6dKnWrVun1atXS5JmzJihbt26KT8/X5L0wAMPaMyYMerTp48qKir0yCOP6MCBA7rlllus/BpnYGdwAACsY2m4KS8v14wZM1RaWiqn06khQ4Zo9erVuuyyyyRJxcXFstlOdy4dO3ZMt956q1wul1JTUzVixAht2rSpyQHIVmm8LcXO4AAAhJ9hmqZpdRHh5Ha75XQ6VVlZKYfDEZLPeGbjfj246mNdPTRbf5g2PCSfAQBAR9KS3+92N+YmGjROBafnBgCA8CPchEDjzuCMuQEAIPwINyFweswNs6UAAAg3wk0IpLHODQAAliHchEBKw87gJ+q8OlnntbgaAAA6FsJNCCTbYxVrMySJLRgAAAgzwk0IGIahlIZbU8dqGHcDAEA4EW5CJK3h1hQ9NwAAhBfhJkT8PTeEGwAAwopwEyJp/ttShBsAAMKJcBMiqf7bUoy5AQAgnAg3IZLKWjcAAFiCcBMi7AwOAIA1CDch4t9fittSAACEFeEmRNgZHAAAaxBuQoSdwQEAsAbhJkTYGRwAAGsQbkKkcZ2bas8p1Z7yWVwNAAAdB+EmRJITYtWwdybjbgAACCPCTYjYbMbptW4INwAAhA3hJoRSGmZMsTM4AADhQ7gJobRObJ4JAEC4EW5CKIUtGAAACDvCTQh17WyXJH1VTbgBACBcCDchdF5yfbg5XH3S4koAAOg4CDchdF7n+ttSh6s8FlcCAEDHQbgJIX/PDeEGAICwIdyEUGO4OcKYGwAAwoZwE0KNA4oPV3lkmqbF1QAA0DEQbkKoMdycqPOqptZrcTUAAHQMhJsQ6mSPVaf4GEnSEcbdAAAQFoSbEDs9HZxwAwBAOBBuQuzr424AAEDoWRpuFi5cqCFDhsjhcMjhcCg3N1dvvvnmOV+zfPly9e/fXwkJCbrgggv0xhtvhKna1mE6OAAA4WVpuOnevbvmz5+v7du3a9u2bfr+97+vKVOmaPfu3Wdtv2nTJk2bNk0333yzdu7cqalTp2rq1Kn66KOPwlx5852eDk64AQAgHAyznc1RTktL0yOPPKKbb775jOduuOEG1dTUaNWqVf5zY8aM0bBhw7Ro0aKzvp/H45HHczpYuN1u5eTkqLKyUg6HI/hf4F/8vuAz/W7tp/rRRTmaf92QkH8eAADRyO12y+l0Nuv3u92MufF6vVq2bJlqamqUm5t71jaFhYWaMGFCwLmJEyeqsLCwyffNz8+X0+n0Hzk5OUGt+5twWwoAgPCyPNzs2rVLnTt3lt1u16xZs7RixQoNHDjwrG1dLpcyMjICzmVkZMjlcjX5/nl5eaqsrPQfJSUlQa3/m5zXmdlSAACEU6zVBfTr109FRUWqrKzUK6+8opkzZ2r9+vVNBpyWstvtstvtQXmv1vCPuaHnBgCAsLA83MTHx6tPnz6SpBEjRmjr1q16/PHH9dRTT53RNjMzU2VlZQHnysrKlJmZGZZaW6Pr19a5MU1ThmFYXBEAANHN8ttS/8rn8wUMAP663NxcFRQUBJxbu3Ztk2N02oOuneMlSXVeU5Un6iyuBgCA6Gdpz01eXp4mTZqkHj16qKqqSkuXLtW6deu0evVqSdKMGTPUrVs35efnS5Lmzp2r8ePHa8GCBZo8ebKWLVumbdu2afHixVZ+jXOyx8bImRinyhN1OlLtUUpSvNUlAQAQ1SwNN+Xl5ZoxY4ZKS0vldDo1ZMgQrV69Wpdddpkkqbi4WDbb6c6lsWPHaunSpfrVr36lu+++W3379tXKlSs1ePBgq75Cs3TtHK/KE3Uqr/KoT3qy1eUAABDV2t06N6HWknnywfKjxYXavO+oHv/RME0Z1i0snwkAQDSJyHVuotl5yQmSWOsGAIBwINyEQeNaN0eqay2uBACA6Ee4CYOuyfWDiOm5AQAg9Ag3YcAqxQAAhA/hJgzYXwoAgPAh3IRBV/+YG8INAAChRrgJg/SGnpuvqj3y+jrUzHsAAMKOcBMGaZ3iZRiSz5SO1jBjCgCAUCLchEFsjE1dOjFjCgCAcCDchAnjbgAACA/CTZgwYwoAgPAg3IQJa90AABAehJsw6drQc3OEnhsAAEKKcBMm9NwAABAehJswYcwNAADhQbgJE8INAADhQbgJE6aCAwAQHoSbMGnsuTl2vE61p3wWVwMAQPQi3IRJSmKcYm2GJOmrGnpvAAAIFcJNmNhshrp0ZgsGAABCjXATRo23phh3AwBA6BBuwsi/1g09NwAAhAzhJoyYDg4AQOgRbsLo9HTwWosrAQAgehFuwoieGwAAQo9wE0aEGwAAQo9wE0Zd2TwTAICQI9yEET03AACEHuEmjDIcCZKkas8pVXtOWVwNAADRiXATRp3tsUpOiJUklVacsLgaAACiE+EmzLKdiZKkQ5UnLa4EAIDoRLgJs6yU+ltT9NwAABAaloab/Px8XXTRRUpOTlZ6erqmTp2qvXv3nvM1zz33nAzDCDgSEhLCVHHbZdFzAwBASFkabtavX6/Zs2dr8+bNWrt2rerq6nT55ZerpqbmnK9zOBwqLS31HwcOHAhTxW2X7awPYq5Kem4AAAiFWCs//K233gp4/Nxzzyk9PV3bt2/Xd7/73SZfZxiGMjMzQ11eSGQ2hJtSem4AAAiJdjXmprKyUpKUlpZ2znbV1dXq2bOncnJyNGXKFO3evbvJth6PR263O+CwUnZKw20pxtwAABAS7Sbc+Hw+3XXXXRo3bpwGDx7cZLt+/fppyZIleu211/TCCy/I5/Np7Nix+vLLL8/aPj8/X06n03/k5OSE6is0S9bXem5M07S0FgAAopFhtpNf2Ntvv11vvvmmNm7cqO7duzf7dXV1dRowYICmTZumBx988IznPR6PPJ7TKwK73W7l5OSosrJSDocjKLW3xIlarwbcV3877oP7LpczKS7sNQAAEGncbrecTmezfr8tHXPTaM6cOVq1apU2bNjQomAjSXFxcRo+fLg+//zzsz5vt9tlt9uDUWZQJMbHKDUpTseO1+lQ5QnCDQAAQWbpbSnTNDVnzhytWLFC77zzjnr37t3i9/B6vdq1a5eysrJCUGFoNE4HL2XGFAAAQWdpuJk9e7ZeeOEFLV26VMnJyXK5XHK5XDpx4vSP/owZM5SXl+d//MADD2jNmjXat2+fduzYoRtvvFEHDhzQLbfcYsVXaJXsFGZMAQAQKpbellq4cKEk6ZJLLgk4/+yzz+onP/mJJKm4uFg22+kMduzYMd16661yuVxKTU3ViBEjtGnTJg0cODBcZbeZv+emgnADAECwWRpumjOWed26dQGPH330UT366KMhqig8Gte6OcRtKQAAgq7dTAXvSPy3pei5AQAg6Ag3FmBAMQAAoUO4sUC2P9ywkB8AAMFGuLFAhrN+3R3PKZ+O1tRaXA0AANGFcGMBe2yMunauDzhMBwcAILgINxZhrRsAAEKDcGORTEdjuGFQMQAAwUS4sUh2Sv2g4kNMBwcAIKgINxbJctJzAwBAKBBuLJKVwhYMAACEAuHGItmNPTduem4AAAgmwo1FGntuXJUn5fOxkB8AAMFCuLFIRrJdNkOq85o6UuOxuhwAAKIG4cYisTE2pSezgSYAAMFGuLFQJjOmAAAIOsKNhRpXKWatGwAAgodwY6Es/+7g9NwAABAshBsLnV7Ij54bAACChXBjocYtGAg3AAAED+HGQv6emwpuSwEAECyEGws19tyUVXnkZSE/AACCgnBjoa6d7Yq1GfL6TJVXcWsKAIBgINxYKMZmKMPBdHAAAIKJcGOxxnE3LgYVAwAQFIQbi2WlsNYNAADBRLixWOMqxV8eI9wAABAMhBuL9UzrJEk68FWNxZUAABAdCDcW69klSZJ04OhxiysBACA6EG4s1iOtPtx8efQEa90AABAEhBuLZackKi7GUK3XJ5ebGVMAALQV4cZiMTZD3VMbbk0x7gYAgDYj3LQDjbemir9i3A0AAG1FuGkHGFQMAEDwtCrclJSU6Msvv/Q/3rJli+666y4tXry4Re+Tn5+viy66SMnJyUpPT9fUqVO1d+/eb3zd8uXL1b9/fyUkJOiCCy7QG2+80eLv0J7QcwMAQPC0Ktz8+7//u959911Jksvl0mWXXaYtW7bonnvu0QMPPNDs91m/fr1mz56tzZs3a+3ataqrq9Pll1+umpqmx55s2rRJ06ZN080336ydO3dq6tSpmjp1qj766KPWfJV2oWeXhrVujjLmBgCAtjJM02zx/OPU1FRt3rxZ/fr10+9//3u99NJLeu+997RmzRrNmjVL+/bta1Uxhw8fVnp6utavX6/vfve7Z21zww03qKamRqtWrfKfGzNmjIYNG6ZFixZ942e43W45nU5VVlbK4XC0qs5g+6ysSpc9ukHJCbH68NeXyzAMq0sCAKBdacnvd6t6burq6mS32yVJb7/9tn7wgx9Ikvr376/S0tLWvKUkqbKyUpKUlpbWZJvCwkJNmDAh4NzEiRNVWFh41vYej0dutzvgaG9yGm5LVZ08pWPH6yyuBgCAyNaqcDNo0CAtWrRI//jHP7R27VpdccUVkqRDhw6pS5curSrE5/Pprrvu0rhx4zR48OAm27lcLmVkZAScy8jIkMvlOmv7/Px8OZ1O/5GTk9Oq+kIpIS5GmY76PaaYDg4AQNu0Ktw8/PDDeuqpp3TJJZdo2rRpGjp0qCTp9ddf16hRo1pVyOzZs/XRRx9p2bJlrXp9U/Ly8lRZWek/SkpKgvr+wdKjYcZUMTOmAABok9jWvOiSSy7RkSNH5Ha7lZqa6j9/2223KSkpqcXvN2fOHK1atUobNmxQ9+7dz9k2MzNTZWVlAefKysqUmZl51vZ2u91/C60965mWpC37j+oAM6YAAGiTVvXcnDhxQh6Pxx9sDhw4oMcee0x79+5Venp6s9/HNE3NmTNHK1as0DvvvKPevXt/42tyc3NVUFAQcG7t2rXKzc1t2ZdoZ/xr3RBuAABok1aFmylTpujPf/6zJKmiokKjR4/WggULNHXqVC1cuLDZ7zN79my98MILWrp0qZKTk+VyueRyuXTixAl/mxkzZigvL8//eO7cuXrrrbe0YMEC7dmzR/fff7+2bdumOXPmtOartBs9GqaDFzMdHACANmlVuNmxY4e+853vSJJeeeUVZWRk6MCBA/rzn/+s3//+981+n4ULF6qyslKXXHKJsrKy/MdLL73kb1NcXBwwA2vs2LFaunSpFi9erKFDh+qVV17RypUrzzkIORL0TKPnBgCAYGjVmJvjx48rOTlZkrRmzRpde+21stlsGjNmjA4cONDs92nOEjvr1q0749z111+v66+/vtmfEwkab0uVV3l0otarxPgYiysCACAytarnpk+fPlq5cqVKSkq0evVqXX755ZKk8vLydrMwXqRJSYqXI6E+azJjCgCA1mtVuLnvvvv0s5/9TL169dKoUaP8g3nXrFmj4cOHB7XAjqRX14ZtGFjrBgCAVmvVbakf/vCHuvjii1VaWupf40aSLr30Ul1zzTVBK66j6ZGWpA+/rGTcDQAAbdCqcCPVrzeTmZnp3x28e/furV7AD/X808GZMQUAQKu16raUz+fTAw88IKfTqZ49e6pnz55KSUnRgw8+KJ/PF+waO4yeaY23pei5AQCgtVrVc3PPPffomWee0fz58zVu3DhJ0saNG3X//ffr5MmTeuihh4JaZEfBFgwAALRdq8LN888/r6efftq/G7gkDRkyRN26ddMdd9xBuGmlxttSB4+d0CmvT7ExrepYAwCgQ2vVr+fRo0fVv3//M873799fR48ebXNRHVVGcoLiY2065TN1qOKk1eUAABCRWhVuhg4dqieeeOKM80888YSGDBnS5qI6KpvNUI80BhUDANAWrbot9b//+7+aPHmy3n77bf8aN4WFhSopKdEbb7wR1AI7mp5pSfq8vFoHvjqu7/S1uhoAACJPq3puxo8fr08//VTXXHONKioqVFFRoWuvvVa7d+/WX/7yl2DX2KH09G+gyaBiAABao9Xr3GRnZ58xcPiDDz7QM888o8WLF7e5sI6qcVDxF0e4LQUAQGswHaedYTo4AABtQ7hpZ3qmnQ43zdk1HQAABCLctDPdU5NkM6TjtV6VV3msLgcAgIjTojE311577Tmfr6ioaEstkBQfa1OvLp2070iNPiurVoYjweqSAACIKC0KN06n8xufnzFjRpsKgtQnvXN9uCmv0sV9u1pdDgAAEaVF4ebZZ58NVR34mm9nJGvNx2X6tKza6lIAAIg4jLlph/pmdJYkfVZWZXElAABEHsJNO9Q3PVmS9GlZFTOmAABoIcJNO3T+eZ1kMyT3yVM6zIwpAABahHDTDiXExahXwzYMjLsBAKBlCDftVOO4m08ZdwMAQIsQbtqpxnE3n5XTcwMAQEsQbtopZkwBANA6hJt26tsZzJgCAKA1CDftVO+up2dMsccUAADNR7hpp74+Y+ozZkwBANBshJt2jBlTAAC0HOGmHTs9Y4pwAwBAcxFu2rHTM6a4LQUAQHMRbtoxZkwBANByhJt2jBlTAAC0nKXhZsOGDbr66quVnZ0twzC0cuXKc7Zft26dDMM443C5XOEpOMwC95hi3A0AAM1habipqanR0KFD9eSTT7bodXv37lVpaan/SE9PD1GF1mPcDQAALRNr5YdPmjRJkyZNavHr0tPTlZKSEvyC2qG+6clavbuMGVMAADRTRI65GTZsmLKysnTZZZfpvffeO2dbj8cjt9sdcESS02vd0HMDAEBzRFS4ycrK0qJFi/S3v/1Nf/vb35STk6NLLrlEO3bsaPI1+fn5cjqd/iMnJyeMFbdd44ypz5gxBQBAsxhmO/nFNAxDK1as0NSpU1v0uvHjx6tHjx76y1/+ctbnPR6PPJ7TM43cbrdycnJUWVkph8PRlpLDwnPKqwH3viWfKb1/96XKcCRYXRIAAGHndrvldDqb9fsdUT03ZzNq1Ch9/vnnTT5vt9vlcDgCjkhij2XGFAAALRHx4aaoqEhZWVlWlxFSjLsBAKD5LJ0tVV1dHdDrsn//fhUVFSktLU09evRQXl6eDh48qD//+c+SpMcee0y9e/fWoEGDdPLkST399NN65513tGbNGqu+Qlh8O6N+xtSnLnpuAAD4JpaGm23btul73/ue//G8efMkSTNnztRzzz2n0tJSFRcX+5+vra3Vf/3Xf+ngwYNKSkrSkCFD9Pbbbwe8RzQakFV/K213aaXFlQAA0P61mwHF4dKSAUntRcnR4/rO/76ruBhDu+6fqIS4GKtLAgAgrDrUgOKOoHtqolKT4lTnNbWXW1MAAJwT4SYCGIahC7qnSJI+PMitKQAAzoVwEyGGdHNKknZ9WWFtIQAAtHOEmwhxQff6cPPhl/TcAABwLoSbCDGkIdx8Vl6tE7Vei6sBAKD9ItxEiExHgrp2tsvrM/VxaWRt/gkAQDgRbiKEYRj+3hvG3QAA0DTCTQS5oGFQMTOmAABoGuEmgjT23HxEuAEAoEmEmwjS2HPzeXm1ajynLK4GAID2iXATQdIdCcp0JMhnikHFAAA0gXATYVjvBgCAcyPcRBhWKgYA4NwINxHG33PDoGIAAM6KcBNhGgcV7ztco6qTdRZXAwBA+0O4iTBdOtvVLSVRkvTRQQYVAwDwrwg3Eaix92bXwQprCwEAoB0i3EQgZkwBANA0wk0E8u8xxaBiAADOQLiJQI23pQ58dVyVxxlUDADA1xFuIlBKUrx6dkmSJO0oPmZxNQAAtC+Emwg1qleaJGnz/q8srgQAgPaFcBOhRp/fRZL0/r6jFlcCAED7QriJUKN71/fc7DpYyQ7hAAB8DeEmQuWkJalbSqK8PlPbDjDuBgCARoSbCDb6/Prem/f3Me4GAIBGhJsINqZ3w7ib/Yy7AQCgEeEmgjX23Hz4ZYVO1HotrgYAgPaBcBPBeqQlKcuZoDqvyXo3AAA0INxEMMMw/LOmNjPuBgAASYSbiMd6NwAABCLcRLjGnpuikgqdrGPcDQAAhJsI17trJ6Un21Xr9WlncYXV5QAAYDlLw82GDRt09dVXKzs7W4ZhaOXKld/4mnXr1unCCy+U3W5Xnz599Nxzz4W8zvbMMIzTt6bYZwoAAGvDTU1NjYYOHaonn3yyWe3379+vyZMn63vf+56Kiop011136ZZbbtHq1atDXGn7xqBiAABOi7XywydNmqRJkyY1u/2iRYvUu3dvLViwQJI0YMAAbdy4UY8++qgmTpx41td4PB55PB7/Y7fb3bai26ExDevd7CyukOeUV/bYGIsrAgDAOhE15qawsFATJkwIODdx4kQVFhY2+Zr8/Hw5nU7/kZOTE+oyw+5b53VW187x8pzy6YOSSqvLAQDAUhEVblwulzIyMgLOZWRkyO1268SJE2d9TV5eniorK/1HSUlJOEoNq/r1bhqnhHNrCgDQsUVUuGkNu90uh8MRcESjxq0YNv2TcAMA6NgiKtxkZmaqrKws4FxZWZkcDocSExMtqqp9+E7f8yRJW784KvfJOourAQDAOhEVbnJzc1VQUBBwbu3atcrNzbWoovajd9dOOv+8TjrlM7Xh08NWlwMAgGUsDTfV1dUqKipSUVGRpPqp3kVFRSouLpZUP15mxowZ/vazZs3Svn379Itf/EJ79uzRH//4R7388sv66U9/akX57c6EAfXjkQo+Kbe4EgAArGNpuNm2bZuGDx+u4cOHS5LmzZun4cOH67777pMklZaW+oOOJPXu3Vt///vftXbtWg0dOlQLFizQ008/3eQ08I7m0v7pkqR395brlNdncTUAAFjDME3TtLqIcHK73XI6naqsrIy6wcWnvD6N+O3bqjxRp5f/X65GNSzuBwBApGvJ73dEjbnBucXG2HRJv/qBxQWflH1DawAAohPhJspc2jDu5m3CDQCggyLcRJnx3z5PsTZD/zxcoy+O1FhdDgAAYUe4iTLOxDhd1Kt+rA29NwCAjohwE4UuHVA/a+qdPUwJBwB0PISbKNS43s2W/axWDADoeAg3UahX1076VsNqxev3sloxAKBjIdxEqdOrFTPuBgDQsRBuotT3/asVH2a1YgBAh0K4iVIjeqbKmRinyhN12vrFMavLAQAgbAg3USo2xqbLB9bfmnqt6KDF1QAAED6Emyh2zYXdJEl/31Wqk3Vei6sBACA8CDdRbEzvLsp2Jqjq5CkW9AMAdBiEmyhmsxmaOry+92bFDm5NAQA6BsJNlLu24dbUuk8P60i1x+JqAAAIPcJNlOuTnqwh3Z3y+kz93weHrC4HAICQI9x0ANc23Jp6lVtTAIAOgHDTAVw9NFuxNkO7Dlbqs7Iqq8sBACCkCDcdQJfOdl3S7zxJ0qs76b0BAEQ3wk0Hce2F3SVJK3celM9nWlwNAAChQ7jpIL7fP13JCbEqrTypzfu+srocAABChnDTQSTExeiqIdmSpL8xsBgAEMUINx3Idf7tGA7pWE2txdUAABAahJsOZETPVA3KduhknU9LtxRbXQ4AACFBuOlADMPQzRf3liQ9v+kL1Z7yWVwRAADBR7jpYK4akq30ZLvKqzxa9SErFgMAog/hpoOJj7Vp5thekqSn/7Ffpsm0cABAdCHcdEDTR/dQYlyMPi51q5Bp4QCAKEO46YBSkuJ13Yj6mVNLNu63uBoAAIKLcNNB/ce4+oHFb39Srn2Hqy2uBgCA4CHcdFDnn9dZEwakS5KWvEfvDQAgehBuOrCbLz5fkvTK9i9VcZxF/QAA0YFw04GNOT/Nv6jf0/+g9wYAEB3aRbh58skn1atXLyUkJGj06NHasmVLk22fe+45GYYRcCQkJISx2uhhGIbu/H5fSdIzG/er3H3S4ooAAGg7y8PNSy+9pHnz5unXv/61duzYoaFDh2rixIkqLy9v8jUOh0OlpaX+48CBA2GsOLpMHJSh4T1SdKLOq8cKPrO6HAAA2szycPO73/1Ot956q2666SYNHDhQixYtUlJSkpYsWdLkawzDUGZmpv/IyMhosq3H45Hb7Q44cJphGMqbNECS9NLWEv2TmVMAgAhnabipra3V9u3bNWHCBP85m82mCRMmqLCwsMnXVVdXq2fPnsrJydGUKVO0e/fuJtvm5+fL6XT6j5ycnKB+h2gwqneaJgxIl9dn6pG39lpdDgAAbWJpuDly5Ii8Xu8ZPS8ZGRlyuVxnfU2/fv20ZMkSvfbaa3rhhRfk8/k0duxYffnll2dtn5eXp8rKSv9RUlIS9O8RDX4+sb9shvTWbpd2FB+zuhwAAFrN8ttSLZWbm6sZM2Zo2LBhGj9+vF599VWdd955euqpp87a3m63y+FwBBw4U7/MZF13YXdJ0vw39rDnFAAgYlkabrp27aqYmBiVlZUFnC8rK1NmZmaz3iMuLk7Dhw/X559/HooSO5SfXvZt2WNt2vLFUb2zp+kB3QAAtGeWhpv4+HiNGDFCBQUF/nM+n08FBQXKzc1t1nt4vV7t2rVLWVlZoSqzw8hOSdRPxvWSJOW/uUe1p3zWFgQAQCtYfltq3rx5+tOf/qTnn39en3zyiW6//XbV1NTopptukiTNmDFDeXl5/vYPPPCA1qxZo3379mnHjh268cYbdeDAAd1yyy1WfYWocsf4PkrrFK/Py6v15Lv0hgEAIk+s1QXccMMNOnz4sO677z65XC4NGzZMb731ln+QcXFxsWy20xns2LFjuvXWW+VyuZSamqoRI0Zo06ZNGjhwoFVfIao4k+L0mx8M0p1/3akn3/1cVwzO1IAsxikBACKHYXawkaNut1tOp1OVlZUMLm6CaZr6f3/ZrjUfl2lwN4dW3DFOcTGWd/IBADqwlvx+84uFMxiGod9OHSxnYpw+OujW4g37rC4JAIBmI9zgrNIdCbrvqvpbfY+//Zk+K6uyuCIAAJqHcIMmXXthN13S7zzVen36+SsfyuvrUHcwAQARinCDJhmGofxrL1CyPVZFJRVauI7ZUwCA9o9wg3PKcibq3qvrb08tWPup3t3L4n4AgPaNcINv9G8jczRtVI5MU5r715364kiN1SUBANAkwg2a5f4fDNLwHilynzyl2/6yTTWeU1aXBADAWRFu0Cz22BgtunGEzku269Oyav38lQ/YXBMA0C4RbtBsGY4ELbrxQsXFGHpjl0t/XPdPq0sCAOAMhBu0yIieabr/B4MkSY+s3quXt5VYXBEAAIEIN2ix6aN76qaG3cP/+28fauXOg9YWBADA1xBu0Cr3XTVQ/z66h0xTmvdykf7+YanVJQEAIIlwg1YyDEO/nTJY14/oLp8pzV22U2t2u6wuCwAAwg1az2YzNP+6IZo6LFunfKZmL92h1QQcAIDFCDdokxibof/v+qGafEGW6rymZr2wXX/asI9p4gAAyxBu0GaxMTY99qNh/jE4D73xie5esUt1Xp/VpQEAOiDCDYIiLsamh6YO1r1XDZRhSH/dUqKZS7ao8nid1aUBADoYwg2CxjAM3Xxxbz09Y6Q6xcdo0z+/0tQ/vqddX1ZaXRoAoAMh3CDoLh2QoVduH6tsZ4L2H6nRNX98T38o+EynuE0FAAgDwg1CYkCWQ3//z+/oygsydcpnasHaT3X9U4XsKA4ACDnCDUImtVO8nvz3C/XoDUOVbI/VzuIKTXr8H3pm434GGwMAQoZwg5AyDEPXDO+ut376XY05P00n6rx6cNXHmvjYBr27p9zq8gAAUYhwg7DolpKopbeM0f9cc4G6dIrXvsM1uum5rZq5ZIs+K6uyujwAQBQxzA622prb7ZbT6VRlZaUcDofV5XRI7pN1evKdz7Xkvf2q85qyGdLkIdmaNf58Dcp2Wl0eAKAdasnvN+EGlvniSI3y3/xEq3eX+c+N//Z5uv2Sb2l07zQZhmFhdQCA9oRwcw6Em/bn40NuPbXhn/q/Dw7J1/C/xkHZDt1wUY6mDO0mZ1KctQUCACxHuDkHwk37VfzVcS3+xz/18rYvVXuqfjaVPdamKwZn6t9G5mjM+V0UY6M3BwA6IsLNORBu2r9jNbVasfOgXt5Woj2u04ONu3SK14QBGZo4OENjv9VVCXExFlYJAAgnws05EG4ih2ma2nWwUi9tLdGqD0tVeeL0PlWd4mP0nb7naVyfLhrbp6vO79qJMToAEMUIN+dAuIlMdV6ftu4/qrd2u7Rmd5lc7pMBz2c6EjT2W100vGeqhuekqF9msuJiWOkAAKIF4eYcCDeRz+er79HZ+PkRvff5EW07cMw/RqeRPdamC7o5NbibUwOyktUv06FvZ3RWUnysRVUDANqCcHMOhJvoc7LOq+0Hjmnzvq9UVFKhopIKVZ08dUY7w5B6pCXp/K6d1KtrJ/Xu2km9utQfmc4ExcfS0wMA7VXEhZsnn3xSjzzyiFwul4YOHao//OEPGjVqVJPtly9frnvvvVdffPGF+vbtq4cfflhXXnllsz6LcBP9fD5T+7+qUVFxhT4udWuPy629riodqa5t8jWGIaUn29UtJVHZKYnKcCQoPdmudIdd6ckJOi/ZrrRO8UpNimfGFgBYIKLCzUsvvaQZM2Zo0aJFGj16tB577DEtX75ce/fuVXp6+hntN23apO9+97vKz8/XVVddpaVLl+rhhx/Wjh07NHjw4G/8PMJNx3Wk2qNPXVXa/1WNvjhSo/1HjuuLr2pUfPT4Gbe1mmIYUkpinFIbgo4zMc5/OBJi1TkhVp3tceqcEKtke6yS4mPUyR6rxPgYdYqv/5sYF6O4GIMB0ADQAhEVbkaPHq2LLrpITzzxhCTJ5/MpJydHd955p375y1+e0f6GG25QTU2NVq1a5T83ZswYDRs2TIsWLfrGzyPc4F+Zpqmvamp18NgJHao4oYMVJ1Re5VG5+6TKqzwqc5/UkeragNlabWUzpMS4GCU0HPGxNtkbjviGIy7GpvgYm+Ji6//G2gzFxdoUZzMUG2NTbIyhWJuhGJut4a/h/9t42IyGfxuGbDZDNkOKsdUHqxij/rGtoZ3NkGyGITX8bXxsqH4D1IanZLM1npOk0+cN4/R5o+F8o8Zz/n83PHdGu6+1//qZs+XA022NgMcBbZqRH42zvDKcuZOMi2gUH2tTenJCUN+zJb/flo6urK2t1fbt25WXl+c/Z7PZNGHCBBUWFp71NYWFhZo3b17AuYkTJ2rlypVnbe/xeOTxePyP3W532wtHVDEMQ10729W1s11Dc1KabFfn9anieJ2O1tTqqxqP3CfqVPm1w33ilGo8p1TlOaWqk3Wq9pzScY9XNbWndLzWq+O1XnkblmD2mVJNrVc1td4wfUsACJ8Le6To1TvGWfb5loabI0eOyOv1KiMjI+B8RkaG9uzZc9bXuFyus7Z3uVxnbZ+fn6/f/OY3wSkYHVpcjE3nJdt1XrJdUnKLX2+apmq9Pp2s88lT59WJhsNT51Ot1ydPnU+eU155TvlU5/Wp9pRPdV5Ttae8qvOaqvP5dMprqs5bf97r8+mUz5TXZ/ofe32q/2s2/PWZ8vokn1nf7ut/fWb9+CSfacpUfeAyzYbHZuDjxn+bkmTK/xrTlEzVt2/sAza/9lx9c/Nr/z59Xl873/hc4+sDHwdew68/d8aLz31K/9pR3Zxu69b2bZvNevfwsX50JToSq5fiiPp5sXl5eQE9PW63Wzk5ORZWhI7KMAzZY2Nkj42REtkvCwBCxdJw07VrV8XExKisrCzgfFlZmTIzM8/6mszMzBa1t9vtstvtwSkYAAC0e5b2G8XHx2vEiBEqKCjwn/P5fCooKFBubu5ZX5ObmxvQXpLWrl3bZHsAANCxWH5bat68eZo5c6ZGjhypUaNG6bHHHlNNTY1uuukmSdKMGTPUrVs35efnS5Lmzp2r8ePHa8GCBZo8ebKWLVumbdu2afHixVZ+DQAA0E5YHm5uuOEGHT58WPfdd59cLpeGDRumt956yz9ouLi4WDbb6Q6msWPHaunSpfrVr36lu+++W3379tXKlSubtcYNAACIfpavcxNurHMDAEDkacnvN5vpAACAqEK4AQAAUYVwAwAAogrhBgAARBXCDQAAiCqEGwAAEFUINwAAIKoQbgAAQFQh3AAAgKhi+fYL4da4ILPb7ba4EgAA0FyNv9vN2Vihw4WbqqoqSVJOTo7FlQAAgJaqqqqS0+k8Z5sOt7eUz+fToUOHlJycLMMwgvrebrdbOTk5KikpYd+qEONahw/XOny41uHDtQ6fYF1r0zRVVVWl7OzsgA21z6bD9dzYbDZ17949pJ/hcDj4P0uYcK3Dh2sdPlzr8OFah08wrvU39dg0YkAxAACIKoQbAAAQVQg3QWS32/XrX/9adrvd6lKiHtc6fLjW4cO1Dh+udfhYca073IBiAAAQ3ei5AQAAUYVwAwAAogrhBgAARBXCDQAAiCqEmyB58skn1atXLyUkJGj06NHasmWL1SVFvPz8fF100UVKTk5Wenq6pk6dqr179wa0OXnypGbPnq0uXbqoc+fOuu6661RWVmZRxdFj/vz5MgxDd911l/8c1zp4Dh48qBtvvFFdunRRYmKiLrjgAm3bts3/vGmauu+++5SVlaXExERNmDBBn332mYUVRyav16t7771XvXv3VmJior71rW/pwQcfDNibiGvdehs2bNDVV1+t7OxsGYahlStXBjzfnGt79OhRTZ8+XQ6HQykpKbr55ptVXV3d9uJMtNmyZcvM+Ph4c8mSJebu3bvNW2+91UxJSTHLysqsLi2iTZw40Xz22WfNjz76yCwqKjKvvPJKs0ePHmZ1dbW/zaxZs8ycnByzoKDA3LZtmzlmzBhz7NixFlYd+bZs2WL26tXLHDJkiDl37lz/ea51cBw9etTs2bOn+ZOf/MR8//33zX379pmrV682P//8c3+b+fPnm06n01y5cqX5wQcfmD/4wQ/M3r17mydOnLCw8sjz0EMPmV26dDFXrVpl7t+/31y+fLnZuXNn8/HHH/e34Vq33htvvGHec8895quvvmpKMlesWBHwfHOu7RVXXGEOHTrU3Lx5s/mPf/zD7NOnjzlt2rQ210a4CYJRo0aZs2fP9j/2er1mdna2mZ+fb2FV0ae8vNyUZK5fv940TdOsqKgw4+LizOXLl/vbfPLJJ6Yks7Cw0KoyI1pVVZXZt29fc+3ateb48eP94YZrHTz//d//bV588cVNPu/z+czMzEzzkUce8Z+rqKgw7Xa7+de//jUcJUaNyZMnm//xH/8RcO7aa681p0+fbpom1zqY/jXcNOfafvzxx6Ykc+vWrf42b775pmkYhnnw4ME21cNtqTaqra3V9u3bNWHCBP85m82mCRMmqLCw0MLKok9lZaUkKS0tTZK0fft21dXVBVz7/v37q0ePHlz7Vpo9e7YmT54ccE0lrnUwvf766xo5cqSuv/56paena/jw4frTn/7kf37//v1yuVwB19rpdGr06NFc6xYaO3asCgoK9Omnn0qSPvjgA23cuFGTJk2SxLUOpeZc28LCQqWkpGjkyJH+NhMmTJDNZtP777/fps/vcBtnBtuRI0fk9XqVkZERcD4jI0N79uyxqKro4/P5dNddd2ncuHEaPHiwJMnlcik+Pl4pKSkBbTMyMuRyuSyoMrItW7ZMO3bs0NatW894jmsdPPv27dPChQs1b9483X333dq6dav+8z//U/Hx8Zo5c6b/ep7tvylc65b55S9/Kbfbrf79+ysmJkZer1cPPfSQpk+fLklc6xBqzrV1uVxKT08PeD42NlZpaWltvv6EG0SE2bNn66OPPtLGjRutLiUqlZSUaO7cuVq7dq0SEhKsLieq+Xw+jRw5Uv/zP/8jSRo+fLg++ugjLVq0SDNnzrS4uujy8ssv68UXX9TSpUs1aNAgFRUV6a677lJ2djbXOspxW6qNunbtqpiYmDNmjZSVlSkzM9OiqqLLnDlztGrVKr377rvq3r27/3xmZqZqa2tVUVER0J5r33Lbt29XeXm5LrzwQsXGxio2Nlbr16/X73//e8XGxiojI4NrHSRZWVkaOHBgwLkBAwaouLhYkvzXk/+mtN3Pf/5z/fKXv9SPfvQjXXDBBfrxj3+sn/70p8rPz5fEtQ6l5lzbzMxMlZeXBzx/6tQpHT16tM3Xn3DTRvHx8RoxYoQKCgr853w+nwoKCpSbm2thZZHPNE3NmTNHK1as0DvvvKPevXsHPD9ixAjFxcUFXPu9e/equLiYa99Cl156qXbt2qWioiL/MXLkSE2fPt3/b651cIwbN+6MJQ0+/fRT9ezZU5LUu3dvZWZmBlxrt9ut999/n2vdQsePH5fNFvgzFxMTI5/PJ4lrHUrNuba5ubmqqKjQ9u3b/W3eeecd+Xw+jR49um0FtGk4MkzTrJ8Kbrfbzeeee878+OOPzdtuu81MSUkxXS6X1aVFtNtvv910Op3munXrzNLSUv9x/Phxf5tZs2aZPXr0MN955x1z27ZtZm5urpmbm2th1dHj67OlTJNrHSxbtmwxY2NjzYceesj87LPPzBdffNFMSkoyX3jhBX+b+fPnmykpKeZrr71mfvjhh+aUKVOYntwKM2fONLt16+afCv7qq6+aXbt2NX/xi1/423CtW6+qqsrcuXOnuXPnTlOS+bvf/c7cuXOneeDAAdM0m3dtr7jiCnP48OHm+++/b27cuNHs27cvU8Hbkz/84Q9mjx49zPj4eHPUqFHm5s2brS4p4kk66/Hss8/625w4ccK84447zNTUVDMpKcm85pprzNLSUuuKjiL/Gm641sHzf//3f+bgwYNNu91u9u/f31y8eHHA8z6fz7z33nvNjIwM0263m5deeqm5d+9ei6qNXG6325w7d67Zo0cPMyEhwTz//PPNe+65x/R4PP42XOvWe/fdd8/63+iZM2eaptm8a/vVV1+Z06ZNMzt37mw6HA7zpptuMquqqtpcm2GaX1uqEQAAIMIx5gYAAEQVwg0AAIgqhBsAABBVCDcAACCqEG4AAEBUIdwAAICoQrgBAABRhXADAACiCuEGQIdnGIZWrlxpdRkAgoRwA8BSP/nJT2QYxhnHFVdcYXVpACJUrNUFAMAVV1yhZ599NuCc3W63qBoAkY6eGwCWs9vtyszMDDhSU1Ml1d8yWrhwoSZNmqTExESdf/75euWVVwJev2vXLn3/+99XYmKiunTpottuu03V1dUBbZYsWaJBgwbJbrcrKytLc+bMCXj+yJEjuuaaa5SUlKS+ffvq9ddfD+2XBhAyhBsA7d69996r6667Th988IGmT5+uH/3oR/rkk08kSTU1NZo4caJSU1O1detWLV++XG+//XZAeFm4cKFmz56t2267Tbt27dLrr7+uPn36BHzGb37zG/3bv/2bPvzwQ1155ZWaPn26jh49GtbvCSBI2ryvOAC0wcyZM82YmBizU6dOAcdDDz1kmqZpSjJnzZoV8JrRo0ebt99+u2maprl48WIzNTXVrK6u9j//97//3bTZbKbL5TJN0zSzs7PNe+65p8kaJJm/+tWv/I+rq6tNSeabb74ZtO8JIHwYcwPAct/73ve0cOHCgHNpaWn+f+fm5gY8l5ubq6KiIknSJ598oqFDh6pTp07+58eNGyefz6e9e/fKMAwdOnRIl1566TlrGDJkiP/fnTp1ksPhUHl5eWu/EgALEW4AWK5Tp05n3CYKlsTExGa1i4uLC3hsGIZ8Pl8oSgIQYoy5AdDubd68+YzHAwYMkCQNGDBAH3zwgWpqavzPv/fee7LZbOrXr5+Sk5PVq1cvFRQUhLVmANah5waA5Twej1wuV8C52NhYde3aVZK0fPlyjRw5UhdffLFefPFFbdmyRc8884wkafr06fr1r3+tmTNn6v7779fhw4d155136sc//rEyMjIkSffff79mzZql9PR0TZo0SVVVVXrvvfd05513hveLAggLwg0Ay7311lvKysoKONevXz/t2bNHUv1MpmXLlumOO+5QVlaW/vrXv2rgwIGSpKSkJK1evVpz587VRRddpKSkJF133XX63e9+53+vmTNn6uTJk3r00Uf1s5/9TF27dtUPf/jD8H1BAGFlmKZpWl0EADTFMAytWLFCU6dOtboUABGCMTcAACCqEG4AAEBUYcwNgHaNO+cAWoqeGwAAEFUINwAAIKoQbgAAQFQh3AAAgKhCuAEAAFGFcAMAAKIK4QYAAEQVwg0AAIgq/z9LIBrhyI+c2QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02_manual_gradient.py"
      ],
      "metadata": {
        "id": "6bateGiDivo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# training data\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = 1.0\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "# compute gradient\n",
        "def gradient(x, y):\n",
        "  return 2 * x * (x * w - y)\n",
        "\n",
        "# before training\n",
        "print('prediction (before training)', 4, forward(4))\n",
        "\n",
        "# training loop\n",
        "for epoch in range(10):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad = gradient(x_val, y_val)\n",
        "    w = w - 0.01 * grad\n",
        "    print('\\tgrad: ', x_val, y_val, round(grad, 2))\n",
        "    l = loss(x_val, y_val)\n",
        "  print('progress:', epoch, 'w=', round(w, 2), 'loss=', round(1, 2))\n",
        "\n",
        "#after training\n",
        "print('predicted score (after training)', 4, forward(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi2OAKCRlrco",
        "outputId": "1c28a4c5-5d29-4710-9191-c0ed233234bb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.84\n",
            "\tgrad:  3.0 6.0 -16.23\n",
            "progress: 0 w= 1.26 loss= 1\n",
            "\tgrad:  1.0 2.0 -1.48\n",
            "\tgrad:  2.0 4.0 -5.8\n",
            "\tgrad:  3.0 6.0 -12.0\n",
            "progress: 1 w= 1.45 loss= 1\n",
            "\tgrad:  1.0 2.0 -1.09\n",
            "\tgrad:  2.0 4.0 -4.29\n",
            "\tgrad:  3.0 6.0 -8.87\n",
            "progress: 2 w= 1.6 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.81\n",
            "\tgrad:  2.0 4.0 -3.17\n",
            "\tgrad:  3.0 6.0 -6.56\n",
            "progress: 3 w= 1.7 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.6\n",
            "\tgrad:  2.0 4.0 -2.34\n",
            "\tgrad:  3.0 6.0 -4.85\n",
            "progress: 4 w= 1.78 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.44\n",
            "\tgrad:  2.0 4.0 -1.73\n",
            "\tgrad:  3.0 6.0 -3.58\n",
            "progress: 5 w= 1.84 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.33\n",
            "\tgrad:  2.0 4.0 -1.28\n",
            "\tgrad:  3.0 6.0 -2.65\n",
            "progress: 6 w= 1.88 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.24\n",
            "\tgrad:  2.0 4.0 -0.95\n",
            "\tgrad:  3.0 6.0 -1.96\n",
            "progress: 7 w= 1.91 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.18\n",
            "\tgrad:  2.0 4.0 -0.7\n",
            "\tgrad:  3.0 6.0 -1.45\n",
            "progress: 8 w= 1.93 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.13\n",
            "\tgrad:  2.0 4.0 -0.52\n",
            "\tgrad:  3.0 6.0 -1.07\n",
            "progress: 9 w= 1.95 loss= 1\n",
            "predicted score (after training) 4 7.804863933862125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "MVEsUBeOqkZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "w = 1.0\n",
        "\n",
        "def forward(x, w):\n",
        "  return x * w\n",
        "\n",
        "def loss(w, x, y):\n",
        "  y_pred = forward(x, w)\n",
        "  return jnp.mean((y_pred - y) ** 2)\n",
        "\n",
        "# gradient 계산,loss 함수를 입력으로 받아들여 그래디언트 계산하는 함수로 반환\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "print('prediction (before_training)', forward(4, w))\n",
        "\n",
        "# training\n",
        "learning_rate= 0.01\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad_w = grad_loss(w, x_val, y_val)\n",
        "    w -= learning_rate * grad_w\n",
        "    print('\\tgrad:', x_val, y_val, round(grad_w, 2))\n",
        "    loss_val = loss(w, x_val, y_val)\n",
        "  print('progress:', epoch, 'w=', round(w, 2), 'loss=', round(loss_val, 2))\n",
        "\n",
        "# 학습 후 예측\n",
        "print('predicted score (after training):', forward(4, w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXDrwd8rnDUU",
        "outputId": "61c459b6-cf70-4062-d59e-bef95e2044db"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before_training) 4.0\n",
            "\tgrad: 1.0 2.0 -2.0\n",
            "\tgrad: 2.0 4.0 -7.8399997\n",
            "\tgrad: 3.0 6.0 -16.23\n",
            "progress: 0 w= 1.26 loss= 4.92\n",
            "\tgrad: 1.0 2.0 -1.48\n",
            "\tgrad: 2.0 4.0 -5.7999997\n",
            "\tgrad: 3.0 6.0 -12.0\n",
            "progress: 1 w= 1.4499999 loss= 2.69\n",
            "\tgrad: 1.0 2.0 -1.09\n",
            "\tgrad: 2.0 4.0 -4.29\n",
            "\tgrad: 3.0 6.0 -8.87\n",
            "progress: 2 w= 1.5999999 loss= 1.4699999\n",
            "\tgrad: 1.0 2.0 -0.81\n",
            "\tgrad: 2.0 4.0 -3.1699998\n",
            "\tgrad: 3.0 6.0 -6.56\n",
            "progress: 3 w= 1.6999999 loss= 0.79999995\n",
            "\tgrad: 1.0 2.0 -0.59999996\n",
            "\tgrad: 2.0 4.0 -2.34\n",
            "\tgrad: 3.0 6.0 -4.85\n",
            "progress: 4 w= 1.78 loss= 0.44\n",
            "\tgrad: 1.0 2.0 -0.44\n",
            "\tgrad: 2.0 4.0 -1.73\n",
            "\tgrad: 3.0 6.0 -3.58\n",
            "progress: 5 w= 1.8399999 loss= 0.24\n",
            "\tgrad: 1.0 2.0 -0.32999998\n",
            "\tgrad: 2.0 4.0 -1.28\n",
            "\tgrad: 3.0 6.0 -2.6499999\n",
            "progress: 6 w= 1.88 loss= 0.13\n",
            "\tgrad: 1.0 2.0 -0.24\n",
            "\tgrad: 2.0 4.0 -0.95\n",
            "\tgrad: 3.0 6.0 -1.9599999\n",
            "progress: 7 w= 1.91 loss= 0.07\n",
            "\tgrad: 1.0 2.0 -0.17999999\n",
            "\tgrad: 2.0 4.0 -0.7\n",
            "\tgrad: 3.0 6.0 -1.4499999\n",
            "progress: 8 w= 1.93 loss= 0.04\n",
            "\tgrad: 1.0 2.0 -0.13\n",
            "\tgrad: 2.0 4.0 -0.52\n",
            "\tgrad: 3.0 6.0 -1.0699999\n",
            "progress: 9 w= 1.9499999 loss= 0.02\n",
            "predicted score (after training): 7.8048644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grad_w, loss_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zjYLusNpLwt",
        "outputId": "3a1ba073-c1b1-4888-867e-37834e43dd7a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array(-1.0708666, dtype=float32, weak_type=True),\n",
              " Array(0.02141885, dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03_auto_gradient.py"
      ],
      "metadata": {
        "id": "UDkcoyF_qV4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import pdb\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "w = torch.tensor([1.0], requires_grad = True)\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(y_pred, y_val):\n",
        "  return (y_pred - y_val) **2\n",
        "\n",
        "# before training\n",
        "print('prediction (before training)', 4, forward(4).item())\n",
        "\n",
        "# training loop\n",
        "for epoch in range(20):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    y_pred = forward(x_val) # 1) forward pass\n",
        "    l = loss(y_pred, y_val) # 2) compute loss\n",
        "    l.backward() # 3) backpropagation to update weights\n",
        "    print('\\t grad:', x_val, y_val, w.grad.item())\n",
        "    w.data = w.data - 0.01 * w.grad.item()\n",
        "\n",
        "    w.grad.data.zero_()\n",
        "  print(f'Epoch: {epoch} | loss : {l.item()}')\n",
        "\n",
        "# after training\n",
        "print('prediction (after training)', 4, forward(4).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqTUzucjpVbX",
        "outputId": "f568f48c-ab45-43b4-be60-8370b30f72ce"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "\t grad: 1.0 2.0 -2.0\n",
            "\t grad: 2.0 4.0 -7.840000152587891\n",
            "\t grad: 3.0 6.0 -16.228801727294922\n",
            "Epoch: 0 | loss : 7.315943717956543\n",
            "\t grad: 1.0 2.0 -1.478623867034912\n",
            "\t grad: 2.0 4.0 -5.796205520629883\n",
            "\t grad: 3.0 6.0 -11.998146057128906\n",
            "Epoch: 1 | loss : 3.9987640380859375\n",
            "\t grad: 1.0 2.0 -1.0931644439697266\n",
            "\t grad: 2.0 4.0 -4.285204887390137\n",
            "\t grad: 3.0 6.0 -8.870372772216797\n",
            "Epoch: 2 | loss : 2.1856532096862793\n",
            "\t grad: 1.0 2.0 -0.8081896305084229\n",
            "\t grad: 2.0 4.0 -3.1681032180786133\n",
            "\t grad: 3.0 6.0 -6.557973861694336\n",
            "Epoch: 3 | loss : 1.1946394443511963\n",
            "\t grad: 1.0 2.0 -0.5975041389465332\n",
            "\t grad: 2.0 4.0 -2.3422164916992188\n",
            "\t grad: 3.0 6.0 -4.848389625549316\n",
            "Epoch: 4 | loss : 0.6529689431190491\n",
            "\t grad: 1.0 2.0 -0.4417421817779541\n",
            "\t grad: 2.0 4.0 -1.7316293716430664\n",
            "\t grad: 3.0 6.0 -3.58447265625\n",
            "Epoch: 5 | loss : 0.35690122842788696\n",
            "\t grad: 1.0 2.0 -0.3265852928161621\n",
            "\t grad: 2.0 4.0 -1.2802143096923828\n",
            "\t grad: 3.0 6.0 -2.650045394897461\n",
            "Epoch: 6 | loss : 0.195076122879982\n",
            "\t grad: 1.0 2.0 -0.24144840240478516\n",
            "\t grad: 2.0 4.0 -0.9464778900146484\n",
            "\t grad: 3.0 6.0 -1.9592113494873047\n",
            "Epoch: 7 | loss : 0.10662525147199631\n",
            "\t grad: 1.0 2.0 -0.17850565910339355\n",
            "\t grad: 2.0 4.0 -0.699742317199707\n",
            "\t grad: 3.0 6.0 -1.4484672546386719\n",
            "Epoch: 8 | loss : 0.0582793727517128\n",
            "\t grad: 1.0 2.0 -0.1319713592529297\n",
            "\t grad: 2.0 4.0 -0.5173273086547852\n",
            "\t grad: 3.0 6.0 -1.070866584777832\n",
            "Epoch: 9 | loss : 0.03185431286692619\n",
            "\t grad: 1.0 2.0 -0.09756779670715332\n",
            "\t grad: 2.0 4.0 -0.3824653625488281\n",
            "\t grad: 3.0 6.0 -0.7917022705078125\n",
            "Epoch: 10 | loss : 0.017410902306437492\n",
            "\t grad: 1.0 2.0 -0.07213282585144043\n",
            "\t grad: 2.0 4.0 -0.2827606201171875\n",
            "\t grad: 3.0 6.0 -0.5853137969970703\n",
            "Epoch: 11 | loss : 0.009516451507806778\n",
            "\t grad: 1.0 2.0 -0.053328514099121094\n",
            "\t grad: 2.0 4.0 -0.2090473175048828\n",
            "\t grad: 3.0 6.0 -0.43272972106933594\n",
            "Epoch: 12 | loss : 0.005201528314501047\n",
            "\t grad: 1.0 2.0 -0.039426326751708984\n",
            "\t grad: 2.0 4.0 -0.15455150604248047\n",
            "\t grad: 3.0 6.0 -0.3199195861816406\n",
            "Epoch: 13 | loss : 0.0028430151287466288\n",
            "\t grad: 1.0 2.0 -0.029148340225219727\n",
            "\t grad: 2.0 4.0 -0.11426162719726562\n",
            "\t grad: 3.0 6.0 -0.23652076721191406\n",
            "Epoch: 14 | loss : 0.0015539465239271522\n",
            "\t grad: 1.0 2.0 -0.021549701690673828\n",
            "\t grad: 2.0 4.0 -0.08447456359863281\n",
            "\t grad: 3.0 6.0 -0.17486286163330078\n",
            "Epoch: 15 | loss : 0.0008493617060594261\n",
            "\t grad: 1.0 2.0 -0.01593184471130371\n",
            "\t grad: 2.0 4.0 -0.062453269958496094\n",
            "\t grad: 3.0 6.0 -0.12927818298339844\n",
            "Epoch: 16 | loss : 0.00046424579340964556\n",
            "\t grad: 1.0 2.0 -0.011778593063354492\n",
            "\t grad: 2.0 4.0 -0.046172142028808594\n",
            "\t grad: 3.0 6.0 -0.09557533264160156\n",
            "Epoch: 17 | loss : 0.0002537401160225272\n",
            "\t grad: 1.0 2.0 -0.00870823860168457\n",
            "\t grad: 2.0 4.0 -0.03413581848144531\n",
            "\t grad: 3.0 6.0 -0.07066154479980469\n",
            "Epoch: 18 | loss : 0.00013869594840798527\n",
            "\t grad: 1.0 2.0 -0.006437778472900391\n",
            "\t grad: 2.0 4.0 -0.025236129760742188\n",
            "\t grad: 3.0 6.0 -0.052239418029785156\n",
            "Epoch: 19 | loss : 7.580435340059921e-05\n",
            "prediction (after training) 4 7.990480899810791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax\n",
        "- torch 구현보다 예측력이 떨어진다!?"
      ],
      "metadata": {
        "id": "kSwVz9EdqnPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "w = jnp.array([1.0])\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "def loss(y_pred, y_val):\n",
        "  return jnp.mean((y_pred - y_val)**2)\n",
        "\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "print('prediction (before training)', 4, forward(4)[0])\n",
        "\n",
        "lr = 0.01\n",
        "for epoch in range(20):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    # y_pred = forward(x_val) # 1) forward pass\n",
        "    #l = loss(y_pred, y_val) # 2) compute loss\n",
        "    #l.backward() # 3) backpropagation to update weights\n",
        "    #print('\\t grad:', x_val, y_val, w.grad.item())\n",
        "    y_pred = forward(x_val)\n",
        "    grad_w = grad_loss(y_pred, y_val)\n",
        "    w -= learning_rate * grad_w[0]\n",
        "    print('\\tgrad: ', x_val, y_val, grad_w)\n",
        "\n",
        "  print(f'Epoch: {epoch} | Loss: {loss(forward(x_data), y_data)}')\n",
        "\n",
        "print('prediction (after training)', 4, forward(4)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTRvGqLtqm7X",
        "outputId": "66fb3e03-a670-4648-f3a5-c11248b1608e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 [-2.]\n",
            "\tgrad:  2.0 4.0 [-3.92]\n",
            "\tgrad:  3.0 6.0 [-5.6448]\n",
            "Epoch: 0 | Loss: 3.649700164794922\n",
            "\tgrad:  1.0 2.0 [-1.7687042]\n",
            "\tgrad:  2.0 4.0 [-3.46666]\n",
            "\tgrad:  3.0 6.0 [-4.99199]\n",
            "Epoch: 1 | Loss: 2.854351758956909\n",
            "\tgrad:  1.0 2.0 [-1.564157]\n",
            "\tgrad:  2.0 4.0 [-3.0657477]\n",
            "\tgrad:  3.0 6.0 [-4.4146767]\n",
            "Epoch: 2 | Loss: 2.2323265075683594\n",
            "\tgrad:  1.0 2.0 [-1.3832653]\n",
            "\tgrad:  2.0 4.0 [-2.7111998]\n",
            "\tgrad:  3.0 6.0 [-3.9041271]\n",
            "Epoch: 3 | Loss: 1.7458542585372925\n",
            "\tgrad:  1.0 2.0 [-1.2232933]\n",
            "\tgrad:  2.0 4.0 [-2.397655]\n",
            "\tgrad:  3.0 6.0 [-3.4526234]\n",
            "Epoch: 4 | Loss: 1.365395426750183\n",
            "\tgrad:  1.0 2.0 [-1.0818219]\n",
            "\tgrad:  2.0 4.0 [-2.1203709]\n",
            "\tgrad:  3.0 6.0 [-3.0533333]\n",
            "Epoch: 5 | Loss: 1.0678461790084839\n",
            "\tgrad:  1.0 2.0 [-0.9567113]\n",
            "\tgrad:  2.0 4.0 [-1.875154]\n",
            "\tgrad:  3.0 6.0 [-2.700222]\n",
            "Epoch: 6 | Loss: 0.8351394534111023\n",
            "\tgrad:  1.0 2.0 [-0.8460696]\n",
            "\tgrad:  2.0 4.0 [-1.6582966]\n",
            "\tgrad:  3.0 6.0 [-2.387947]\n",
            "Epoch: 7 | Loss: 0.6531445384025574\n",
            "\tgrad:  1.0 2.0 [-0.7482233]\n",
            "\tgrad:  2.0 4.0 [-1.4665174]\n",
            "\tgrad:  3.0 6.0 [-2.111786]\n",
            "Epoch: 8 | Loss: 0.5108104944229126\n",
            "\tgrad:  1.0 2.0 [-0.66169286]\n",
            "\tgrad:  2.0 4.0 [-1.2969179]\n",
            "\tgrad:  3.0 6.0 [-1.8675623]\n",
            "Epoch: 9 | Loss: 0.39949390292167664\n",
            "\tgrad:  1.0 2.0 [-0.58516955]\n",
            "\tgrad:  2.0 4.0 [-1.1469321]\n",
            "\tgrad:  3.0 6.0 [-1.6515818]\n",
            "Epoch: 10 | Loss: 0.31243523955345154\n",
            "\tgrad:  1.0 2.0 [-0.51749563]\n",
            "\tgrad:  2.0 4.0 [-1.0142913]\n",
            "\tgrad:  3.0 6.0 [-1.4605789]\n",
            "Epoch: 11 | Loss: 0.24434894323349\n",
            "\tgrad:  1.0 2.0 [-0.45764828]\n",
            "\tgrad:  2.0 4.0 [-0.8969908]\n",
            "\tgrad:  3.0 6.0 [-1.291667]\n",
            "Epoch: 12 | Loss: 0.1911000907421112\n",
            "\tgrad:  1.0 2.0 [-0.4047222]\n",
            "\tgrad:  2.0 4.0 [-0.7932553]\n",
            "\tgrad:  3.0 6.0 [-1.1422882]\n",
            "Epoch: 13 | Loss: 0.1494552195072174\n",
            "\tgrad:  1.0 2.0 [-0.35791683]\n",
            "\tgrad:  2.0 4.0 [-0.7015171]\n",
            "\tgrad:  3.0 6.0 [-1.0101843]\n",
            "Epoch: 14 | Loss: 0.1168857216835022\n",
            "\tgrad:  1.0 2.0 [-0.3165245]\n",
            "\tgrad:  2.0 4.0 [-0.62038803]\n",
            "\tgrad:  3.0 6.0 [-0.89335823]\n",
            "Epoch: 15 | Loss: 0.09141391515731812\n",
            "\tgrad:  1.0 2.0 [-0.27991915]\n",
            "\tgrad:  2.0 4.0 [-0.5486417]\n",
            "\tgrad:  3.0 6.0 [-0.7900448]\n",
            "Epoch: 16 | Loss: 0.07149285823106766\n",
            "\tgrad:  1.0 2.0 [-0.24754715]\n",
            "\tgrad:  2.0 4.0 [-0.4851923]\n",
            "\tgrad:  3.0 6.0 [-0.69867706]\n",
            "Epoch: 17 | Loss: 0.05591301620006561\n",
            "\tgrad:  1.0 2.0 [-0.2189188]\n",
            "\tgrad:  2.0 4.0 [-0.42908096]\n",
            "\tgrad:  3.0 6.0 [-0.61787605]\n",
            "Epoch: 18 | Loss: 0.043728381395339966\n",
            "\tgrad:  1.0 2.0 [-0.19360137]\n",
            "\tgrad:  2.0 4.0 [-0.3794589]\n",
            "\tgrad:  3.0 6.0 [-0.54642105]\n",
            "Epoch: 19 | Loss: 0.034199103713035583\n",
            "prediction (after training) 4 7.657576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05_linear_regression.py"
      ],
      "metadata": {
        "id": "hyGPMymMsn74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])"
      ],
      "metadata": {
        "id": "5w1GKtijsTJL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear module\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1) # one in and one out\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and\n",
        "    we must return a Variable of output data.\n",
        "    We can use Modules defined in the constructor as well as arbitary operators\n",
        "    on Variables.\n",
        "    \"\"\"\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "YEzueZR5s7Jz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "\n",
        "# loss function & optimizer\n",
        "criterion = torch.nn.MSELoss(reduction = 'sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "yVbyU1NBtmtT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# traning loop\n",
        "for epoch in range(500):\n",
        "  # forward pass : compute prediction y by passing x to the model\n",
        "  y_pred = model(x_data)\n",
        "\n",
        "  # compute and print loss\n",
        "  loss = criterion(y_pred, y_data)\n",
        "  print(f'Epoch: {epoch} | Loss: {loss.item()}')\n",
        "\n",
        "  # zero gradients, backward pass, update the weights\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# after training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print('prediction (after training)', 4, model(hour_var).data[0][0].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4BH4Ro7tobL",
        "outputId": "6c7413e5-43e5-44cc-b66b-8022b2aebd5f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 0.00018834293587133288\n",
            "Epoch: 1 | Loss: 0.0001856395392678678\n",
            "Epoch: 2 | Loss: 0.00018296904454473406\n",
            "Epoch: 3 | Loss: 0.00018033820379059762\n",
            "Epoch: 4 | Loss: 0.00017775199376046658\n",
            "Epoch: 5 | Loss: 0.00017519446555525064\n",
            "Epoch: 6 | Loss: 0.00017267459770664573\n",
            "Epoch: 7 | Loss: 0.00017019266670104116\n",
            "Epoch: 8 | Loss: 0.00016774755204096437\n",
            "Epoch: 9 | Loss: 0.00016534252790734172\n",
            "Epoch: 10 | Loss: 0.00016295863315463066\n",
            "Epoch: 11 | Loss: 0.00016062222130130976\n",
            "Epoch: 12 | Loss: 0.00015831220662221313\n",
            "Epoch: 13 | Loss: 0.00015604069631081074\n",
            "Epoch: 14 | Loss: 0.00015378993703052402\n",
            "Epoch: 15 | Loss: 0.0001515841722721234\n",
            "Epoch: 16 | Loss: 0.00014940783148631454\n",
            "Epoch: 17 | Loss: 0.00014725570508744568\n",
            "Epoch: 18 | Loss: 0.00014514299982693046\n",
            "Epoch: 19 | Loss: 0.00014305739023257047\n",
            "Epoch: 20 | Loss: 0.0001410054974257946\n",
            "Epoch: 21 | Loss: 0.00013897119788452983\n",
            "Epoch: 22 | Loss: 0.00013698023394681513\n",
            "Epoch: 23 | Loss: 0.00013500973000191152\n",
            "Epoch: 24 | Loss: 0.00013306690379977226\n",
            "Epoch: 25 | Loss: 0.00013115881301928312\n",
            "Epoch: 26 | Loss: 0.0001292711094720289\n",
            "Epoch: 27 | Loss: 0.00012740955571644008\n",
            "Epoch: 28 | Loss: 0.00012558235903270543\n",
            "Epoch: 29 | Loss: 0.00012377742677927017\n",
            "Epoch: 30 | Loss: 0.00012200030323583633\n",
            "Epoch: 31 | Loss: 0.00012024943134747446\n",
            "Epoch: 32 | Loss: 0.00011851691670017317\n",
            "Epoch: 33 | Loss: 0.00011681080650305375\n",
            "Epoch: 34 | Loss: 0.00011513638310134411\n",
            "Epoch: 35 | Loss: 0.00011348210682626814\n",
            "Epoch: 36 | Loss: 0.00011185518087586388\n",
            "Epoch: 37 | Loss: 0.0001102418391383253\n",
            "Epoch: 38 | Loss: 0.00010865536023629829\n",
            "Epoch: 39 | Loss: 0.00010709842899814248\n",
            "Epoch: 40 | Loss: 0.00010555810877121985\n",
            "Epoch: 41 | Loss: 0.00010403608030173928\n",
            "Epoch: 42 | Loss: 0.00010254392691422254\n",
            "Epoch: 43 | Loss: 0.00010106666013598442\n",
            "Epoch: 44 | Loss: 9.961692558135837e-05\n",
            "Epoch: 45 | Loss: 9.818803664529696e-05\n",
            "Epoch: 46 | Loss: 9.677118941908702e-05\n",
            "Epoch: 47 | Loss: 9.538338053971529e-05\n",
            "Epoch: 48 | Loss: 9.401348506798968e-05\n",
            "Epoch: 49 | Loss: 9.266188862966374e-05\n",
            "Epoch: 50 | Loss: 9.133230923907831e-05\n",
            "Epoch: 51 | Loss: 9.002115984912962e-05\n",
            "Epoch: 52 | Loss: 8.872606849763542e-05\n",
            "Epoch: 53 | Loss: 8.74501492944546e-05\n",
            "Epoch: 54 | Loss: 8.618779247626662e-05\n",
            "Epoch: 55 | Loss: 8.494961366523057e-05\n",
            "Epoch: 56 | Loss: 8.373208402190357e-05\n",
            "Epoch: 57 | Loss: 8.253233681898564e-05\n",
            "Epoch: 58 | Loss: 8.134072413668036e-05\n",
            "Epoch: 59 | Loss: 8.01692294771783e-05\n",
            "Epoch: 60 | Loss: 7.902173092588782e-05\n",
            "Epoch: 61 | Loss: 7.788969378452748e-05\n",
            "Epoch: 62 | Loss: 7.676683890167624e-05\n",
            "Epoch: 63 | Loss: 7.566225394839421e-05\n",
            "Epoch: 64 | Loss: 7.457772153429687e-05\n",
            "Epoch: 65 | Loss: 7.350501255132258e-05\n",
            "Epoch: 66 | Loss: 7.244403241202235e-05\n",
            "Epoch: 67 | Loss: 7.140844536479563e-05\n",
            "Epoch: 68 | Loss: 7.037934847176075e-05\n",
            "Epoch: 69 | Loss: 6.937030411791056e-05\n",
            "Epoch: 70 | Loss: 6.837143882876262e-05\n",
            "Epoch: 71 | Loss: 6.739031232427806e-05\n",
            "Epoch: 72 | Loss: 6.642101652687415e-05\n",
            "Epoch: 73 | Loss: 6.546814984176308e-05\n",
            "Epoch: 74 | Loss: 6.452496745623648e-05\n",
            "Epoch: 75 | Loss: 6.35965188848786e-05\n",
            "Epoch: 76 | Loss: 6.268537254072726e-05\n",
            "Epoch: 77 | Loss: 6.178401235956699e-05\n",
            "Epoch: 78 | Loss: 6.089552698540501e-05\n",
            "Epoch: 79 | Loss: 6.002113877912052e-05\n",
            "Epoch: 80 | Loss: 5.915843939874321e-05\n",
            "Epoch: 81 | Loss: 5.830996815348044e-05\n",
            "Epoch: 82 | Loss: 5.7470282627036795e-05\n",
            "Epoch: 83 | Loss: 5.664324635290541e-05\n",
            "Epoch: 84 | Loss: 5.582828453043476e-05\n",
            "Epoch: 85 | Loss: 5.5025695473887026e-05\n",
            "Epoch: 86 | Loss: 5.4236628784565255e-05\n",
            "Epoch: 87 | Loss: 5.3454539738595486e-05\n",
            "Epoch: 88 | Loss: 5.268867971608415e-05\n",
            "Epoch: 89 | Loss: 5.193168908590451e-05\n",
            "Epoch: 90 | Loss: 5.118600893183611e-05\n",
            "Epoch: 91 | Loss: 5.045190846431069e-05\n",
            "Epoch: 92 | Loss: 4.972598617314361e-05\n",
            "Epoch: 93 | Loss: 4.9008987843990326e-05\n",
            "Epoch: 94 | Loss: 4.830527541344054e-05\n",
            "Epoch: 95 | Loss: 4.7610268666176125e-05\n",
            "Epoch: 96 | Loss: 4.692627044278197e-05\n",
            "Epoch: 97 | Loss: 4.625314977602102e-05\n",
            "Epoch: 98 | Loss: 4.5587636122945696e-05\n",
            "Epoch: 99 | Loss: 4.4933578465133905e-05\n",
            "Epoch: 100 | Loss: 4.4288888602750376e-05\n",
            "Epoch: 101 | Loss: 4.3649630242725834e-05\n",
            "Epoch: 102 | Loss: 4.3025305785704404e-05\n",
            "Epoch: 103 | Loss: 4.240358975948766e-05\n",
            "Epoch: 104 | Loss: 4.179467214271426e-05\n",
            "Epoch: 105 | Loss: 4.1195748053723946e-05\n",
            "Epoch: 106 | Loss: 4.060374340042472e-05\n",
            "Epoch: 107 | Loss: 4.0022277971729636e-05\n",
            "Epoch: 108 | Loss: 3.9446465962100774e-05\n",
            "Epoch: 109 | Loss: 3.8878817576915026e-05\n",
            "Epoch: 110 | Loss: 3.8317441067192703e-05\n",
            "Epoch: 111 | Loss: 3.7767291360069066e-05\n",
            "Epoch: 112 | Loss: 3.722714973264374e-05\n",
            "Epoch: 113 | Loss: 3.668914359877817e-05\n",
            "Epoch: 114 | Loss: 3.616448520915583e-05\n",
            "Epoch: 115 | Loss: 3.564360667951405e-05\n",
            "Epoch: 116 | Loss: 3.513133560772985e-05\n",
            "Epoch: 117 | Loss: 3.4626194974407554e-05\n",
            "Epoch: 118 | Loss: 3.412980368011631e-05\n",
            "Epoch: 119 | Loss: 3.36390221491456e-05\n",
            "Epoch: 120 | Loss: 3.315715002827346e-05\n",
            "Epoch: 121 | Loss: 3.267842475906946e-05\n",
            "Epoch: 122 | Loss: 3.2208125048782676e-05\n",
            "Epoch: 123 | Loss: 3.174582525389269e-05\n",
            "Epoch: 124 | Loss: 3.1289466278394684e-05\n",
            "Epoch: 125 | Loss: 3.083705814788118e-05\n",
            "Epoch: 126 | Loss: 3.039659895875957e-05\n",
            "Epoch: 127 | Loss: 2.9959948733448982e-05\n",
            "Epoch: 128 | Loss: 2.952897921204567e-05\n",
            "Epoch: 129 | Loss: 2.9105529392836615e-05\n",
            "Epoch: 130 | Loss: 2.868482260964811e-05\n",
            "Epoch: 131 | Loss: 2.8274291253183037e-05\n",
            "Epoch: 132 | Loss: 2.7865185984410346e-05\n",
            "Epoch: 133 | Loss: 2.746728205238469e-05\n",
            "Epoch: 134 | Loss: 2.7072241209680215e-05\n",
            "Epoch: 135 | Loss: 2.6685171178542078e-05\n",
            "Epoch: 136 | Loss: 2.6300591343897395e-05\n",
            "Epoch: 137 | Loss: 2.5920280677382834e-05\n",
            "Epoch: 138 | Loss: 2.555037644924596e-05\n",
            "Epoch: 139 | Loss: 2.5182554963976145e-05\n",
            "Epoch: 140 | Loss: 2.4821743863867596e-05\n",
            "Epoch: 141 | Loss: 2.4463533918606117e-05\n",
            "Epoch: 142 | Loss: 2.4113067411235534e-05\n",
            "Epoch: 143 | Loss: 2.3763997887726873e-05\n",
            "Epoch: 144 | Loss: 2.342450534342788e-05\n",
            "Epoch: 145 | Loss: 2.3086897272150964e-05\n",
            "Epoch: 146 | Loss: 2.2756454200134613e-05\n",
            "Epoch: 147 | Loss: 2.2428121155826375e-05\n",
            "Epoch: 148 | Loss: 2.2106272808741778e-05\n",
            "Epoch: 149 | Loss: 2.1788648155052215e-05\n",
            "Epoch: 150 | Loss: 2.147548002540134e-05\n",
            "Epoch: 151 | Loss: 2.1166451915632933e-05\n",
            "Epoch: 152 | Loss: 2.0863379177171737e-05\n",
            "Epoch: 153 | Loss: 2.056328594335355e-05\n",
            "Epoch: 154 | Loss: 2.026667061727494e-05\n",
            "Epoch: 155 | Loss: 1.9974555470980704e-05\n",
            "Epoch: 156 | Loss: 1.9687649910338223e-05\n",
            "Epoch: 157 | Loss: 1.9405641069170088e-05\n",
            "Epoch: 158 | Loss: 1.912541119963862e-05\n",
            "Epoch: 159 | Loss: 1.885024903458543e-05\n",
            "Epoch: 160 | Loss: 1.8579587049316615e-05\n",
            "Epoch: 161 | Loss: 1.8314613043912686e-05\n",
            "Epoch: 162 | Loss: 1.8050308426609263e-05\n",
            "Epoch: 163 | Loss: 1.779062586138025e-05\n",
            "Epoch: 164 | Loss: 1.7535010556457564e-05\n",
            "Epoch: 165 | Loss: 1.728269489831291e-05\n",
            "Epoch: 166 | Loss: 1.7034613847499713e-05\n",
            "Epoch: 167 | Loss: 1.6789512301329523e-05\n",
            "Epoch: 168 | Loss: 1.654831794439815e-05\n",
            "Epoch: 169 | Loss: 1.6309102647937834e-05\n",
            "Epoch: 170 | Loss: 1.6076755855465308e-05\n",
            "Epoch: 171 | Loss: 1.584584970260039e-05\n",
            "Epoch: 172 | Loss: 1.5617757526342757e-05\n",
            "Epoch: 173 | Loss: 1.5393601643154398e-05\n",
            "Epoch: 174 | Loss: 1.5171065570029896e-05\n",
            "Epoch: 175 | Loss: 1.4955771803215612e-05\n",
            "Epoch: 176 | Loss: 1.473956126574194e-05\n",
            "Epoch: 177 | Loss: 1.4528248357237317e-05\n",
            "Epoch: 178 | Loss: 1.431890359526733e-05\n",
            "Epoch: 179 | Loss: 1.4113914403424133e-05\n",
            "Epoch: 180 | Loss: 1.3909105291531887e-05\n",
            "Epoch: 181 | Loss: 1.370923564536497e-05\n",
            "Epoch: 182 | Loss: 1.3513162230083253e-05\n",
            "Epoch: 183 | Loss: 1.3319139725354034e-05\n",
            "Epoch: 184 | Loss: 1.3128203136147931e-05\n",
            "Epoch: 185 | Loss: 1.2939902262587566e-05\n",
            "Epoch: 186 | Loss: 1.2752961993101053e-05\n",
            "Epoch: 187 | Loss: 1.2568824786285404e-05\n",
            "Epoch: 188 | Loss: 1.2389710718707647e-05\n",
            "Epoch: 189 | Loss: 1.2211270586703904e-05\n",
            "Epoch: 190 | Loss: 1.2037150554533582e-05\n",
            "Epoch: 191 | Loss: 1.186328154290095e-05\n",
            "Epoch: 192 | Loss: 1.1691472536767833e-05\n",
            "Epoch: 193 | Loss: 1.1522891327331308e-05\n",
            "Epoch: 194 | Loss: 1.1358077244949527e-05\n",
            "Epoch: 195 | Loss: 1.1194842045370024e-05\n",
            "Epoch: 196 | Loss: 1.1034141607524361e-05\n",
            "Epoch: 197 | Loss: 1.087421605916461e-05\n",
            "Epoch: 198 | Loss: 1.0718696103140246e-05\n",
            "Epoch: 199 | Loss: 1.05641083791852e-05\n",
            "Epoch: 200 | Loss: 1.0412143637950066e-05\n",
            "Epoch: 201 | Loss: 1.0264072443533223e-05\n",
            "Epoch: 202 | Loss: 1.0117061719938647e-05\n",
            "Epoch: 203 | Loss: 9.97148163150996e-06\n",
            "Epoch: 204 | Loss: 9.8278651421424e-06\n",
            "Epoch: 205 | Loss: 9.686557859822642e-06\n",
            "Epoch: 206 | Loss: 9.54753159021493e-06\n",
            "Epoch: 207 | Loss: 9.410046004632022e-06\n",
            "Epoch: 208 | Loss: 9.27444125409238e-06\n",
            "Epoch: 209 | Loss: 9.141405826085247e-06\n",
            "Epoch: 210 | Loss: 9.010201210912783e-06\n",
            "Epoch: 211 | Loss: 8.880465429683682e-06\n",
            "Epoch: 212 | Loss: 8.752705070946831e-06\n",
            "Epoch: 213 | Loss: 8.627403985883575e-06\n",
            "Epoch: 214 | Loss: 8.502838682034053e-06\n",
            "Epoch: 215 | Loss: 8.381197403650731e-06\n",
            "Epoch: 216 | Loss: 8.258765774371568e-06\n",
            "Epoch: 217 | Loss: 8.14270242699422e-06\n",
            "Epoch: 218 | Loss: 8.023184818739537e-06\n",
            "Epoch: 219 | Loss: 7.909124178695492e-06\n",
            "Epoch: 220 | Loss: 7.79588026489364e-06\n",
            "Epoch: 221 | Loss: 7.683293006266467e-06\n",
            "Epoch: 222 | Loss: 7.572803951916285e-06\n",
            "Epoch: 223 | Loss: 7.463435395038687e-06\n",
            "Epoch: 224 | Loss: 7.356752576015424e-06\n",
            "Epoch: 225 | Loss: 7.251308488775976e-06\n",
            "Epoch: 226 | Loss: 7.146158168325201e-06\n",
            "Epoch: 227 | Loss: 7.045323400234338e-06\n",
            "Epoch: 228 | Loss: 6.943063453945797e-06\n",
            "Epoch: 229 | Loss: 6.842767106718384e-06\n",
            "Epoch: 230 | Loss: 6.7456148826749995e-06\n",
            "Epoch: 231 | Loss: 6.647809641435742e-06\n",
            "Epoch: 232 | Loss: 6.5517615439603105e-06\n",
            "Epoch: 233 | Loss: 6.457295057771262e-06\n",
            "Epoch: 234 | Loss: 6.366009984049015e-06\n",
            "Epoch: 235 | Loss: 6.274647603277117e-06\n",
            "Epoch: 236 | Loss: 6.183511686685961e-06\n",
            "Epoch: 237 | Loss: 6.094047421356663e-06\n",
            "Epoch: 238 | Loss: 6.007512638461776e-06\n",
            "Epoch: 239 | Loss: 5.920325747865718e-06\n",
            "Epoch: 240 | Loss: 5.836161108163651e-06\n",
            "Epoch: 241 | Loss: 5.751207027060445e-06\n",
            "Epoch: 242 | Loss: 5.66950529901078e-06\n",
            "Epoch: 243 | Loss: 5.587839041254483e-06\n",
            "Epoch: 244 | Loss: 5.508265530806966e-06\n",
            "Epoch: 245 | Loss: 5.428855729405768e-06\n",
            "Epoch: 246 | Loss: 5.350293577066623e-06\n",
            "Epoch: 247 | Loss: 5.2743052947334945e-06\n",
            "Epoch: 248 | Loss: 5.197534846956842e-06\n",
            "Epoch: 249 | Loss: 5.122643415234052e-06\n",
            "Epoch: 250 | Loss: 5.049862011219375e-06\n",
            "Epoch: 251 | Loss: 4.976955096935853e-06\n",
            "Epoch: 252 | Loss: 4.905607056571171e-06\n",
            "Epoch: 253 | Loss: 4.834647370444145e-06\n",
            "Epoch: 254 | Loss: 4.765218818647554e-06\n",
            "Epoch: 255 | Loss: 4.696167707152199e-06\n",
            "Epoch: 256 | Loss: 4.628620445146225e-06\n",
            "Epoch: 257 | Loss: 4.562555204756791e-06\n",
            "Epoch: 258 | Loss: 4.496842848311644e-06\n",
            "Epoch: 259 | Loss: 4.433441972651053e-06\n",
            "Epoch: 260 | Loss: 4.368669578980189e-06\n",
            "Epoch: 261 | Loss: 4.305580660002306e-06\n",
            "Epoch: 262 | Loss: 4.243547664373182e-06\n",
            "Epoch: 263 | Loss: 4.183748387731612e-06\n",
            "Epoch: 264 | Loss: 4.122485734114889e-06\n",
            "Epoch: 265 | Loss: 4.063549113197951e-06\n",
            "Epoch: 266 | Loss: 4.006084964203183e-06\n",
            "Epoch: 267 | Loss: 3.94787457480561e-06\n",
            "Epoch: 268 | Loss: 3.891237611242104e-06\n",
            "Epoch: 269 | Loss: 3.835577899735654e-06\n",
            "Epoch: 270 | Loss: 3.7797551613039104e-06\n",
            "Epoch: 271 | Loss: 3.7260244880599203e-06\n",
            "Epoch: 272 | Loss: 3.671452986964141e-06\n",
            "Epoch: 273 | Loss: 3.6192748211760772e-06\n",
            "Epoch: 274 | Loss: 3.5674702303367667e-06\n",
            "Epoch: 275 | Loss: 3.516582864904194e-06\n",
            "Epoch: 276 | Loss: 3.4655211038625566e-06\n",
            "Epoch: 277 | Loss: 3.4155850698880386e-06\n",
            "Epoch: 278 | Loss: 3.366758392076008e-06\n",
            "Epoch: 279 | Loss: 3.3183882806042675e-06\n",
            "Epoch: 280 | Loss: 3.2711045605537947e-06\n",
            "Epoch: 281 | Loss: 3.22322216561588e-06\n",
            "Epoch: 282 | Loss: 3.1774518447491573e-06\n",
            "Epoch: 283 | Loss: 3.1319070785684744e-06\n",
            "Epoch: 284 | Loss: 3.0874064123054268e-06\n",
            "Epoch: 285 | Loss: 3.0417033940466354e-06\n",
            "Epoch: 286 | Loss: 2.9981531497469405e-06\n",
            "Epoch: 287 | Loss: 2.954616547867772e-06\n",
            "Epoch: 288 | Loss: 2.9126863410056103e-06\n",
            "Epoch: 289 | Loss: 2.8706649572995957e-06\n",
            "Epoch: 290 | Loss: 2.829046252372791e-06\n",
            "Epoch: 291 | Loss: 2.789671043501585e-06\n",
            "Epoch: 292 | Loss: 2.749030954873888e-06\n",
            "Epoch: 293 | Loss: 2.7097414658783237e-06\n",
            "Epoch: 294 | Loss: 2.670449248398654e-06\n",
            "Epoch: 295 | Loss: 2.6321044970245566e-06\n",
            "Epoch: 296 | Loss: 2.594318402771023e-06\n",
            "Epoch: 297 | Loss: 2.5565261694282526e-06\n",
            "Epoch: 298 | Loss: 2.520672524042311e-06\n",
            "Epoch: 299 | Loss: 2.4843395749485353e-06\n",
            "Epoch: 300 | Loss: 2.4489074803568656e-06\n",
            "Epoch: 301 | Loss: 2.4135476905939868e-06\n",
            "Epoch: 302 | Loss: 2.3786258225300116e-06\n",
            "Epoch: 303 | Loss: 2.344402446396998e-06\n",
            "Epoch: 304 | Loss: 2.310692707396811e-06\n",
            "Epoch: 305 | Loss: 2.2778415313950973e-06\n",
            "Epoch: 306 | Loss: 2.244963752673357e-06\n",
            "Epoch: 307 | Loss: 2.211893388448516e-06\n",
            "Epoch: 308 | Loss: 2.180355750169838e-06\n",
            "Epoch: 309 | Loss: 2.1490445760719012e-06\n",
            "Epoch: 310 | Loss: 2.1186363028391497e-06\n",
            "Epoch: 311 | Loss: 2.088444944092771e-06\n",
            "Epoch: 312 | Loss: 2.0577201667038025e-06\n",
            "Epoch: 313 | Loss: 2.028547214649734e-06\n",
            "Epoch: 314 | Loss: 1.9995825368823716e-06\n",
            "Epoch: 315 | Loss: 1.9703379621205386e-06\n",
            "Epoch: 316 | Loss: 1.9423605408519506e-06\n",
            "Epoch: 317 | Loss: 1.9145832084177528e-06\n",
            "Epoch: 318 | Loss: 1.887005964817945e-06\n",
            "Epoch: 319 | Loss: 1.8596288100525271e-06\n",
            "Epoch: 320 | Loss: 1.8330028979107738e-06\n",
            "Epoch: 321 | Loss: 1.8065688891510945e-06\n",
            "Epoch: 322 | Loss: 1.781103264875128e-06\n",
            "Epoch: 323 | Loss: 1.7550476059113862e-06\n",
            "Epoch: 324 | Loss: 1.7293382370553445e-06\n",
            "Epoch: 325 | Loss: 1.705410113572725e-06\n",
            "Epoch: 326 | Loss: 1.6804438018880319e-06\n",
            "Epoch: 327 | Loss: 1.6564104043936823e-06\n",
            "Epoch: 328 | Loss: 1.6324004263879033e-06\n",
            "Epoch: 329 | Loss: 1.6092308214865625e-06\n",
            "Epoch: 330 | Loss: 1.5860791791055817e-06\n",
            "Epoch: 331 | Loss: 1.563095338497078e-06\n",
            "Epoch: 332 | Loss: 1.5404245914396597e-06\n",
            "Epoch: 333 | Loss: 1.5188495581242023e-06\n",
            "Epoch: 334 | Loss: 1.4970717074902495e-06\n",
            "Epoch: 335 | Loss: 1.474886630603578e-06\n",
            "Epoch: 336 | Loss: 1.4537076822307426e-06\n",
            "Epoch: 337 | Loss: 1.4328909401228884e-06\n",
            "Epoch: 338 | Loss: 1.4125691905064741e-06\n",
            "Epoch: 339 | Loss: 1.392050080539775e-06\n",
            "Epoch: 340 | Loss: 1.371681264572544e-06\n",
            "Epoch: 341 | Loss: 1.3519359072233783e-06\n",
            "Epoch: 342 | Loss: 1.3326684893399943e-06\n",
            "Epoch: 343 | Loss: 1.3142062016413547e-06\n",
            "Epoch: 344 | Loss: 1.2948803487233818e-06\n",
            "Epoch: 345 | Loss: 1.2761577181663597e-06\n",
            "Epoch: 346 | Loss: 1.258092424905044e-06\n",
            "Epoch: 347 | Loss: 1.2397026694088709e-06\n",
            "Epoch: 348 | Loss: 1.2223483736306662e-06\n",
            "Epoch: 349 | Loss: 1.2041596164635848e-06\n",
            "Epoch: 350 | Loss: 1.1874360552610597e-06\n",
            "Epoch: 351 | Loss: 1.1705790257110493e-06\n",
            "Epoch: 352 | Loss: 1.1537172213138547e-06\n",
            "Epoch: 353 | Loss: 1.1369777439540485e-06\n",
            "Epoch: 354 | Loss: 1.1207914667465957e-06\n",
            "Epoch: 355 | Loss: 1.1042934602301102e-06\n",
            "Epoch: 356 | Loss: 1.0887672488024691e-06\n",
            "Epoch: 357 | Loss: 1.0729888799687615e-06\n",
            "Epoch: 358 | Loss: 1.0576850399957038e-06\n",
            "Epoch: 359 | Loss: 1.0417800240247743e-06\n",
            "Epoch: 360 | Loss: 1.027642838380416e-06\n",
            "Epoch: 361 | Loss: 1.0122573712578742e-06\n",
            "Epoch: 362 | Loss: 9.97858705886756e-07\n",
            "Epoch: 363 | Loss: 9.839670838118764e-07\n",
            "Epoch: 364 | Loss: 9.694863365439232e-07\n",
            "Epoch: 365 | Loss: 9.557941211824073e-07\n",
            "Epoch: 366 | Loss: 9.421994491276564e-07\n",
            "Epoch: 367 | Loss: 9.27862174648908e-07\n",
            "Epoch: 368 | Loss: 9.146354500444431e-07\n",
            "Epoch: 369 | Loss: 9.017242632580746e-07\n",
            "Epoch: 370 | Loss: 8.885210718290182e-07\n",
            "Epoch: 371 | Loss: 8.75850275861012e-07\n",
            "Epoch: 372 | Loss: 8.633790571366262e-07\n",
            "Epoch: 373 | Loss: 8.508362157044758e-07\n",
            "Epoch: 374 | Loss: 8.384924967685947e-07\n",
            "Epoch: 375 | Loss: 8.267136308859335e-07\n",
            "Epoch: 376 | Loss: 8.148084020831448e-07\n",
            "Epoch: 377 | Loss: 8.030411322579312e-07\n",
            "Epoch: 378 | Loss: 7.918772553239251e-07\n",
            "Epoch: 379 | Loss: 7.802264576639573e-07\n",
            "Epoch: 380 | Loss: 7.692228791711386e-07\n",
            "Epoch: 381 | Loss: 7.582470971101429e-07\n",
            "Epoch: 382 | Loss: 7.468473199878645e-07\n",
            "Epoch: 383 | Loss: 7.364317866631609e-07\n",
            "Epoch: 384 | Loss: 7.259405379045347e-07\n",
            "Epoch: 385 | Loss: 7.15279213636677e-07\n",
            "Epoch: 386 | Loss: 7.050869612612587e-07\n",
            "Epoch: 387 | Loss: 6.94822176683374e-07\n",
            "Epoch: 388 | Loss: 6.847296845080564e-07\n",
            "Epoch: 389 | Loss: 6.749964995833579e-07\n",
            "Epoch: 390 | Loss: 6.651919761679892e-07\n",
            "Epoch: 391 | Loss: 6.555524123541545e-07\n",
            "Epoch: 392 | Loss: 6.464973125730467e-07\n",
            "Epoch: 393 | Loss: 6.370883625095303e-07\n",
            "Epoch: 394 | Loss: 6.28162069915561e-07\n",
            "Epoch: 395 | Loss: 6.187956955727714e-07\n",
            "Epoch: 396 | Loss: 6.099540428294858e-07\n",
            "Epoch: 397 | Loss: 6.011760547153244e-07\n",
            "Epoch: 398 | Loss: 5.928193900217593e-07\n",
            "Epoch: 399 | Loss: 5.841660026817408e-07\n",
            "Epoch: 400 | Loss: 5.757963776886754e-07\n",
            "Epoch: 401 | Loss: 5.672685006175016e-07\n",
            "Epoch: 402 | Loss: 5.588922249444295e-07\n",
            "Epoch: 403 | Loss: 5.513530254575016e-07\n",
            "Epoch: 404 | Loss: 5.433515184449789e-07\n",
            "Epoch: 405 | Loss: 5.353660981199937e-07\n",
            "Epoch: 406 | Loss: 5.277356649457943e-07\n",
            "Epoch: 407 | Loss: 5.202014676797262e-07\n",
            "Epoch: 408 | Loss: 5.127214990352513e-07\n",
            "Epoch: 409 | Loss: 5.052957590123697e-07\n",
            "Epoch: 410 | Loss: 4.982114774065849e-07\n",
            "Epoch: 411 | Loss: 4.908513915324875e-07\n",
            "Epoch: 412 | Loss: 4.838694280806521e-07\n",
            "Epoch: 413 | Loss: 4.769374868374143e-07\n",
            "Epoch: 414 | Loss: 4.70055567802774e-07\n",
            "Epoch: 415 | Loss: 4.6322367097673123e-07\n",
            "Epoch: 416 | Loss: 4.5683486860070843e-07\n",
            "Epoch: 417 | Loss: 4.4978833102504723e-07\n",
            "Epoch: 418 | Loss: 4.4368636054059607e-07\n",
            "Epoch: 419 | Loss: 4.373187039163895e-07\n",
            "Epoch: 420 | Loss: 4.309970904614602e-07\n",
            "Epoch: 421 | Loss: 4.2483543438720517e-07\n",
            "Epoch: 422 | Loss: 4.187928084320447e-07\n",
            "Epoch: 423 | Loss: 4.1268214090450783e-07\n",
            "Epoch: 424 | Loss: 4.065418579557445e-07\n",
            "Epoch: 425 | Loss: 4.011101282230811e-07\n",
            "Epoch: 426 | Loss: 3.950568157051748e-07\n",
            "Epoch: 427 | Loss: 3.893035795954347e-07\n",
            "Epoch: 428 | Loss: 3.839161308860639e-07\n",
            "Epoch: 429 | Loss: 3.782447492994834e-07\n",
            "Epoch: 430 | Loss: 3.7297087374099647e-07\n",
            "Epoch: 431 | Loss: 3.6766306266144966e-07\n",
            "Epoch: 432 | Loss: 3.6246387935534585e-07\n",
            "Epoch: 433 | Loss: 3.572315563360462e-07\n",
            "Epoch: 434 | Loss: 3.52037432094221e-07\n",
            "Epoch: 435 | Loss: 3.468472300482972e-07\n",
            "Epoch: 436 | Loss: 3.4206959753646515e-07\n",
            "Epoch: 437 | Loss: 3.369537466824113e-07\n",
            "Epoch: 438 | Loss: 3.321777626297262e-07\n",
            "Epoch: 439 | Loss: 3.2746930855864775e-07\n",
            "Epoch: 440 | Loss: 3.2246430237137247e-07\n",
            "Epoch: 441 | Loss: 3.1785822329766233e-07\n",
            "Epoch: 442 | Loss: 3.134153416795016e-07\n",
            "Epoch: 443 | Loss: 3.088428570663382e-07\n",
            "Epoch: 444 | Loss: 3.044951881747693e-07\n",
            "Epoch: 445 | Loss: 3.0017901053724927e-07\n",
            "Epoch: 446 | Loss: 2.95672634820221e-07\n",
            "Epoch: 447 | Loss: 2.91733385893167e-07\n",
            "Epoch: 448 | Loss: 2.872286586352857e-07\n",
            "Epoch: 449 | Loss: 2.833462531270925e-07\n",
            "Epoch: 450 | Loss: 2.7890700948773883e-07\n",
            "Epoch: 451 | Loss: 2.7483793019200675e-07\n",
            "Epoch: 452 | Loss: 2.7116135470350855e-07\n",
            "Epoch: 453 | Loss: 2.671493462003127e-07\n",
            "Epoch: 454 | Loss: 2.632864379847888e-07\n",
            "Epoch: 455 | Loss: 2.5977743689509225e-07\n",
            "Epoch: 456 | Loss: 2.560568646003958e-07\n",
            "Epoch: 457 | Loss: 2.523042894608807e-07\n",
            "Epoch: 458 | Loss: 2.486090124875773e-07\n",
            "Epoch: 459 | Loss: 2.45112971697381e-07\n",
            "Epoch: 460 | Loss: 2.4149926503014285e-07\n",
            "Epoch: 461 | Loss: 2.3791250214344473e-07\n",
            "Epoch: 462 | Loss: 2.346899350413878e-07\n",
            "Epoch: 463 | Loss: 2.3115421754482668e-07\n",
            "Epoch: 464 | Loss: 2.2806148081144784e-07\n",
            "Epoch: 465 | Loss: 2.2471368765764055e-07\n",
            "Epoch: 466 | Loss: 2.2144564582049497e-07\n",
            "Epoch: 467 | Loss: 2.1833722030351055e-07\n",
            "Epoch: 468 | Loss: 2.1511601744350628e-07\n",
            "Epoch: 469 | Loss: 2.11918916193099e-07\n",
            "Epoch: 470 | Loss: 2.0895805619147723e-07\n",
            "Epoch: 471 | Loss: 2.0593876115526655e-07\n",
            "Epoch: 472 | Loss: 2.02810781502194e-07\n",
            "Epoch: 473 | Loss: 2.0009656509500928e-07\n",
            "Epoch: 474 | Loss: 1.9734898160095327e-07\n",
            "Epoch: 475 | Loss: 1.9446639498710283e-07\n",
            "Epoch: 476 | Loss: 1.9142709106745315e-07\n",
            "Epoch: 477 | Loss: 1.8879040908359457e-07\n",
            "Epoch: 478 | Loss: 1.8617203068060917e-07\n",
            "Epoch: 479 | Loss: 1.8357195585849695e-07\n",
            "Epoch: 480 | Loss: 1.807427452149568e-07\n",
            "Epoch: 481 | Loss: 1.7818098285715678e-07\n",
            "Epoch: 482 | Loss: 1.7544266484037507e-07\n",
            "Epoch: 483 | Loss: 1.732817054289626e-07\n",
            "Epoch: 484 | Loss: 1.7070152580345166e-07\n",
            "Epoch: 485 | Loss: 1.6802141544758342e-07\n",
            "Epoch: 486 | Loss: 1.6578860595473088e-07\n",
            "Epoch: 487 | Loss: 1.6345285303032142e-07\n",
            "Epoch: 488 | Loss: 1.6099380673040287e-07\n",
            "Epoch: 489 | Loss: 1.587387146173569e-07\n",
            "Epoch: 490 | Loss: 1.5652244655939285e-07\n",
            "Epoch: 491 | Loss: 1.5429895938723348e-07\n",
            "Epoch: 492 | Loss: 1.5209138837235514e-07\n",
            "Epoch: 493 | Loss: 1.4989973351475783e-07\n",
            "Epoch: 494 | Loss: 1.4772399481444154e-07\n",
            "Epoch: 495 | Loss: 1.4556417227140628e-07\n",
            "Epoch: 496 | Loss: 1.4342026588565204e-07\n",
            "Epoch: 497 | Loss: 1.4122656466497574e-07\n",
            "Epoch: 498 | Loss: 1.3944037391411257e-07\n",
            "Epoch: 499 | Loss: 1.3734216963712242e-07\n",
            "prediction (after training) 4 7.999573707580566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax\n",
        "- jax 는 pytorch와 다르게 함수형 프로그램을 강조하고 있음 (class 가 필요없음)\n",
        "- 모델과 관련된 파라미터와 계산을 함수로 구현하여 사용하는 것이 일반적임\n",
        "\n",
        "\n",
        "### 변경사항\n",
        "  - loss 를 optax 내장함수로 바꾸어줌\n",
        "  - opimizer 가 2줄짜리로 변경 : opt_state, params 업데이트 필요\n",
        "\n",
        "### optax 를 사용한 최적화 코드\n",
        "\n",
        "[참고자료](https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.sgd)\n",
        "\n",
        "```\n",
        "# optax.sgd 를 활용해 optimizer 를 초기화 할때, params 가 jax 배열이 아니기때문에 오류가 발생\n",
        "optimizer = optax.sgd(learning_rate = 0.01)\n",
        "\n",
        "# 아래와 같이 한번 init 해주어야함\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "\n",
        "# optimizer 를 활용해 업데이트를 하면 opt_state 가 업데이트 되며,\n",
        "# 이를 다시 apply_updates 로 처리해서 실제 업데이트도 진행\n",
        "updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "params = optax.apply_updates(params, updates)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Q. jit 을 gradient 계산하는 부분에 적용하기 위해서는 어떻게 해야 하는가?\n",
        "\n",
        "\n",
        "chat gpt 설명\n",
        "```\n",
        "@jit\n",
        "def value_and_grad(params, x, y):\n",
        "    return jax.value_and_grad(loss)(params, x, y)\n",
        "```\n"
      ],
      "metadata": {
        "id": "5t0YGypVuwGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit\n",
        "import optax\n",
        "\n",
        "x_data = jnp.array([[1.0], [2.0], [3.0]])\n",
        "y_data = jnp.array([[2.0], [4.0], [6.0]])\n",
        "\n",
        "# define model\n",
        "def model(params, x):\n",
        "  return jnp.dot(x, params['weight']) + params['bias']\n",
        "\n",
        "# define loss(params)\n",
        "def loss(params, x, y):\n",
        "  y_pred = model(params, x)\n",
        "  return jnp.mean((y_pred - y) ** 2)\n",
        "\n",
        "# gradient computation\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "# initialize parameters\n",
        "init_params = {'weight': jnp.array([[1.0]]), 'bias': jnp.array([0.0])}\n",
        "\n",
        "# initialize optimizer\n"
      ],
      "metadata": {
        "id": "LyIA4C4Puh_s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit\n",
        "import optax\n",
        "\n",
        "# Define data\n",
        "x_data = jnp.array([[1.0], [2.0], [3.0]])\n",
        "y_data = jnp.array([[2.0], [4.0], [6.0]])\n",
        "\n",
        "# Define model\n",
        "def model(params, x):\n",
        "    return jnp.dot(x, params['weight']) + params['bias']\n",
        "\n",
        "# Loss function\n",
        "def loss(params, x, y):\n",
        "    y_pred = model(params, x)\n",
        "    return jnp.mean((y_pred - y) ** 2)\n",
        "\n",
        "# Gradient computation\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "# Initialize parameters\n",
        "init_params = {'weight': jnp.array([[1.0]]), 'bias': jnp.array([0.0])}\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optax.sgd(learning_rate=0.01)\n",
        "\n",
        "@jit\n",
        "def update(params, opt_state, x, y):\n",
        "    grads = grad_loss(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, opt_state\n",
        "\n",
        "# Initialize optimizer state\n",
        "opt_state = optimizer.init(init_params)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "params = init_params\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    params, opt_state = update(params, opt_state, x_data, y_data)\n",
        "    loss_val = loss(params, x_data, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss_val}')\n",
        "\n",
        "# After training\n",
        "hour_var = jnp.array([[4.0]])\n",
        "y_pred = model(params, hour_var)\n",
        "print(\"Prediction (after training):\", 4, y_pred[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba3XGbKLxuUe",
        "outputId": "f0207bbc-c2e8-4897-b221-a01ff021cb0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 3.6927404403686523\n",
            "Epoch: 1 | Loss: 2.9228854179382324\n",
            "Epoch: 2 | Loss: 2.3143362998962402\n",
            "Epoch: 3 | Loss: 1.8332923650741577\n",
            "Epoch: 4 | Loss: 1.4530338048934937\n",
            "Epoch: 5 | Loss: 1.1524405479431152\n",
            "Epoch: 6 | Loss: 0.9148194193840027\n",
            "Epoch: 7 | Loss: 0.7269737720489502\n",
            "Epoch: 8 | Loss: 0.5784738659858704\n",
            "Epoch: 9 | Loss: 0.46107369661331177\n",
            "Epoch: 10 | Loss: 0.36825668811798096\n",
            "Epoch: 11 | Loss: 0.2948717176914215\n",
            "Epoch: 12 | Loss: 0.23684683442115784\n",
            "Epoch: 13 | Loss: 0.19096308946609497\n",
            "Epoch: 14 | Loss: 0.15467634797096252\n",
            "Epoch: 15 | Loss: 0.1259755939245224\n",
            "Epoch: 16 | Loss: 0.1032714694738388\n",
            "Epoch: 17 | Loss: 0.08530738204717636\n",
            "Epoch: 18 | Loss: 0.07108993083238602\n",
            "Epoch: 19 | Loss: 0.05983422324061394\n",
            "Epoch: 20 | Loss: 0.05091980844736099\n",
            "Epoch: 21 | Loss: 0.04385606199502945\n",
            "Epoch: 22 | Loss: 0.03825516626238823\n",
            "Epoch: 23 | Loss: 0.03381074592471123\n",
            "Epoch: 24 | Loss: 0.0302805807441473\n",
            "Epoch: 25 | Loss: 0.027473006397485733\n",
            "Epoch: 26 | Loss: 0.02523679845035076\n",
            "Epoch: 27 | Loss: 0.02345230244100094\n",
            "Epoch: 28 | Loss: 0.022024905309081078\n",
            "Epoch: 29 | Loss: 0.02087988518178463\n",
            "Epoch: 30 | Loss: 0.019958246499300003\n",
            "Epoch: 31 | Loss: 0.019212985411286354\n",
            "Epoch: 32 | Loss: 0.018607493489980698\n",
            "Epoch: 33 | Loss: 0.018112465739250183\n",
            "Epoch: 34 | Loss: 0.01770477555692196\n",
            "Epoch: 35 | Loss: 0.01736629009246826\n",
            "Epoch: 36 | Loss: 0.01708250865340233\n",
            "Epoch: 37 | Loss: 0.01684209704399109\n",
            "Epoch: 38 | Loss: 0.01663602516055107\n",
            "Epoch: 39 | Loss: 0.016457153484225273\n",
            "Epoch: 40 | Loss: 0.016299894079566002\n",
            "Epoch: 41 | Loss: 0.01615983620285988\n",
            "Epoch: 42 | Loss: 0.016033349558711052\n",
            "Epoch: 43 | Loss: 0.015917692333459854\n",
            "Epoch: 44 | Loss: 0.015810763463377953\n",
            "Epoch: 45 | Loss: 0.01571068912744522\n",
            "Epoch: 46 | Loss: 0.015616197139024734\n",
            "Epoch: 47 | Loss: 0.015526125207543373\n",
            "Epoch: 48 | Loss: 0.015439623966813087\n",
            "Epoch: 49 | Loss: 0.015356041491031647\n",
            "Epoch: 50 | Loss: 0.015274867415428162\n",
            "Epoch: 51 | Loss: 0.015195593237876892\n",
            "Epoch: 52 | Loss: 0.015118014067411423\n",
            "Epoch: 53 | Loss: 0.015041693113744259\n",
            "Epoch: 54 | Loss: 0.014966603368520737\n",
            "Epoch: 55 | Loss: 0.01489241048693657\n",
            "Epoch: 56 | Loss: 0.014819113537669182\n",
            "Epoch: 57 | Loss: 0.01474647969007492\n",
            "Epoch: 58 | Loss: 0.014674535021185875\n",
            "Epoch: 59 | Loss: 0.01460318174213171\n",
            "Epoch: 60 | Loss: 0.014532344415783882\n",
            "Epoch: 61 | Loss: 0.014461960643529892\n",
            "Epoch: 62 | Loss: 0.014392105862498283\n",
            "Epoch: 63 | Loss: 0.014322626404464245\n",
            "Epoch: 64 | Loss: 0.014253555797040462\n",
            "Epoch: 65 | Loss: 0.014184905216097832\n",
            "Epoch: 66 | Loss: 0.01411665789783001\n",
            "Epoch: 67 | Loss: 0.014048680663108826\n",
            "Epoch: 68 | Loss: 0.013981116935610771\n",
            "Epoch: 69 | Loss: 0.0139138950034976\n",
            "Epoch: 70 | Loss: 0.013847004622220993\n",
            "Epoch: 71 | Loss: 0.01378045417368412\n",
            "Epoch: 72 | Loss: 0.013714270666241646\n",
            "Epoch: 73 | Loss: 0.013648368418216705\n",
            "Epoch: 74 | Loss: 0.013582756742835045\n",
            "Epoch: 75 | Loss: 0.01351756602525711\n",
            "Epoch: 76 | Loss: 0.013452589511871338\n",
            "Epoch: 77 | Loss: 0.013388006016612053\n",
            "Epoch: 78 | Loss: 0.013323701918125153\n",
            "Epoch: 79 | Loss: 0.013259721919894218\n",
            "Epoch: 80 | Loss: 0.013196036219596863\n",
            "Epoch: 81 | Loss: 0.013132672756910324\n",
            "Epoch: 82 | Loss: 0.013069591484963894\n",
            "Epoch: 83 | Loss: 0.013006842695176601\n",
            "Epoch: 84 | Loss: 0.012944364920258522\n",
            "Epoch: 85 | Loss: 0.012882212176918983\n",
            "Epoch: 86 | Loss: 0.01282031461596489\n",
            "Epoch: 87 | Loss: 0.012758749537169933\n",
            "Epoch: 88 | Loss: 0.012697495520114899\n",
            "Epoch: 89 | Loss: 0.012636538594961166\n",
            "Epoch: 90 | Loss: 0.01257583312690258\n",
            "Epoch: 91 | Loss: 0.012515469454228878\n",
            "Epoch: 92 | Loss: 0.012455343268811703\n",
            "Epoch: 93 | Loss: 0.012395529076457024\n",
            "Epoch: 94 | Loss: 0.01233602687716484\n",
            "Epoch: 95 | Loss: 0.012276794761419296\n",
            "Epoch: 96 | Loss: 0.012217829935252666\n",
            "Epoch: 97 | Loss: 0.012159126810729504\n",
            "Epoch: 98 | Loss: 0.01210075058043003\n",
            "Epoch: 99 | Loss: 0.012042644433677197\n",
            "Epoch: 100 | Loss: 0.011984815821051598\n",
            "Epoch: 101 | Loss: 0.01192726194858551\n",
            "Epoch: 102 | Loss: 0.011870015412569046\n",
            "Epoch: 103 | Loss: 0.011813003569841385\n",
            "Epoch: 104 | Loss: 0.011756262741982937\n",
            "Epoch: 105 | Loss: 0.011699827387928963\n",
            "Epoch: 106 | Loss: 0.011643621139228344\n",
            "Epoch: 107 | Loss: 0.011587703600525856\n",
            "Epoch: 108 | Loss: 0.011532067321240902\n",
            "Epoch: 109 | Loss: 0.011476724408566952\n",
            "Epoch: 110 | Loss: 0.011421570554375648\n",
            "Epoch: 111 | Loss: 0.011366765014827251\n",
            "Epoch: 112 | Loss: 0.011312148533761501\n",
            "Epoch: 113 | Loss: 0.011257810518145561\n",
            "Epoch: 114 | Loss: 0.011203768663108349\n",
            "Epoch: 115 | Loss: 0.01114995963871479\n",
            "Epoch: 116 | Loss: 0.011096443049609661\n",
            "Epoch: 117 | Loss: 0.011043129488825798\n",
            "Epoch: 118 | Loss: 0.010990103706717491\n",
            "Epoch: 119 | Loss: 0.010937312617897987\n",
            "Epoch: 120 | Loss: 0.010884800925850868\n",
            "Epoch: 121 | Loss: 0.010832518339157104\n",
            "Epoch: 122 | Loss: 0.010780523531138897\n",
            "Epoch: 123 | Loss: 0.010728754103183746\n",
            "Epoch: 124 | Loss: 0.010677244514226913\n",
            "Epoch: 125 | Loss: 0.010625950060784817\n",
            "Epoch: 126 | Loss: 0.010574947111308575\n",
            "Epoch: 127 | Loss: 0.01052413322031498\n",
            "Epoch: 128 | Loss: 0.01047360897064209\n",
            "Epoch: 129 | Loss: 0.010423323139548302\n",
            "Epoch: 130 | Loss: 0.010373263619840145\n",
            "Epoch: 131 | Loss: 0.010323448106646538\n",
            "Epoch: 132 | Loss: 0.010273878462612629\n",
            "Epoch: 133 | Loss: 0.010224560275673866\n",
            "Epoch: 134 | Loss: 0.0101754330098629\n",
            "Epoch: 135 | Loss: 0.01012658141553402\n",
            "Epoch: 136 | Loss: 0.01007796823978424\n",
            "Epoch: 137 | Loss: 0.010029580444097519\n",
            "Epoch: 138 | Loss: 0.00998140312731266\n",
            "Epoch: 139 | Loss: 0.009933466091752052\n",
            "Epoch: 140 | Loss: 0.009885763749480247\n",
            "Epoch: 141 | Loss: 0.009838307276368141\n",
            "Epoch: 142 | Loss: 0.00979106966406107\n",
            "Epoch: 143 | Loss: 0.009744038805365562\n",
            "Epoch: 144 | Loss: 0.009697243571281433\n",
            "Epoch: 145 | Loss: 0.009650657884776592\n",
            "Epoch: 146 | Loss: 0.009604334831237793\n",
            "Epoch: 147 | Loss: 0.009558221325278282\n",
            "Epoch: 148 | Loss: 0.009512318298220634\n",
            "Epoch: 149 | Loss: 0.00946661178022623\n",
            "Epoch: 150 | Loss: 0.00942117627710104\n",
            "Epoch: 151 | Loss: 0.0093759223818779\n",
            "Epoch: 152 | Loss: 0.009330915287137032\n",
            "Epoch: 153 | Loss: 0.009286114014685154\n",
            "Epoch: 154 | Loss: 0.009241517633199692\n",
            "Epoch: 155 | Loss: 0.00919712521135807\n",
            "Epoch: 156 | Loss: 0.009152939543128014\n",
            "Epoch: 157 | Loss: 0.009109025821089745\n",
            "Epoch: 158 | Loss: 0.009065285325050354\n",
            "Epoch: 159 | Loss: 0.009021727368235588\n",
            "Epoch: 160 | Loss: 0.008978413417935371\n",
            "Epoch: 161 | Loss: 0.008935295976698399\n",
            "Epoch: 162 | Loss: 0.008892405778169632\n",
            "Epoch: 163 | Loss: 0.008849685080349445\n",
            "Epoch: 164 | Loss: 0.0088071683421731\n",
            "Epoch: 165 | Loss: 0.008764892816543579\n",
            "Epoch: 166 | Loss: 0.00872280914336443\n",
            "Epoch: 167 | Loss: 0.008680938743054867\n",
            "Epoch: 168 | Loss: 0.008639222010970116\n",
            "Epoch: 169 | Loss: 0.00859774462878704\n",
            "Epoch: 170 | Loss: 0.00855646189302206\n",
            "Epoch: 171 | Loss: 0.008515377528965473\n",
            "Epoch: 172 | Loss: 0.008474498055875301\n",
            "Epoch: 173 | Loss: 0.008433780632913113\n",
            "Epoch: 174 | Loss: 0.00839326623827219\n",
            "Epoch: 175 | Loss: 0.008352978155016899\n",
            "Epoch: 176 | Loss: 0.008312857709825039\n",
            "Epoch: 177 | Loss: 0.008272949606180191\n",
            "Epoch: 178 | Loss: 0.008233223110437393\n",
            "Epoch: 179 | Loss: 0.008193688467144966\n",
            "Epoch: 180 | Loss: 0.008154338225722313\n",
            "Epoch: 181 | Loss: 0.008115176111459732\n",
            "Epoch: 182 | Loss: 0.008076219819486141\n",
            "Epoch: 183 | Loss: 0.008037419989705086\n",
            "Epoch: 184 | Loss: 0.007998834364116192\n",
            "Epoch: 185 | Loss: 0.007960425689816475\n",
            "Epoch: 186 | Loss: 0.00792221911251545\n",
            "Epoch: 187 | Loss: 0.007884152233600616\n",
            "Epoch: 188 | Loss: 0.007846287451684475\n",
            "Epoch: 189 | Loss: 0.007808598689734936\n",
            "Epoch: 190 | Loss: 0.007771120872348547\n",
            "Epoch: 191 | Loss: 0.007733791135251522\n",
            "Epoch: 192 | Loss: 0.007696646265685558\n",
            "Epoch: 193 | Loss: 0.007659689988940954\n",
            "Epoch: 194 | Loss: 0.007622893434017897\n",
            "Epoch: 195 | Loss: 0.007586302235722542\n",
            "Epoch: 196 | Loss: 0.0075498949736356735\n",
            "Epoch: 197 | Loss: 0.007513602264225483\n",
            "Epoch: 198 | Loss: 0.007477547042071819\n",
            "Epoch: 199 | Loss: 0.007441628724336624\n",
            "Epoch: 200 | Loss: 0.0074059003964066505\n",
            "Epoch: 201 | Loss: 0.007370343431830406\n",
            "Epoch: 202 | Loss: 0.007334940135478973\n",
            "Epoch: 203 | Loss: 0.007299704011529684\n",
            "Epoch: 204 | Loss: 0.0072646597400307655\n",
            "Epoch: 205 | Loss: 0.0072297584265470505\n",
            "Epoch: 206 | Loss: 0.007195055019110441\n",
            "Epoch: 207 | Loss: 0.00716052483767271\n",
            "Epoch: 208 | Loss: 0.007126107346266508\n",
            "Epoch: 209 | Loss: 0.00709188636392355\n",
            "Epoch: 210 | Loss: 0.007057852111756802\n",
            "Epoch: 211 | Loss: 0.007023964077234268\n",
            "Epoch: 212 | Loss: 0.0069902194663882256\n",
            "Epoch: 213 | Loss: 0.00695665692910552\n",
            "Epoch: 214 | Loss: 0.006923259235918522\n",
            "Epoch: 215 | Loss: 0.006889985874295235\n",
            "Epoch: 216 | Loss: 0.006856904365122318\n",
            "Epoch: 217 | Loss: 0.006823994219303131\n",
            "Epoch: 218 | Loss: 0.0067912316881120205\n",
            "Epoch: 219 | Loss: 0.006758606992661953\n",
            "Epoch: 220 | Loss: 0.006726167630404234\n",
            "Epoch: 221 | Loss: 0.006693858187645674\n",
            "Epoch: 222 | Loss: 0.006661722436547279\n",
            "Epoch: 223 | Loss: 0.00662971381098032\n",
            "Epoch: 224 | Loss: 0.006597890984266996\n",
            "Epoch: 225 | Loss: 0.006566202268004417\n",
            "Epoch: 226 | Loss: 0.006534667685627937\n",
            "Epoch: 227 | Loss: 0.00650327792391181\n",
            "Epoch: 228 | Loss: 0.006472059525549412\n",
            "Epoch: 229 | Loss: 0.006440976168960333\n",
            "Epoch: 230 | Loss: 0.006410051137208939\n",
            "Epoch: 231 | Loss: 0.006379272788763046\n",
            "Epoch: 232 | Loss: 0.0063486406579613686\n",
            "Epoch: 233 | Loss: 0.0063181305304169655\n",
            "Epoch: 234 | Loss: 0.006287800148129463\n",
            "Epoch: 235 | Loss: 0.006257619243115187\n",
            "Epoch: 236 | Loss: 0.006227556616067886\n",
            "Epoch: 237 | Loss: 0.006197643466293812\n",
            "Epoch: 238 | Loss: 0.0061678956262767315\n",
            "Epoch: 239 | Loss: 0.006138278171420097\n",
            "Epoch: 240 | Loss: 0.006108790170401335\n",
            "Epoch: 241 | Loss: 0.006079461425542831\n",
            "Epoch: 242 | Loss: 0.006050271913409233\n",
            "Epoch: 243 | Loss: 0.006021230947226286\n",
            "Epoch: 244 | Loss: 0.005992300342768431\n",
            "Epoch: 245 | Loss: 0.005963518284261227\n",
            "Epoch: 246 | Loss: 0.005934884771704674\n",
            "Epoch: 247 | Loss: 0.005906369537115097\n",
            "Epoch: 248 | Loss: 0.005878006108105183\n",
            "Epoch: 249 | Loss: 0.00584978936240077\n",
            "Epoch: 250 | Loss: 0.00582171231508255\n",
            "Epoch: 251 | Loss: 0.00579374423250556\n",
            "Epoch: 252 | Loss: 0.005765925161540508\n",
            "Epoch: 253 | Loss: 0.005738237872719765\n",
            "Epoch: 254 | Loss: 0.005710681900382042\n",
            "Epoch: 255 | Loss: 0.005683263298124075\n",
            "Epoch: 256 | Loss: 0.0056559862568974495\n",
            "Epoch: 257 | Loss: 0.005628816783428192\n",
            "Epoch: 258 | Loss: 0.005601784214377403\n",
            "Epoch: 259 | Loss: 0.005574870388954878\n",
            "Epoch: 260 | Loss: 0.005548086017370224\n",
            "Epoch: 261 | Loss: 0.005521473940461874\n",
            "Epoch: 262 | Loss: 0.005494961515069008\n",
            "Epoch: 263 | Loss: 0.005468565039336681\n",
            "Epoch: 264 | Loss: 0.0054423133842647076\n",
            "Epoch: 265 | Loss: 0.005416177213191986\n",
            "Epoch: 266 | Loss: 0.005390155594795942\n",
            "Epoch: 267 | Loss: 0.005364258773624897\n",
            "Epoch: 268 | Loss: 0.005338494665920734\n",
            "Epoch: 269 | Loss: 0.005312883295118809\n",
            "Epoch: 270 | Loss: 0.005287367384880781\n",
            "Epoch: 271 | Loss: 0.005261990707367659\n",
            "Epoch: 272 | Loss: 0.005236702039837837\n",
            "Epoch: 273 | Loss: 0.0052115595899522305\n",
            "Epoch: 274 | Loss: 0.005186540074646473\n",
            "Epoch: 275 | Loss: 0.0051616220735013485\n",
            "Epoch: 276 | Loss: 0.0051368409767746925\n",
            "Epoch: 277 | Loss: 0.005112162791192532\n",
            "Epoch: 278 | Loss: 0.005087622441351414\n",
            "Epoch: 279 | Loss: 0.00506318686529994\n",
            "Epoch: 280 | Loss: 0.005038886331021786\n",
            "Epoch: 281 | Loss: 0.005014684516936541\n",
            "Epoch: 282 | Loss: 0.0049905842170119286\n",
            "Epoch: 283 | Loss: 0.0049666245467960835\n",
            "Epoch: 284 | Loss: 0.004942798055708408\n",
            "Epoch: 285 | Loss: 0.004919035360217094\n",
            "Epoch: 286 | Loss: 0.00489541981369257\n",
            "Epoch: 287 | Loss: 0.004871909972280264\n",
            "Epoch: 288 | Loss: 0.004848513286560774\n",
            "Epoch: 289 | Loss: 0.0048252190463244915\n",
            "Epoch: 290 | Loss: 0.004802071955054998\n",
            "Epoch: 291 | Loss: 0.004778996575623751\n",
            "Epoch: 292 | Loss: 0.00475606694817543\n",
            "Epoch: 293 | Loss: 0.004733202047646046\n",
            "Epoch: 294 | Loss: 0.00471048429608345\n",
            "Epoch: 295 | Loss: 0.004687868990004063\n",
            "Epoch: 296 | Loss: 0.004665365908294916\n",
            "Epoch: 297 | Loss: 0.004642965272068977\n",
            "Epoch: 298 | Loss: 0.004620668478310108\n",
            "Epoch: 299 | Loss: 0.004598475061357021\n",
            "Epoch: 300 | Loss: 0.0045763831585645676\n",
            "Epoch: 301 | Loss: 0.004554418846964836\n",
            "Epoch: 302 | Loss: 0.004532536957412958\n",
            "Epoch: 303 | Loss: 0.004510757513344288\n",
            "Epoch: 304 | Loss: 0.004489120561629534\n",
            "Epoch: 305 | Loss: 0.004467566963285208\n",
            "Epoch: 306 | Loss: 0.004446107428520918\n",
            "Epoch: 307 | Loss: 0.004424760118126869\n",
            "Epoch: 308 | Loss: 0.004403506405651569\n",
            "Epoch: 309 | Loss: 0.00438235979527235\n",
            "Epoch: 310 | Loss: 0.004361310042440891\n",
            "Epoch: 311 | Loss: 0.004340382292866707\n",
            "Epoch: 312 | Loss: 0.004319530911743641\n",
            "Epoch: 313 | Loss: 0.004298782907426357\n",
            "Epoch: 314 | Loss: 0.004278130829334259\n",
            "Epoch: 315 | Loss: 0.004257585387676954\n",
            "Epoch: 316 | Loss: 0.004237161483615637\n",
            "Epoch: 317 | Loss: 0.00421679113060236\n",
            "Epoch: 318 | Loss: 0.004196550231426954\n",
            "Epoch: 319 | Loss: 0.004176395013928413\n",
            "Epoch: 320 | Loss: 0.004156344570219517\n",
            "Epoch: 321 | Loss: 0.004136381205171347\n",
            "Epoch: 322 | Loss: 0.004116510972380638\n",
            "Epoch: 323 | Loss: 0.004096757620573044\n",
            "Epoch: 324 | Loss: 0.004077077377587557\n",
            "Epoch: 325 | Loss: 0.004057512618601322\n",
            "Epoch: 326 | Loss: 0.004038025625050068\n",
            "Epoch: 327 | Loss: 0.004018629901111126\n",
            "Epoch: 328 | Loss: 0.0039993333630263805\n",
            "Epoch: 329 | Loss: 0.003980129957199097\n",
            "Epoch: 330 | Loss: 0.003961019683629274\n",
            "Epoch: 331 | Loss: 0.003941982984542847\n",
            "Epoch: 332 | Loss: 0.003923061303794384\n",
            "Epoch: 333 | Loss: 0.0039042308926582336\n",
            "Epoch: 334 | Loss: 0.003885469166561961\n",
            "Epoch: 335 | Loss: 0.003866808954626322\n",
            "Epoch: 336 | Loss: 0.0038482355885207653\n",
            "Epoch: 337 | Loss: 0.003829773049801588\n",
            "Epoch: 338 | Loss: 0.0038113673217594624\n",
            "Epoch: 339 | Loss: 0.0037930821999907494\n",
            "Epoch: 340 | Loss: 0.003774859942495823\n",
            "Epoch: 341 | Loss: 0.0037567371036857367\n",
            "Epoch: 342 | Loss: 0.0037386827170848846\n",
            "Epoch: 343 | Loss: 0.0037207268178462982\n",
            "Epoch: 344 | Loss: 0.0037028572987765074\n",
            "Epoch: 345 | Loss: 0.0036850813776254654\n",
            "Epoch: 346 | Loss: 0.003667393233627081\n",
            "Epoch: 347 | Loss: 0.0036497614346444607\n",
            "Epoch: 348 | Loss: 0.00363224558532238\n",
            "Epoch: 349 | Loss: 0.00361480750143528\n",
            "Epoch: 350 | Loss: 0.0035974541679024696\n",
            "Epoch: 351 | Loss: 0.0035801706835627556\n",
            "Epoch: 352 | Loss: 0.003562991274520755\n",
            "Epoch: 353 | Loss: 0.003545862389728427\n",
            "Epoch: 354 | Loss: 0.0035288408398628235\n",
            "Epoch: 355 | Loss: 0.0035119024105370045\n",
            "Epoch: 356 | Loss: 0.003495034296065569\n",
            "Epoch: 357 | Loss: 0.003478253958746791\n",
            "Epoch: 358 | Loss: 0.0034615383483469486\n",
            "Epoch: 359 | Loss: 0.0034449249505996704\n",
            "Epoch: 360 | Loss: 0.003428395837545395\n",
            "Epoch: 361 | Loss: 0.003411923535168171\n",
            "Epoch: 362 | Loss: 0.00339552853256464\n",
            "Epoch: 363 | Loss: 0.0033792369067668915\n",
            "Epoch: 364 | Loss: 0.0033630123361945152\n",
            "Epoch: 365 | Loss: 0.003346853656694293\n",
            "Epoch: 366 | Loss: 0.0033307764679193497\n",
            "Epoch: 367 | Loss: 0.00331478426232934\n",
            "Epoch: 368 | Loss: 0.00329885957762599\n",
            "Epoch: 369 | Loss: 0.0032830291893333197\n",
            "Epoch: 370 | Loss: 0.003267263527959585\n",
            "Epoch: 371 | Loss: 0.0032515707425773144\n",
            "Epoch: 372 | Loss: 0.0032359552569687366\n",
            "Epoch: 373 | Loss: 0.003220404265448451\n",
            "Epoch: 374 | Loss: 0.0032049627043306828\n",
            "Epoch: 375 | Loss: 0.0031895549036562443\n",
            "Epoch: 376 | Loss: 0.0031742460560053587\n",
            "Epoch: 377 | Loss: 0.003159001236781478\n",
            "Epoch: 378 | Loss: 0.0031438337173312902\n",
            "Epoch: 379 | Loss: 0.0031287295278161764\n",
            "Epoch: 380 | Loss: 0.003113723127171397\n",
            "Epoch: 381 | Loss: 0.0030987586360424757\n",
            "Epoch: 382 | Loss: 0.0030838767997920513\n",
            "Epoch: 383 | Loss: 0.0030690773855894804\n",
            "Epoch: 384 | Loss: 0.0030543338507413864\n",
            "Epoch: 385 | Loss: 0.0030396683141589165\n",
            "Epoch: 386 | Loss: 0.0030250679701566696\n",
            "Epoch: 387 | Loss: 0.0030105430632829666\n",
            "Epoch: 388 | Loss: 0.002996093360707164\n",
            "Epoch: 389 | Loss: 0.002981704194098711\n",
            "Epoch: 390 | Loss: 0.0029673806857317686\n",
            "Epoch: 391 | Loss: 0.0029531321488320827\n",
            "Epoch: 392 | Loss: 0.002938944613561034\n",
            "Epoch: 393 | Loss: 0.002924820873886347\n",
            "Epoch: 394 | Loss: 0.0029107797890901566\n",
            "Epoch: 395 | Loss: 0.0028968031983822584\n",
            "Epoch: 396 | Loss: 0.0028828997164964676\n",
            "Epoch: 397 | Loss: 0.0028690570034086704\n",
            "Epoch: 398 | Loss: 0.0028552727308124304\n",
            "Epoch: 399 | Loss: 0.0028415662236511707\n",
            "Epoch: 400 | Loss: 0.002827912336215377\n",
            "Epoch: 401 | Loss: 0.002814335748553276\n",
            "Epoch: 402 | Loss: 0.0028008311055600643\n",
            "Epoch: 403 | Loss: 0.002787369303405285\n",
            "Epoch: 404 | Loss: 0.0027739848010241985\n",
            "Epoch: 405 | Loss: 0.0027606706134974957\n",
            "Epoch: 406 | Loss: 0.0027474132366478443\n",
            "Epoch: 407 | Loss: 0.0027342194225639105\n",
            "Epoch: 408 | Loss: 0.002721088705584407\n",
            "Epoch: 409 | Loss: 0.0027080215513706207\n",
            "Epoch: 410 | Loss: 0.0026950098108500242\n",
            "Epoch: 411 | Loss: 0.002682067919522524\n",
            "Epoch: 412 | Loss: 0.0026691951788961887\n",
            "Epoch: 413 | Loss: 0.0026563776191323996\n",
            "Epoch: 414 | Loss: 0.0026436219923198223\n",
            "Epoch: 415 | Loss: 0.002630916191264987\n",
            "Epoch: 416 | Loss: 0.002618297003209591\n",
            "Epoch: 417 | Loss: 0.0026057192590087652\n",
            "Epoch: 418 | Loss: 0.002593209035694599\n",
            "Epoch: 419 | Loss: 0.002580758184194565\n",
            "Epoch: 420 | Loss: 0.002568357391282916\n",
            "Epoch: 421 | Loss: 0.00255604088306427\n",
            "Epoch: 422 | Loss: 0.0025437658187001944\n",
            "Epoch: 423 | Loss: 0.002531548961997032\n",
            "Epoch: 424 | Loss: 0.0025193816982209682\n",
            "Epoch: 425 | Loss: 0.0025072842836380005\n",
            "Epoch: 426 | Loss: 0.002495227614417672\n",
            "Epoch: 427 | Loss: 0.0024832619819790125\n",
            "Epoch: 428 | Loss: 0.0024713249877095222\n",
            "Epoch: 429 | Loss: 0.0024594650603830814\n",
            "Epoch: 430 | Loss: 0.002447648672387004\n",
            "Epoch: 431 | Loss: 0.0024358988739550114\n",
            "Epoch: 432 | Loss: 0.0024242005310952663\n",
            "Epoch: 433 | Loss: 0.0024125531781464815\n",
            "Epoch: 434 | Loss: 0.00240098824724555\n",
            "Epoch: 435 | Loss: 0.0023894445039331913\n",
            "Epoch: 436 | Loss: 0.002377969678491354\n",
            "Epoch: 437 | Loss: 0.0023665581829845905\n",
            "Epoch: 438 | Loss: 0.0023551841732114553\n",
            "Epoch: 439 | Loss: 0.002343866741284728\n",
            "Epoch: 440 | Loss: 0.002332620322704315\n",
            "Epoch: 441 | Loss: 0.0023214281536638737\n",
            "Epoch: 442 | Loss: 0.002310285810381174\n",
            "Epoch: 443 | Loss: 0.0022991816513240337\n",
            "Epoch: 444 | Loss: 0.0022881394252181053\n",
            "Epoch: 445 | Loss: 0.002277158899232745\n",
            "Epoch: 446 | Loss: 0.002266207942739129\n",
            "Epoch: 447 | Loss: 0.0022553298622369766\n",
            "Epoch: 448 | Loss: 0.0022445032373070717\n",
            "Epoch: 449 | Loss: 0.002233723411336541\n",
            "Epoch: 450 | Loss: 0.002222997834905982\n",
            "Epoch: 451 | Loss: 0.0022123283706605434\n",
            "Epoch: 452 | Loss: 0.002201698487624526\n",
            "Epoch: 453 | Loss: 0.002191130304709077\n",
            "Epoch: 454 | Loss: 0.0021806098520755768\n",
            "Epoch: 455 | Loss: 0.002170123625546694\n",
            "Epoch: 456 | Loss: 0.0021597142331302166\n",
            "Epoch: 457 | Loss: 0.0021493379026651382\n",
            "Epoch: 458 | Loss: 0.0021390237379819155\n",
            "Epoch: 459 | Loss: 0.00212874379940331\n",
            "Epoch: 460 | Loss: 0.002118523931130767\n",
            "Epoch: 461 | Loss: 0.002108355052769184\n",
            "Epoch: 462 | Loss: 0.0020982290152460337\n",
            "Epoch: 463 | Loss: 0.0020881532691419125\n",
            "Epoch: 464 | Loss: 0.002078112680464983\n",
            "Epoch: 465 | Loss: 0.0020681528840214014\n",
            "Epoch: 466 | Loss: 0.002058215206488967\n",
            "Epoch: 467 | Loss: 0.0020483287516981363\n",
            "Epoch: 468 | Loss: 0.0020384935196489096\n",
            "Epoch: 469 | Loss: 0.0020287109073251486\n",
            "Epoch: 470 | Loss: 0.00201895902864635\n",
            "Epoch: 471 | Loss: 0.0020092669874429703\n",
            "Epoch: 472 | Loss: 0.001999617787078023\n",
            "Epoch: 473 | Loss: 0.001990014221519232\n",
            "Epoch: 474 | Loss: 0.0019804590847343206\n",
            "Epoch: 475 | Loss: 0.0019709598273038864\n",
            "Epoch: 476 | Loss: 0.0019614938646554947\n",
            "Epoch: 477 | Loss: 0.0019520726054906845\n",
            "Epoch: 478 | Loss: 0.0019427052466198802\n",
            "Epoch: 479 | Loss: 0.0019333651289343834\n",
            "Epoch: 480 | Loss: 0.0019240856636315584\n",
            "Epoch: 481 | Loss: 0.0019148432184010744\n",
            "Epoch: 482 | Loss: 0.0019056450109928846\n",
            "Epoch: 483 | Loss: 0.0018965037306770682\n",
            "Epoch: 484 | Loss: 0.0018873920198529959\n",
            "Epoch: 485 | Loss: 0.0018783225677907467\n",
            "Epoch: 486 | Loss: 0.001869311323389411\n",
            "Epoch: 487 | Loss: 0.0018603303469717503\n",
            "Epoch: 488 | Loss: 0.001851382665336132\n",
            "Epoch: 489 | Loss: 0.001842498080804944\n",
            "Epoch: 490 | Loss: 0.001833656569942832\n",
            "Epoch: 491 | Loss: 0.0018248393898829818\n",
            "Epoch: 492 | Loss: 0.0018160961335524917\n",
            "Epoch: 493 | Loss: 0.0018073581159114838\n",
            "Epoch: 494 | Loss: 0.001798677141778171\n",
            "Epoch: 495 | Loss: 0.0017900591483339667\n",
            "Epoch: 496 | Loss: 0.0017814537277445197\n",
            "Epoch: 497 | Loss: 0.0017728888196870685\n",
            "Epoch: 498 | Loss: 0.00176438398193568\n",
            "Epoch: 499 | Loss: 0.0017559122061356902\n",
            "Prediction (after training): 4 7.9159613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06_logistic_regression.py"
      ],
      "metadata": {
        "id": "JpunUVTmIUhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# dataset\n",
        "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = tensor([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    instantiate nn.Linear Module\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = nn.Linear(1, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "\n",
        "# out model\n",
        "model = Model()\n",
        "\n",
        "# loss function & optimizer contruction\n",
        "# call to model.parameters()\n",
        "criterion = nn.BCELoss(reduction = 'mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "\n",
        "#Traning loop\n",
        "for epoch in range(1000):\n",
        "  # forward pass : compute predicted y by passing x to the model\n",
        "  y_pred = model(x_data)\n",
        "\n",
        "  # compute and print loss\n",
        "  loss = criterion(y_pred, y_data)\n",
        "  print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
        "\n",
        "  #zero gradient, perform a backward pass, update the weights\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "#After training\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(tensor([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(tensor([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
      ],
      "metadata": {
        "id": "kyqRLp6dxuwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax\n"
      ],
      "metadata": {
        "id": "Twe-cP48R9hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit\n",
        "import optax\n",
        "\n",
        "x_data = jnp.array([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = jnp.array([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "# defind model\n",
        "def model(params, x): # 이 부분에서, init_params 에서 받은 값들을\n",
        "  return jax.nn.sigmoid(jnp.dot(x, params['weight']) + params['bias'])\n",
        "\n",
        "def loss(params, x, y):\n",
        "  y_pred = model(params, x)\n",
        "  return optax.sigmoid_binary_cross_entropy(y_pred, y).mean()\n",
        "\n",
        "# value and gradient computation\n",
        "\n",
        "\n",
        "# Initialize parameters\n",
        "key = jax.random.PRNGKey(0)  # Define random key\n",
        "init_params = {\n",
        "    'weight': jax.random.normal(key, (1, 1)),\n",
        "    'bias': jax.random.normal(key, (1,))\n",
        "}\n",
        "optimizer = optax.sgd(learning_rate = 0.01)\n",
        "# optax.sgd 를 활용해 optimizer 를 초기화 할때, params 가 jax 배열이 아니기때문에 오류가 발생\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "num_epochs = 1000\n",
        "params = init_params\n",
        "\n",
        "# @jit\n",
        "# def value_and_grad(params, x, y):\n",
        "#     return jax.value_and_grad(loss)(params, x, y)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # jit으로 선언한 함수를 사용할 떄\n",
        "  # loss_val, grads = value_and_grad(params,x_data, y_data)\n",
        "\n",
        "  loss_val, grads = jax.value_and_grad(loss)(params,x_data, y_data)\n",
        "  #print(params)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  #print(params)\n",
        "  print(f'Epoch {epoch + 1}/{num_epochs} | loss : {loss_val}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoffS53dIqqF",
        "outputId": "23b4487b-518a-452d-c263-e930b24ec90f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000 | loss : 0.7295204401016235\n",
            "Epoch 2/1000 | loss : 0.7294896841049194\n",
            "Epoch 3/1000 | loss : 0.7294589281082153\n",
            "Epoch 4/1000 | loss : 0.7294280529022217\n",
            "Epoch 5/1000 | loss : 0.729397177696228\n",
            "Epoch 6/1000 | loss : 0.7293662428855896\n",
            "Epoch 7/1000 | loss : 0.7293352484703064\n",
            "Epoch 8/1000 | loss : 0.7293041944503784\n",
            "Epoch 9/1000 | loss : 0.7292732000350952\n",
            "Epoch 10/1000 | loss : 0.7292420268058777\n",
            "Epoch 11/1000 | loss : 0.7292108535766602\n",
            "Epoch 12/1000 | loss : 0.7291796207427979\n",
            "Epoch 13/1000 | loss : 0.7291483879089355\n",
            "Epoch 14/1000 | loss : 0.7291171550750732\n",
            "Epoch 15/1000 | loss : 0.7290857434272766\n",
            "Epoch 16/1000 | loss : 0.7290543913841248\n",
            "Epoch 17/1000 | loss : 0.7290228605270386\n",
            "Epoch 18/1000 | loss : 0.7289913892745972\n",
            "Epoch 19/1000 | loss : 0.7289597988128662\n",
            "Epoch 20/1000 | loss : 0.7289282083511353\n",
            "Epoch 21/1000 | loss : 0.7288966178894043\n",
            "Epoch 22/1000 | loss : 0.7288649082183838\n",
            "Epoch 23/1000 | loss : 0.7288331985473633\n",
            "Epoch 24/1000 | loss : 0.7288013696670532\n",
            "Epoch 25/1000 | loss : 0.7287695407867432\n",
            "Epoch 26/1000 | loss : 0.7287375926971436\n",
            "Epoch 27/1000 | loss : 0.7287057042121887\n",
            "Epoch 28/1000 | loss : 0.7286736965179443\n",
            "Epoch 29/1000 | loss : 0.7286417484283447\n",
            "Epoch 30/1000 | loss : 0.7286096215248108\n",
            "Epoch 31/1000 | loss : 0.7285774946212769\n",
            "Epoch 32/1000 | loss : 0.7285453081130981\n",
            "Epoch 33/1000 | loss : 0.7285131216049194\n",
            "Epoch 34/1000 | loss : 0.7284808158874512\n",
            "Epoch 35/1000 | loss : 0.7284485101699829\n",
            "Epoch 36/1000 | loss : 0.7284161448478699\n",
            "Epoch 37/1000 | loss : 0.7283837795257568\n",
            "Epoch 38/1000 | loss : 0.728351354598999\n",
            "Epoch 39/1000 | loss : 0.7283188104629517\n",
            "Epoch 40/1000 | loss : 0.7282862663269043\n",
            "Epoch 41/1000 | loss : 0.7282536625862122\n",
            "Epoch 42/1000 | loss : 0.7282209992408752\n",
            "Epoch 43/1000 | loss : 0.7281882762908936\n",
            "Epoch 44/1000 | loss : 0.7281556129455566\n",
            "Epoch 45/1000 | loss : 0.7281227707862854\n",
            "Epoch 46/1000 | loss : 0.7280899286270142\n",
            "Epoch 47/1000 | loss : 0.7280570268630981\n",
            "Epoch 48/1000 | loss : 0.7280241250991821\n",
            "Epoch 49/1000 | loss : 0.7279911041259766\n",
            "Epoch 50/1000 | loss : 0.727958083152771\n",
            "Epoch 51/1000 | loss : 0.7279250025749207\n",
            "Epoch 52/1000 | loss : 0.7278919219970703\n",
            "Epoch 53/1000 | loss : 0.7278587222099304\n",
            "Epoch 54/1000 | loss : 0.7278255224227905\n",
            "Epoch 55/1000 | loss : 0.7277922034263611\n",
            "Epoch 56/1000 | loss : 0.7277589440345764\n",
            "Epoch 57/1000 | loss : 0.7277255654335022\n",
            "Epoch 58/1000 | loss : 0.7276921272277832\n",
            "Epoch 59/1000 | loss : 0.727658748626709\n",
            "Epoch 60/1000 | loss : 0.7276251912117004\n",
            "Epoch 61/1000 | loss : 0.7275916337966919\n",
            "Epoch 62/1000 | loss : 0.7275580167770386\n",
            "Epoch 63/1000 | loss : 0.7275243997573853\n",
            "Epoch 64/1000 | loss : 0.7274907231330872\n",
            "Epoch 65/1000 | loss : 0.7274569869041443\n",
            "Epoch 66/1000 | loss : 0.7274231910705566\n",
            "Epoch 67/1000 | loss : 0.7273893356323242\n",
            "Epoch 68/1000 | loss : 0.727355420589447\n",
            "Epoch 69/1000 | loss : 0.7273215055465698\n",
            "Epoch 70/1000 | loss : 0.7272875308990479\n",
            "Epoch 71/1000 | loss : 0.7272535562515259\n",
            "Epoch 72/1000 | loss : 0.7272194623947144\n",
            "Epoch 73/1000 | loss : 0.7271853685379028\n",
            "Epoch 74/1000 | loss : 0.7271511554718018\n",
            "Epoch 75/1000 | loss : 0.7271170020103455\n",
            "Epoch 76/1000 | loss : 0.7270827293395996\n",
            "Epoch 77/1000 | loss : 0.7270484566688538\n",
            "Epoch 78/1000 | loss : 0.7270140647888184\n",
            "Epoch 79/1000 | loss : 0.726979672908783\n",
            "Epoch 80/1000 | loss : 0.7269452214241028\n",
            "Epoch 81/1000 | loss : 0.7269107103347778\n",
            "Epoch 82/1000 | loss : 0.7268762588500977\n",
            "Epoch 83/1000 | loss : 0.7268416881561279\n",
            "Epoch 84/1000 | loss : 0.7268069982528687\n",
            "Epoch 85/1000 | loss : 0.7267723083496094\n",
            "Epoch 86/1000 | loss : 0.7267376184463501\n",
            "Epoch 87/1000 | loss : 0.726702868938446\n",
            "Epoch 88/1000 | loss : 0.726668119430542\n",
            "Epoch 89/1000 | loss : 0.7266331911087036\n",
            "Epoch 90/1000 | loss : 0.72659832239151\n",
            "Epoch 91/1000 | loss : 0.7265633344650269\n",
            "Epoch 92/1000 | loss : 0.7265284061431885\n",
            "Epoch 93/1000 | loss : 0.7264933586120605\n",
            "Epoch 94/1000 | loss : 0.7264582514762878\n",
            "Epoch 95/1000 | loss : 0.7264231443405151\n",
            "Epoch 96/1000 | loss : 0.7263879776000977\n",
            "Epoch 97/1000 | loss : 0.7263527512550354\n",
            "Epoch 98/1000 | loss : 0.7263175249099731\n",
            "Epoch 99/1000 | loss : 0.7262821793556213\n",
            "Epoch 100/1000 | loss : 0.7262468338012695\n",
            "Epoch 101/1000 | loss : 0.726211428642273\n",
            "Epoch 102/1000 | loss : 0.7261760234832764\n",
            "Epoch 103/1000 | loss : 0.7261404991149902\n",
            "Epoch 104/1000 | loss : 0.7261049747467041\n",
            "Epoch 105/1000 | loss : 0.726069450378418\n",
            "Epoch 106/1000 | loss : 0.7260338068008423\n",
            "Epoch 107/1000 | loss : 0.7259981632232666\n",
            "Epoch 108/1000 | loss : 0.7259624004364014\n",
            "Epoch 109/1000 | loss : 0.7259266376495361\n",
            "Epoch 110/1000 | loss : 0.7258908748626709\n",
            "Epoch 111/1000 | loss : 0.7258549928665161\n",
            "Epoch 112/1000 | loss : 0.7258191108703613\n",
            "Epoch 113/1000 | loss : 0.7257832288742065\n",
            "Epoch 114/1000 | loss : 0.7257472276687622\n",
            "Epoch 115/1000 | loss : 0.7257112264633179\n",
            "Epoch 116/1000 | loss : 0.7256752252578735\n",
            "Epoch 117/1000 | loss : 0.7256391048431396\n",
            "Epoch 118/1000 | loss : 0.725602924823761\n",
            "Epoch 119/1000 | loss : 0.7255667448043823\n",
            "Epoch 120/1000 | loss : 0.7255305051803589\n",
            "Epoch 121/1000 | loss : 0.7254942655563354\n",
            "Epoch 122/1000 | loss : 0.7254579663276672\n",
            "Epoch 123/1000 | loss : 0.7254216074943542\n",
            "Epoch 124/1000 | loss : 0.7253851890563965\n",
            "Epoch 125/1000 | loss : 0.725348711013794\n",
            "Epoch 126/1000 | loss : 0.7253122329711914\n",
            "Epoch 127/1000 | loss : 0.7252757549285889\n",
            "Epoch 128/1000 | loss : 0.7252391576766968\n",
            "Epoch 129/1000 | loss : 0.7252025604248047\n",
            "Epoch 130/1000 | loss : 0.7251659035682678\n",
            "Epoch 131/1000 | loss : 0.725129246711731\n",
            "Epoch 132/1000 | loss : 0.7250924110412598\n",
            "Epoch 133/1000 | loss : 0.7250556945800781\n",
            "Epoch 134/1000 | loss : 0.7250188589096069\n",
            "Epoch 135/1000 | loss : 0.7249820232391357\n",
            "Epoch 136/1000 | loss : 0.7249451279640198\n",
            "Epoch 137/1000 | loss : 0.724908173084259\n",
            "Epoch 138/1000 | loss : 0.7248711585998535\n",
            "Epoch 139/1000 | loss : 0.7248342037200928\n",
            "Epoch 140/1000 | loss : 0.7247971296310425\n",
            "Epoch 141/1000 | loss : 0.7247600555419922\n",
            "Epoch 142/1000 | loss : 0.7247228622436523\n",
            "Epoch 143/1000 | loss : 0.7246856689453125\n",
            "Epoch 144/1000 | loss : 0.7246484756469727\n",
            "Epoch 145/1000 | loss : 0.724611222743988\n",
            "Epoch 146/1000 | loss : 0.7245739102363586\n",
            "Epoch 147/1000 | loss : 0.7245365977287292\n",
            "Epoch 148/1000 | loss : 0.7244992256164551\n",
            "Epoch 149/1000 | loss : 0.7244617938995361\n",
            "Epoch 150/1000 | loss : 0.7244243621826172\n",
            "Epoch 151/1000 | loss : 0.7243868708610535\n",
            "Epoch 152/1000 | loss : 0.7243492603302002\n",
            "Epoch 153/1000 | loss : 0.7243117094039917\n",
            "Epoch 154/1000 | loss : 0.7242741584777832\n",
            "Epoch 155/1000 | loss : 0.7242364287376404\n",
            "Epoch 156/1000 | loss : 0.7241986989974976\n",
            "Epoch 157/1000 | loss : 0.7241610288619995\n",
            "Epoch 158/1000 | loss : 0.7241232395172119\n",
            "Epoch 159/1000 | loss : 0.7240855097770691\n",
            "Epoch 160/1000 | loss : 0.7240476608276367\n",
            "Epoch 161/1000 | loss : 0.7240097522735596\n",
            "Epoch 162/1000 | loss : 0.7239719033241272\n",
            "Epoch 163/1000 | loss : 0.7239339351654053\n",
            "Epoch 164/1000 | loss : 0.7238959670066833\n",
            "Epoch 165/1000 | loss : 0.7238578796386719\n",
            "Epoch 166/1000 | loss : 0.7238198518753052\n",
            "Epoch 167/1000 | loss : 0.7237817049026489\n",
            "Epoch 168/1000 | loss : 0.7237436771392822\n",
            "Epoch 169/1000 | loss : 0.7237054705619812\n",
            "Epoch 170/1000 | loss : 0.7236672639846802\n",
            "Epoch 171/1000 | loss : 0.7236290574073792\n",
            "Epoch 172/1000 | loss : 0.7235907912254333\n",
            "Epoch 173/1000 | loss : 0.7235524654388428\n",
            "Epoch 174/1000 | loss : 0.7235141396522522\n",
            "Epoch 175/1000 | loss : 0.7234757542610168\n",
            "Epoch 176/1000 | loss : 0.7234373092651367\n",
            "Epoch 177/1000 | loss : 0.7233989238739014\n",
            "Epoch 178/1000 | loss : 0.7233604192733765\n",
            "Epoch 179/1000 | loss : 0.7233219146728516\n",
            "Epoch 180/1000 | loss : 0.7232833504676819\n",
            "Epoch 181/1000 | loss : 0.7232447862625122\n",
            "Epoch 182/1000 | loss : 0.7232061624526978\n",
            "Epoch 183/1000 | loss : 0.7231675386428833\n",
            "Epoch 184/1000 | loss : 0.7231289148330688\n",
            "Epoch 185/1000 | loss : 0.7230901718139648\n",
            "Epoch 186/1000 | loss : 0.7230514287948608\n",
            "Epoch 187/1000 | loss : 0.7230126857757568\n",
            "Epoch 188/1000 | loss : 0.7229738235473633\n",
            "Epoch 189/1000 | loss : 0.7229349613189697\n",
            "Epoch 190/1000 | loss : 0.7228960990905762\n",
            "Epoch 191/1000 | loss : 0.7228572368621826\n",
            "Epoch 192/1000 | loss : 0.7228182554244995\n",
            "Epoch 193/1000 | loss : 0.7227792739868164\n",
            "Epoch 194/1000 | loss : 0.7227402925491333\n",
            "Epoch 195/1000 | loss : 0.7227013111114502\n",
            "Epoch 196/1000 | loss : 0.7226622104644775\n",
            "Epoch 197/1000 | loss : 0.7226231694221497\n",
            "Epoch 198/1000 | loss : 0.7225840091705322\n",
            "Epoch 199/1000 | loss : 0.7225448489189148\n",
            "Epoch 200/1000 | loss : 0.7225056886672974\n",
            "Epoch 201/1000 | loss : 0.7224664688110352\n",
            "Epoch 202/1000 | loss : 0.722427248954773\n",
            "Epoch 203/1000 | loss : 0.7223880290985107\n",
            "Epoch 204/1000 | loss : 0.722348690032959\n",
            "Epoch 205/1000 | loss : 0.722309410572052\n",
            "Epoch 206/1000 | loss : 0.7222700119018555\n",
            "Epoch 207/1000 | loss : 0.7222306728363037\n",
            "Epoch 208/1000 | loss : 0.7221912741661072\n",
            "Epoch 209/1000 | loss : 0.7221518158912659\n",
            "Epoch 210/1000 | loss : 0.7221123576164246\n",
            "Epoch 211/1000 | loss : 0.7220728993415833\n",
            "Epoch 212/1000 | loss : 0.7220333814620972\n",
            "Epoch 213/1000 | loss : 0.7219938635826111\n",
            "Epoch 214/1000 | loss : 0.7219542860984802\n",
            "Epoch 215/1000 | loss : 0.7219147086143494\n",
            "Epoch 216/1000 | loss : 0.7218750715255737\n",
            "Epoch 217/1000 | loss : 0.7218354344367981\n",
            "Epoch 218/1000 | loss : 0.7217957973480225\n",
            "Epoch 219/1000 | loss : 0.721756100654602\n",
            "Epoch 220/1000 | loss : 0.7217163443565369\n",
            "Epoch 221/1000 | loss : 0.7216765880584717\n",
            "Epoch 222/1000 | loss : 0.7216367721557617\n",
            "Epoch 223/1000 | loss : 0.7215970754623413\n",
            "Epoch 224/1000 | loss : 0.7215572595596313\n",
            "Epoch 225/1000 | loss : 0.7215173840522766\n",
            "Epoch 226/1000 | loss : 0.7214775085449219\n",
            "Epoch 227/1000 | loss : 0.7214376330375671\n",
            "Epoch 228/1000 | loss : 0.7213977575302124\n",
            "Epoch 229/1000 | loss : 0.7213578224182129\n",
            "Epoch 230/1000 | loss : 0.7213178873062134\n",
            "Epoch 231/1000 | loss : 0.7212779521942139\n",
            "Epoch 232/1000 | loss : 0.7212378978729248\n",
            "Epoch 233/1000 | loss : 0.7211978435516357\n",
            "Epoch 234/1000 | loss : 0.7211577892303467\n",
            "Epoch 235/1000 | loss : 0.7211177349090576\n",
            "Epoch 236/1000 | loss : 0.7210776805877686\n",
            "Epoch 237/1000 | loss : 0.7210375666618347\n",
            "Epoch 238/1000 | loss : 0.7209973931312561\n",
            "Epoch 239/1000 | loss : 0.7209572792053223\n",
            "Epoch 240/1000 | loss : 0.7209171056747437\n",
            "Epoch 241/1000 | loss : 0.720876932144165\n",
            "Epoch 242/1000 | loss : 0.7208367586135864\n",
            "Epoch 243/1000 | loss : 0.720796525478363\n",
            "Epoch 244/1000 | loss : 0.7207562923431396\n",
            "Epoch 245/1000 | loss : 0.7207159996032715\n",
            "Epoch 246/1000 | loss : 0.7206757068634033\n",
            "Epoch 247/1000 | loss : 0.7206354141235352\n",
            "Epoch 248/1000 | loss : 0.7205950617790222\n",
            "Epoch 249/1000 | loss : 0.720554769039154\n",
            "Epoch 250/1000 | loss : 0.7205144166946411\n",
            "Epoch 251/1000 | loss : 0.7204740643501282\n",
            "Epoch 252/1000 | loss : 0.7204337120056152\n",
            "Epoch 253/1000 | loss : 0.7203933000564575\n",
            "Epoch 254/1000 | loss : 0.7203528881072998\n",
            "Epoch 255/1000 | loss : 0.7203124165534973\n",
            "Epoch 256/1000 | loss : 0.7202719449996948\n",
            "Epoch 257/1000 | loss : 0.7202315330505371\n",
            "Epoch 258/1000 | loss : 0.7201911211013794\n",
            "Epoch 259/1000 | loss : 0.7201505899429321\n",
            "Epoch 260/1000 | loss : 0.7201100587844849\n",
            "Epoch 261/1000 | loss : 0.7200695276260376\n",
            "Epoch 262/1000 | loss : 0.7200289964675903\n",
            "Epoch 263/1000 | loss : 0.7199884653091431\n",
            "Epoch 264/1000 | loss : 0.7199479341506958\n",
            "Epoch 265/1000 | loss : 0.719907283782959\n",
            "Epoch 266/1000 | loss : 0.7198667526245117\n",
            "Epoch 267/1000 | loss : 0.7198261618614197\n",
            "Epoch 268/1000 | loss : 0.7197854518890381\n",
            "Epoch 269/1000 | loss : 0.7197449207305908\n",
            "Epoch 270/1000 | loss : 0.719704270362854\n",
            "Epoch 271/1000 | loss : 0.7196636199951172\n",
            "Epoch 272/1000 | loss : 0.7196229100227356\n",
            "Epoch 273/1000 | loss : 0.7195822596549988\n",
            "Epoch 274/1000 | loss : 0.719541609287262\n",
            "Epoch 275/1000 | loss : 0.7195008993148804\n",
            "Epoch 276/1000 | loss : 0.7194602489471436\n",
            "Epoch 277/1000 | loss : 0.719419538974762\n",
            "Epoch 278/1000 | loss : 0.7193787693977356\n",
            "Epoch 279/1000 | loss : 0.719338059425354\n",
            "Epoch 280/1000 | loss : 0.7192972898483276\n",
            "Epoch 281/1000 | loss : 0.719256579875946\n",
            "Epoch 282/1000 | loss : 0.7192158699035645\n",
            "Epoch 283/1000 | loss : 0.7191751003265381\n",
            "Epoch 284/1000 | loss : 0.7191342711448669\n",
            "Epoch 285/1000 | loss : 0.7190935611724854\n",
            "Epoch 286/1000 | loss : 0.7190527319908142\n",
            "Epoch 287/1000 | loss : 0.7190119028091431\n",
            "Epoch 288/1000 | loss : 0.7189711332321167\n",
            "Epoch 289/1000 | loss : 0.7189303040504456\n",
            "Epoch 290/1000 | loss : 0.7188894748687744\n",
            "Epoch 291/1000 | loss : 0.718848705291748\n",
            "Epoch 292/1000 | loss : 0.7188078761100769\n",
            "Epoch 293/1000 | loss : 0.7187670469284058\n",
            "Epoch 294/1000 | loss : 0.7187262177467346\n",
            "Epoch 295/1000 | loss : 0.7186854481697083\n",
            "Epoch 296/1000 | loss : 0.7186445593833923\n",
            "Epoch 297/1000 | loss : 0.7186037302017212\n",
            "Epoch 298/1000 | loss : 0.7185628414154053\n",
            "Epoch 299/1000 | loss : 0.7185220718383789\n",
            "Epoch 300/1000 | loss : 0.7184811234474182\n",
            "Epoch 301/1000 | loss : 0.7184402942657471\n",
            "Epoch 302/1000 | loss : 0.7183994054794312\n",
            "Epoch 303/1000 | loss : 0.7183585166931152\n",
            "Epoch 304/1000 | loss : 0.7183176875114441\n",
            "Epoch 305/1000 | loss : 0.718276858329773\n",
            "Epoch 306/1000 | loss : 0.718235969543457\n",
            "Epoch 307/1000 | loss : 0.7181950807571411\n",
            "Epoch 308/1000 | loss : 0.7181541919708252\n",
            "Epoch 309/1000 | loss : 0.718113362789154\n",
            "Epoch 310/1000 | loss : 0.7180724143981934\n",
            "Epoch 311/1000 | loss : 0.718031644821167\n",
            "Epoch 312/1000 | loss : 0.7179907560348511\n",
            "Epoch 313/1000 | loss : 0.7179498672485352\n",
            "Epoch 314/1000 | loss : 0.7179089784622192\n",
            "Epoch 315/1000 | loss : 0.7178681492805481\n",
            "Epoch 316/1000 | loss : 0.7178272604942322\n",
            "Epoch 317/1000 | loss : 0.7177863717079163\n",
            "Epoch 318/1000 | loss : 0.7177455425262451\n",
            "Epoch 319/1000 | loss : 0.717704713344574\n",
            "Epoch 320/1000 | loss : 0.7176638245582581\n",
            "Epoch 321/1000 | loss : 0.7176229953765869\n",
            "Epoch 322/1000 | loss : 0.717582106590271\n",
            "Epoch 323/1000 | loss : 0.7175412774085999\n",
            "Epoch 324/1000 | loss : 0.7175004482269287\n",
            "Epoch 325/1000 | loss : 0.7174595594406128\n",
            "Epoch 326/1000 | loss : 0.7174187898635864\n",
            "Epoch 327/1000 | loss : 0.7173779010772705\n",
            "Epoch 328/1000 | loss : 0.7173371315002441\n",
            "Epoch 329/1000 | loss : 0.717296302318573\n",
            "Epoch 330/1000 | loss : 0.7172554731369019\n",
            "Epoch 331/1000 | loss : 0.7172147035598755\n",
            "Epoch 332/1000 | loss : 0.7171738147735596\n",
            "Epoch 333/1000 | loss : 0.7171330451965332\n",
            "Epoch 334/1000 | loss : 0.7170922756195068\n",
            "Epoch 335/1000 | loss : 0.7170515060424805\n",
            "Epoch 336/1000 | loss : 0.7170107364654541\n",
            "Epoch 337/1000 | loss : 0.7169699668884277\n",
            "Epoch 338/1000 | loss : 0.7169291973114014\n",
            "Epoch 339/1000 | loss : 0.7168884873390198\n",
            "Epoch 340/1000 | loss : 0.7168477773666382\n",
            "Epoch 341/1000 | loss : 0.7168070077896118\n",
            "Epoch 342/1000 | loss : 0.7167662978172302\n",
            "Epoch 343/1000 | loss : 0.7167254686355591\n",
            "Epoch 344/1000 | loss : 0.7166848182678223\n",
            "Epoch 345/1000 | loss : 0.7166441679000854\n",
            "Epoch 346/1000 | loss : 0.7166034579277039\n",
            "Epoch 347/1000 | loss : 0.7165627479553223\n",
            "Epoch 348/1000 | loss : 0.7165220975875854\n",
            "Epoch 349/1000 | loss : 0.7164814472198486\n",
            "Epoch 350/1000 | loss : 0.7164407968521118\n",
            "Epoch 351/1000 | loss : 0.716400146484375\n",
            "Epoch 352/1000 | loss : 0.7163594961166382\n",
            "Epoch 353/1000 | loss : 0.7163189649581909\n",
            "Epoch 354/1000 | loss : 0.7162783145904541\n",
            "Epoch 355/1000 | loss : 0.7162377834320068\n",
            "Epoch 356/1000 | loss : 0.7161972522735596\n",
            "Epoch 357/1000 | loss : 0.7161566615104675\n",
            "Epoch 358/1000 | loss : 0.7161160707473755\n",
            "Epoch 359/1000 | loss : 0.7160755395889282\n",
            "Epoch 360/1000 | loss : 0.7160350680351257\n",
            "Epoch 361/1000 | loss : 0.7159945964813232\n",
            "Epoch 362/1000 | loss : 0.715954065322876\n",
            "Epoch 363/1000 | loss : 0.7159136533737183\n",
            "Epoch 364/1000 | loss : 0.7158731818199158\n",
            "Epoch 365/1000 | loss : 0.7158327102661133\n",
            "Epoch 366/1000 | loss : 0.7157922983169556\n",
            "Epoch 367/1000 | loss : 0.7157518863677979\n",
            "Epoch 368/1000 | loss : 0.7157114744186401\n",
            "Epoch 369/1000 | loss : 0.7156710624694824\n",
            "Epoch 370/1000 | loss : 0.7156307697296143\n",
            "Epoch 371/1000 | loss : 0.7155903577804565\n",
            "Epoch 372/1000 | loss : 0.7155500650405884\n",
            "Epoch 373/1000 | loss : 0.7155097723007202\n",
            "Epoch 374/1000 | loss : 0.715469479560852\n",
            "Epoch 375/1000 | loss : 0.7154291868209839\n",
            "Epoch 376/1000 | loss : 0.7153888940811157\n",
            "Epoch 377/1000 | loss : 0.7153487205505371\n",
            "Epoch 378/1000 | loss : 0.7153085470199585\n",
            "Epoch 379/1000 | loss : 0.7152682542800903\n",
            "Epoch 380/1000 | loss : 0.7152280807495117\n",
            "Epoch 381/1000 | loss : 0.7151879668235779\n",
            "Epoch 382/1000 | loss : 0.715147852897644\n",
            "Epoch 383/1000 | loss : 0.7151076793670654\n",
            "Epoch 384/1000 | loss : 0.7150676250457764\n",
            "Epoch 385/1000 | loss : 0.7150275111198425\n",
            "Epoch 386/1000 | loss : 0.7149874567985535\n",
            "Epoch 387/1000 | loss : 0.7149474024772644\n",
            "Epoch 388/1000 | loss : 0.7149074077606201\n",
            "Epoch 389/1000 | loss : 0.7148674130439758\n",
            "Epoch 390/1000 | loss : 0.7148274183273315\n",
            "Epoch 391/1000 | loss : 0.714787483215332\n",
            "Epoch 392/1000 | loss : 0.7147475481033325\n",
            "Epoch 393/1000 | loss : 0.714707612991333\n",
            "Epoch 394/1000 | loss : 0.714667797088623\n",
            "Epoch 395/1000 | loss : 0.7146278619766235\n",
            "Epoch 396/1000 | loss : 0.7145880460739136\n",
            "Epoch 397/1000 | loss : 0.7145482301712036\n",
            "Epoch 398/1000 | loss : 0.7145084142684937\n",
            "Epoch 399/1000 | loss : 0.7144687175750732\n",
            "Epoch 400/1000 | loss : 0.7144289612770081\n",
            "Epoch 401/1000 | loss : 0.7143892049789429\n",
            "Epoch 402/1000 | loss : 0.7143495082855225\n",
            "Epoch 403/1000 | loss : 0.7143098711967468\n",
            "Epoch 404/1000 | loss : 0.7142702341079712\n",
            "Epoch 405/1000 | loss : 0.7142305970191956\n",
            "Epoch 406/1000 | loss : 0.7141909599304199\n",
            "Epoch 407/1000 | loss : 0.7141514420509338\n",
            "Epoch 408/1000 | loss : 0.7141119241714478\n",
            "Epoch 409/1000 | loss : 0.7140724062919617\n",
            "Epoch 410/1000 | loss : 0.7140328884124756\n",
            "Epoch 411/1000 | loss : 0.7139934301376343\n",
            "Epoch 412/1000 | loss : 0.713953971862793\n",
            "Epoch 413/1000 | loss : 0.7139146327972412\n",
            "Epoch 414/1000 | loss : 0.7138751745223999\n",
            "Epoch 415/1000 | loss : 0.7138358354568481\n",
            "Epoch 416/1000 | loss : 0.7137964963912964\n",
            "Epoch 417/1000 | loss : 0.7137572169303894\n",
            "Epoch 418/1000 | loss : 0.7137179970741272\n",
            "Epoch 419/1000 | loss : 0.7136787176132202\n",
            "Epoch 420/1000 | loss : 0.713639497756958\n",
            "Epoch 421/1000 | loss : 0.7136002779006958\n",
            "Epoch 422/1000 | loss : 0.7135611772537231\n",
            "Epoch 423/1000 | loss : 0.7135220766067505\n",
            "Epoch 424/1000 | loss : 0.7134829759597778\n",
            "Epoch 425/1000 | loss : 0.7134438753128052\n",
            "Epoch 426/1000 | loss : 0.7134048938751221\n",
            "Epoch 427/1000 | loss : 0.7133658528327942\n",
            "Epoch 428/1000 | loss : 0.7133269309997559\n",
            "Epoch 429/1000 | loss : 0.7132879495620728\n",
            "Epoch 430/1000 | loss : 0.7132490277290344\n",
            "Epoch 431/1000 | loss : 0.7132101058959961\n",
            "Epoch 432/1000 | loss : 0.7131712436676025\n",
            "Epoch 433/1000 | loss : 0.7131325006484985\n",
            "Epoch 434/1000 | loss : 0.713093638420105\n",
            "Epoch 435/1000 | loss : 0.713054895401001\n",
            "Epoch 436/1000 | loss : 0.713016152381897\n",
            "Epoch 437/1000 | loss : 0.712977409362793\n",
            "Epoch 438/1000 | loss : 0.7129387855529785\n",
            "Epoch 439/1000 | loss : 0.7129002213478088\n",
            "Epoch 440/1000 | loss : 0.7128615975379944\n",
            "Epoch 441/1000 | loss : 0.7128230333328247\n",
            "Epoch 442/1000 | loss : 0.7127845287322998\n",
            "Epoch 443/1000 | loss : 0.7127460241317749\n",
            "Epoch 444/1000 | loss : 0.71270751953125\n",
            "Epoch 445/1000 | loss : 0.7126691341400146\n",
            "Epoch 446/1000 | loss : 0.7126306891441345\n",
            "Epoch 447/1000 | loss : 0.7125923037528992\n",
            "Epoch 448/1000 | loss : 0.7125539779663086\n",
            "Epoch 449/1000 | loss : 0.7125157117843628\n",
            "Epoch 450/1000 | loss : 0.712477445602417\n",
            "Epoch 451/1000 | loss : 0.7124391794204712\n",
            "Epoch 452/1000 | loss : 0.7124010324478149\n",
            "Epoch 453/1000 | loss : 0.7123628854751587\n",
            "Epoch 454/1000 | loss : 0.7123247385025024\n",
            "Epoch 455/1000 | loss : 0.7122865915298462\n",
            "Epoch 456/1000 | loss : 0.7122485637664795\n",
            "Epoch 457/1000 | loss : 0.7122105360031128\n",
            "Epoch 458/1000 | loss : 0.7121726274490356\n",
            "Epoch 459/1000 | loss : 0.7121346592903137\n",
            "Epoch 460/1000 | loss : 0.7120966911315918\n",
            "Epoch 461/1000 | loss : 0.7120587825775146\n",
            "Epoch 462/1000 | loss : 0.712020993232727\n",
            "Epoch 463/1000 | loss : 0.7119832038879395\n",
            "Epoch 464/1000 | loss : 0.7119454145431519\n",
            "Epoch 465/1000 | loss : 0.7119077444076538\n",
            "Epoch 466/1000 | loss : 0.711870014667511\n",
            "Epoch 467/1000 | loss : 0.7118322849273682\n",
            "Epoch 468/1000 | loss : 0.7117947340011597\n",
            "Epoch 469/1000 | loss : 0.7117571234703064\n",
            "Epoch 470/1000 | loss : 0.7117196321487427\n",
            "Epoch 471/1000 | loss : 0.7116820812225342\n",
            "Epoch 472/1000 | loss : 0.7116446495056152\n",
            "Epoch 473/1000 | loss : 0.7116072177886963\n",
            "Epoch 474/1000 | loss : 0.7115697860717773\n",
            "Epoch 475/1000 | loss : 0.711532473564148\n",
            "Epoch 476/1000 | loss : 0.7114951610565186\n",
            "Epoch 477/1000 | loss : 0.7114579081535339\n",
            "Epoch 478/1000 | loss : 0.7114206552505493\n",
            "Epoch 479/1000 | loss : 0.7113834619522095\n",
            "Epoch 480/1000 | loss : 0.7113462686538696\n",
            "Epoch 481/1000 | loss : 0.7113091349601746\n",
            "Epoch 482/1000 | loss : 0.7112720012664795\n",
            "Epoch 483/1000 | loss : 0.7112350463867188\n",
            "Epoch 484/1000 | loss : 0.7111979722976685\n",
            "Epoch 485/1000 | loss : 0.7111610174179077\n",
            "Epoch 486/1000 | loss : 0.711124062538147\n",
            "Epoch 487/1000 | loss : 0.7110872268676758\n",
            "Epoch 488/1000 | loss : 0.7110503911972046\n",
            "Epoch 489/1000 | loss : 0.7110135555267334\n",
            "Epoch 490/1000 | loss : 0.7109767198562622\n",
            "Epoch 491/1000 | loss : 0.7109401226043701\n",
            "Epoch 492/1000 | loss : 0.7109034061431885\n",
            "Epoch 493/1000 | loss : 0.7108666896820068\n",
            "Epoch 494/1000 | loss : 0.7108300924301147\n",
            "Epoch 495/1000 | loss : 0.7107936143875122\n",
            "Epoch 496/1000 | loss : 0.7107570171356201\n",
            "Epoch 497/1000 | loss : 0.7107205390930176\n",
            "Epoch 498/1000 | loss : 0.710684061050415\n",
            "Epoch 499/1000 | loss : 0.7106476426124573\n",
            "Epoch 500/1000 | loss : 0.7106113433837891\n",
            "Epoch 501/1000 | loss : 0.7105749845504761\n",
            "Epoch 502/1000 | loss : 0.7105387449264526\n",
            "Epoch 503/1000 | loss : 0.7105025053024292\n",
            "Epoch 504/1000 | loss : 0.7104662656784058\n",
            "Epoch 505/1000 | loss : 0.7104300856590271\n",
            "Epoch 506/1000 | loss : 0.7103939652442932\n",
            "Epoch 507/1000 | loss : 0.7103579640388489\n",
            "Epoch 508/1000 | loss : 0.7103219032287598\n",
            "Epoch 509/1000 | loss : 0.7102859020233154\n",
            "Epoch 510/1000 | loss : 0.7102499008178711\n",
            "Epoch 511/1000 | loss : 0.7102140188217163\n",
            "Epoch 512/1000 | loss : 0.7101781368255615\n",
            "Epoch 513/1000 | loss : 0.7101423740386963\n",
            "Epoch 514/1000 | loss : 0.7101065516471863\n",
            "Epoch 515/1000 | loss : 0.7100708484649658\n",
            "Epoch 516/1000 | loss : 0.7100351452827454\n",
            "Epoch 517/1000 | loss : 0.7099994421005249\n",
            "Epoch 518/1000 | loss : 0.709963858127594\n",
            "Epoch 519/1000 | loss : 0.7099282741546631\n",
            "Epoch 520/1000 | loss : 0.709892749786377\n",
            "Epoch 521/1000 | loss : 0.7098572850227356\n",
            "Epoch 522/1000 | loss : 0.7098218202590942\n",
            "Epoch 523/1000 | loss : 0.7097864151000977\n",
            "Epoch 524/1000 | loss : 0.7097511291503906\n",
            "Epoch 525/1000 | loss : 0.709715723991394\n",
            "Epoch 526/1000 | loss : 0.7096804976463318\n",
            "Epoch 527/1000 | loss : 0.7096452713012695\n",
            "Epoch 528/1000 | loss : 0.7096100449562073\n",
            "Epoch 529/1000 | loss : 0.7095749378204346\n",
            "Epoch 530/1000 | loss : 0.7095398902893066\n",
            "Epoch 531/1000 | loss : 0.7095047831535339\n",
            "Epoch 532/1000 | loss : 0.7094697952270508\n",
            "Epoch 533/1000 | loss : 0.7094348073005676\n",
            "Epoch 534/1000 | loss : 0.7093998193740845\n",
            "Epoch 535/1000 | loss : 0.7093650102615356\n",
            "Epoch 536/1000 | loss : 0.7093302011489868\n",
            "Epoch 537/1000 | loss : 0.709295392036438\n",
            "Epoch 538/1000 | loss : 0.7092605829238892\n",
            "Epoch 539/1000 | loss : 0.7092258930206299\n",
            "Epoch 540/1000 | loss : 0.7091912031173706\n",
            "Epoch 541/1000 | loss : 0.7091566324234009\n",
            "Epoch 542/1000 | loss : 0.7091220617294312\n",
            "Epoch 543/1000 | loss : 0.7090874910354614\n",
            "Epoch 544/1000 | loss : 0.7090529799461365\n",
            "Epoch 545/1000 | loss : 0.7090185880661011\n",
            "Epoch 546/1000 | loss : 0.7089841365814209\n",
            "Epoch 547/1000 | loss : 0.7089498043060303\n",
            "Epoch 548/1000 | loss : 0.7089154720306396\n",
            "Epoch 549/1000 | loss : 0.7088812589645386\n",
            "Epoch 550/1000 | loss : 0.7088469862937927\n",
            "Epoch 551/1000 | loss : 0.7088128328323364\n",
            "Epoch 552/1000 | loss : 0.7087786793708801\n",
            "Epoch 553/1000 | loss : 0.7087445855140686\n",
            "Epoch 554/1000 | loss : 0.7087105512619019\n",
            "Epoch 555/1000 | loss : 0.7086765170097351\n",
            "Epoch 556/1000 | loss : 0.7086426019668579\n",
            "Epoch 557/1000 | loss : 0.7086086273193359\n",
            "Epoch 558/1000 | loss : 0.7085747718811035\n",
            "Epoch 559/1000 | loss : 0.7085409760475159\n",
            "Epoch 560/1000 | loss : 0.7085071802139282\n",
            "Epoch 561/1000 | loss : 0.7084734439849854\n",
            "Epoch 562/1000 | loss : 0.7084397077560425\n",
            "Epoch 563/1000 | loss : 0.7084060907363892\n",
            "Epoch 564/1000 | loss : 0.7083724737167358\n",
            "Epoch 565/1000 | loss : 0.7083389163017273\n",
            "Epoch 566/1000 | loss : 0.7083053588867188\n",
            "Epoch 567/1000 | loss : 0.7082719206809998\n",
            "Epoch 568/1000 | loss : 0.7082384824752808\n",
            "Epoch 569/1000 | loss : 0.7082051038742065\n",
            "Epoch 570/1000 | loss : 0.7081717252731323\n",
            "Epoch 571/1000 | loss : 0.7081384658813477\n",
            "Epoch 572/1000 | loss : 0.708105206489563\n",
            "Epoch 573/1000 | loss : 0.7080720067024231\n",
            "Epoch 574/1000 | loss : 0.7080388069152832\n",
            "Epoch 575/1000 | loss : 0.7080057263374329\n",
            "Epoch 576/1000 | loss : 0.7079726457595825\n",
            "Epoch 577/1000 | loss : 0.707939624786377\n",
            "Epoch 578/1000 | loss : 0.7079066038131714\n",
            "Epoch 579/1000 | loss : 0.7078737020492554\n",
            "Epoch 580/1000 | loss : 0.7078408002853394\n",
            "Epoch 581/1000 | loss : 0.7078079581260681\n",
            "Epoch 582/1000 | loss : 0.7077751159667969\n",
            "Epoch 583/1000 | loss : 0.7077423930168152\n",
            "Epoch 584/1000 | loss : 0.7077096104621887\n",
            "Epoch 585/1000 | loss : 0.7076770067214966\n",
            "Epoch 586/1000 | loss : 0.7076443433761597\n",
            "Epoch 587/1000 | loss : 0.7076116800308228\n",
            "Epoch 588/1000 | loss : 0.7075791954994202\n",
            "Epoch 589/1000 | loss : 0.7075467109680176\n",
            "Epoch 590/1000 | loss : 0.7075142860412598\n",
            "Epoch 591/1000 | loss : 0.707481861114502\n",
            "Epoch 592/1000 | loss : 0.7074494361877441\n",
            "Epoch 593/1000 | loss : 0.7074171304702759\n",
            "Epoch 594/1000 | loss : 0.7073848843574524\n",
            "Epoch 595/1000 | loss : 0.7073526382446289\n",
            "Epoch 596/1000 | loss : 0.7073204517364502\n",
            "Epoch 597/1000 | loss : 0.7072882652282715\n",
            "Epoch 598/1000 | loss : 0.7072561979293823\n",
            "Epoch 599/1000 | loss : 0.7072241306304932\n",
            "Epoch 600/1000 | loss : 0.7071921229362488\n",
            "Epoch 601/1000 | loss : 0.7071601748466492\n",
            "Epoch 602/1000 | loss : 0.7071282267570496\n",
            "Epoch 603/1000 | loss : 0.7070963382720947\n",
            "Epoch 604/1000 | loss : 0.7070645093917847\n",
            "Epoch 605/1000 | loss : 0.7070327997207642\n",
            "Epoch 606/1000 | loss : 0.7070009708404541\n",
            "Epoch 607/1000 | loss : 0.7069692611694336\n",
            "Epoch 608/1000 | loss : 0.7069375514984131\n",
            "Epoch 609/1000 | loss : 0.7069059610366821\n",
            "Epoch 610/1000 | loss : 0.7068743705749512\n",
            "Epoch 611/1000 | loss : 0.7068428993225098\n",
            "Epoch 612/1000 | loss : 0.7068114280700684\n",
            "Epoch 613/1000 | loss : 0.706779956817627\n",
            "Epoch 614/1000 | loss : 0.7067486047744751\n",
            "Epoch 615/1000 | loss : 0.7067171335220337\n",
            "Epoch 616/1000 | loss : 0.7066858410835266\n",
            "Epoch 617/1000 | loss : 0.7066545486450195\n",
            "Epoch 618/1000 | loss : 0.706623375415802\n",
            "Epoch 619/1000 | loss : 0.7065922021865845\n",
            "Epoch 620/1000 | loss : 0.7065610289573669\n",
            "Epoch 621/1000 | loss : 0.706529974937439\n",
            "Epoch 622/1000 | loss : 0.706498920917511\n",
            "Epoch 623/1000 | loss : 0.706467866897583\n",
            "Epoch 624/1000 | loss : 0.7064369320869446\n",
            "Epoch 625/1000 | loss : 0.7064059972763062\n",
            "Epoch 626/1000 | loss : 0.7063751220703125\n",
            "Epoch 627/1000 | loss : 0.7063443064689636\n",
            "Epoch 628/1000 | loss : 0.7063134908676147\n",
            "Epoch 629/1000 | loss : 0.7062827348709106\n",
            "Epoch 630/1000 | loss : 0.7062520384788513\n",
            "Epoch 631/1000 | loss : 0.706221342086792\n",
            "Epoch 632/1000 | loss : 0.7061907649040222\n",
            "Epoch 633/1000 | loss : 0.7061601877212524\n",
            "Epoch 634/1000 | loss : 0.7061296701431274\n",
            "Epoch 635/1000 | loss : 0.7060991525650024\n",
            "Epoch 636/1000 | loss : 0.7060686945915222\n",
            "Epoch 637/1000 | loss : 0.7060383558273315\n",
            "Epoch 638/1000 | loss : 0.7060079574584961\n",
            "Epoch 639/1000 | loss : 0.7059776186943054\n",
            "Epoch 640/1000 | loss : 0.7059473395347595\n",
            "Epoch 641/1000 | loss : 0.7059171199798584\n",
            "Epoch 642/1000 | loss : 0.7058868408203125\n",
            "Epoch 643/1000 | loss : 0.7058566808700562\n",
            "Epoch 644/1000 | loss : 0.7058265209197998\n",
            "Epoch 645/1000 | loss : 0.705796480178833\n",
            "Epoch 646/1000 | loss : 0.705766499042511\n",
            "Epoch 647/1000 | loss : 0.705736517906189\n",
            "Epoch 648/1000 | loss : 0.7057065367698669\n",
            "Epoch 649/1000 | loss : 0.7056766748428345\n",
            "Epoch 650/1000 | loss : 0.7056468725204468\n",
            "Epoch 651/1000 | loss : 0.7056169509887695\n",
            "Epoch 652/1000 | loss : 0.7055872082710266\n",
            "Epoch 653/1000 | loss : 0.7055574655532837\n",
            "Epoch 654/1000 | loss : 0.7055277824401855\n",
            "Epoch 655/1000 | loss : 0.7054980993270874\n",
            "Epoch 656/1000 | loss : 0.7054685354232788\n",
            "Epoch 657/1000 | loss : 0.7054389119148254\n",
            "Epoch 658/1000 | loss : 0.7054094076156616\n",
            "Epoch 659/1000 | loss : 0.7053799629211426\n",
            "Epoch 660/1000 | loss : 0.7053505182266235\n",
            "Epoch 661/1000 | loss : 0.7053210735321045\n",
            "Epoch 662/1000 | loss : 0.7052916884422302\n",
            "Epoch 663/1000 | loss : 0.7052623629570007\n",
            "Epoch 664/1000 | loss : 0.705233097076416\n",
            "Epoch 665/1000 | loss : 0.7052038908004761\n",
            "Epoch 666/1000 | loss : 0.7051746249198914\n",
            "Epoch 667/1000 | loss : 0.7051454782485962\n",
            "Epoch 668/1000 | loss : 0.705116331577301\n",
            "Epoch 669/1000 | loss : 0.7050872445106506\n",
            "Epoch 670/1000 | loss : 0.7050582766532898\n",
            "Epoch 671/1000 | loss : 0.7050292491912842\n",
            "Epoch 672/1000 | loss : 0.7050002813339233\n",
            "Epoch 673/1000 | loss : 0.7049713134765625\n",
            "Epoch 674/1000 | loss : 0.7049424648284912\n",
            "Epoch 675/1000 | loss : 0.7049136161804199\n",
            "Epoch 676/1000 | loss : 0.7048848271369934\n",
            "Epoch 677/1000 | loss : 0.7048560976982117\n",
            "Epoch 678/1000 | loss : 0.7048273086547852\n",
            "Epoch 679/1000 | loss : 0.704798698425293\n",
            "Epoch 680/1000 | loss : 0.704770028591156\n",
            "Epoch 681/1000 | loss : 0.7047414779663086\n",
            "Epoch 682/1000 | loss : 0.7047128677368164\n",
            "Epoch 683/1000 | loss : 0.7046843767166138\n",
            "Epoch 684/1000 | loss : 0.7046558856964111\n",
            "Epoch 685/1000 | loss : 0.7046273946762085\n",
            "Epoch 686/1000 | loss : 0.7045990228652954\n",
            "Epoch 687/1000 | loss : 0.7045706510543823\n",
            "Epoch 688/1000 | loss : 0.7045422792434692\n",
            "Epoch 689/1000 | loss : 0.7045140266418457\n",
            "Epoch 690/1000 | loss : 0.7044857740402222\n",
            "Epoch 691/1000 | loss : 0.7044575810432434\n",
            "Epoch 692/1000 | loss : 0.7044293880462646\n",
            "Epoch 693/1000 | loss : 0.7044012546539307\n",
            "Epoch 694/1000 | loss : 0.7043732404708862\n",
            "Epoch 695/1000 | loss : 0.704345166683197\n",
            "Epoch 696/1000 | loss : 0.7043170928955078\n",
            "Epoch 697/1000 | loss : 0.7042890787124634\n",
            "Epoch 698/1000 | loss : 0.7042611837387085\n",
            "Epoch 699/1000 | loss : 0.7042332887649536\n",
            "Epoch 700/1000 | loss : 0.7042054533958435\n",
            "Epoch 701/1000 | loss : 0.7041776180267334\n",
            "Epoch 702/1000 | loss : 0.7041498422622681\n",
            "Epoch 703/1000 | loss : 0.7041220664978027\n",
            "Epoch 704/1000 | loss : 0.7040943503379822\n",
            "Epoch 705/1000 | loss : 0.7040666937828064\n",
            "Epoch 706/1000 | loss : 0.7040390968322754\n",
            "Epoch 707/1000 | loss : 0.7040114998817444\n",
            "Epoch 708/1000 | loss : 0.7039839029312134\n",
            "Epoch 709/1000 | loss : 0.7039563655853271\n",
            "Epoch 710/1000 | loss : 0.7039288282394409\n",
            "Epoch 711/1000 | loss : 0.703901469707489\n",
            "Epoch 712/1000 | loss : 0.7038739919662476\n",
            "Epoch 713/1000 | loss : 0.7038466930389404\n",
            "Epoch 714/1000 | loss : 0.7038192749023438\n",
            "Epoch 715/1000 | loss : 0.7037919759750366\n",
            "Epoch 716/1000 | loss : 0.7037646770477295\n",
            "Epoch 717/1000 | loss : 0.7037374973297119\n",
            "Epoch 718/1000 | loss : 0.7037103176116943\n",
            "Epoch 719/1000 | loss : 0.7036831378936768\n",
            "Epoch 720/1000 | loss : 0.7036560773849487\n",
            "Epoch 721/1000 | loss : 0.7036288976669312\n",
            "Epoch 722/1000 | loss : 0.7036018967628479\n",
            "Epoch 723/1000 | loss : 0.7035748958587646\n",
            "Epoch 724/1000 | loss : 0.7035478949546814\n",
            "Epoch 725/1000 | loss : 0.7035208940505981\n",
            "Epoch 726/1000 | loss : 0.7034940719604492\n",
            "Epoch 727/1000 | loss : 0.7034672498703003\n",
            "Epoch 728/1000 | loss : 0.7034403681755066\n",
            "Epoch 729/1000 | loss : 0.7034135460853577\n",
            "Epoch 730/1000 | loss : 0.7033867835998535\n",
            "Epoch 731/1000 | loss : 0.7033600807189941\n",
            "Epoch 732/1000 | loss : 0.7033333778381348\n",
            "Epoch 733/1000 | loss : 0.7033066749572754\n",
            "Epoch 734/1000 | loss : 0.7032800912857056\n",
            "Epoch 735/1000 | loss : 0.7032535076141357\n",
            "Epoch 736/1000 | loss : 0.7032269239425659\n",
            "Epoch 737/1000 | loss : 0.7032004594802856\n",
            "Epoch 738/1000 | loss : 0.7031739950180054\n",
            "Epoch 739/1000 | loss : 0.7031475305557251\n",
            "Epoch 740/1000 | loss : 0.7031211256980896\n",
            "Epoch 741/1000 | loss : 0.7030947208404541\n",
            "Epoch 742/1000 | loss : 0.7030683755874634\n",
            "Epoch 743/1000 | loss : 0.7030420899391174\n",
            "Epoch 744/1000 | loss : 0.7030158042907715\n",
            "Epoch 745/1000 | loss : 0.7029895782470703\n",
            "Epoch 746/1000 | loss : 0.7029633522033691\n",
            "Epoch 747/1000 | loss : 0.702937126159668\n",
            "Epoch 748/1000 | loss : 0.7029110193252563\n",
            "Epoch 749/1000 | loss : 0.7028849124908447\n",
            "Epoch 750/1000 | loss : 0.7028589248657227\n",
            "Epoch 751/1000 | loss : 0.702832818031311\n",
            "Epoch 752/1000 | loss : 0.702806830406189\n",
            "Epoch 753/1000 | loss : 0.7027808427810669\n",
            "Epoch 754/1000 | loss : 0.7027549147605896\n",
            "Epoch 755/1000 | loss : 0.7027289867401123\n",
            "Epoch 756/1000 | loss : 0.7027031183242798\n",
            "Epoch 757/1000 | loss : 0.7026772499084473\n",
            "Epoch 758/1000 | loss : 0.7026515007019043\n",
            "Epoch 759/1000 | loss : 0.7026257514953613\n",
            "Epoch 760/1000 | loss : 0.7026000022888184\n",
            "Epoch 761/1000 | loss : 0.7025742530822754\n",
            "Epoch 762/1000 | loss : 0.7025485634803772\n",
            "Epoch 763/1000 | loss : 0.702522873878479\n",
            "Epoch 764/1000 | loss : 0.7024973630905151\n",
            "Epoch 765/1000 | loss : 0.7024717330932617\n",
            "Epoch 766/1000 | loss : 0.7024462223052979\n",
            "Epoch 767/1000 | loss : 0.702420711517334\n",
            "Epoch 768/1000 | loss : 0.7023952007293701\n",
            "Epoch 769/1000 | loss : 0.702369749546051\n",
            "Epoch 770/1000 | loss : 0.7023442983627319\n",
            "Epoch 771/1000 | loss : 0.7023189067840576\n",
            "Epoch 772/1000 | loss : 0.7022935152053833\n",
            "Epoch 773/1000 | loss : 0.7022682428359985\n",
            "Epoch 774/1000 | loss : 0.7022429704666138\n",
            "Epoch 775/1000 | loss : 0.702217698097229\n",
            "Epoch 776/1000 | loss : 0.7021924257278442\n",
            "Epoch 777/1000 | loss : 0.702167272567749\n",
            "Epoch 778/1000 | loss : 0.7021421194076538\n",
            "Epoch 779/1000 | loss : 0.7021169662475586\n",
            "Epoch 780/1000 | loss : 0.7020918726921082\n",
            "Epoch 781/1000 | loss : 0.7020667791366577\n",
            "Epoch 782/1000 | loss : 0.702041745185852\n",
            "Epoch 783/1000 | loss : 0.7020167112350464\n",
            "Epoch 784/1000 | loss : 0.7019917368888855\n",
            "Epoch 785/1000 | loss : 0.7019667625427246\n",
            "Epoch 786/1000 | loss : 0.7019417881965637\n",
            "Epoch 787/1000 | loss : 0.7019169330596924\n",
            "Epoch 788/1000 | loss : 0.7018921375274658\n",
            "Epoch 789/1000 | loss : 0.7018672227859497\n",
            "Epoch 790/1000 | loss : 0.7018424272537231\n",
            "Epoch 791/1000 | loss : 0.7018176913261414\n",
            "Epoch 792/1000 | loss : 0.7017929553985596\n",
            "Epoch 793/1000 | loss : 0.7017682194709778\n",
            "Epoch 794/1000 | loss : 0.7017435431480408\n",
            "Epoch 795/1000 | loss : 0.7017188668251038\n",
            "Epoch 796/1000 | loss : 0.7016942501068115\n",
            "Epoch 797/1000 | loss : 0.7016696929931641\n",
            "Epoch 798/1000 | loss : 0.7016450762748718\n",
            "Epoch 799/1000 | loss : 0.7016205787658691\n",
            "Epoch 800/1000 | loss : 0.7015960216522217\n",
            "Epoch 801/1000 | loss : 0.701571524143219\n",
            "Epoch 802/1000 | loss : 0.7015470862388611\n",
            "Epoch 803/1000 | loss : 0.7015226483345032\n",
            "Epoch 804/1000 | loss : 0.70149827003479\n",
            "Epoch 805/1000 | loss : 0.7014738917350769\n",
            "Epoch 806/1000 | loss : 0.7014496326446533\n",
            "Epoch 807/1000 | loss : 0.7014252543449402\n",
            "Epoch 808/1000 | loss : 0.7014009952545166\n",
            "Epoch 809/1000 | loss : 0.701376736164093\n",
            "Epoch 810/1000 | loss : 0.7013524770736694\n",
            "Epoch 811/1000 | loss : 0.7013283371925354\n",
            "Epoch 812/1000 | loss : 0.7013041377067566\n",
            "Epoch 813/1000 | loss : 0.7012799978256226\n",
            "Epoch 814/1000 | loss : 0.7012559175491333\n",
            "Epoch 815/1000 | loss : 0.7012317776679993\n",
            "Epoch 816/1000 | loss : 0.7012077569961548\n",
            "Epoch 817/1000 | loss : 0.7011836767196655\n",
            "Epoch 818/1000 | loss : 0.7011597156524658\n",
            "Epoch 819/1000 | loss : 0.7011357545852661\n",
            "Epoch 820/1000 | loss : 0.7011117935180664\n",
            "Epoch 821/1000 | loss : 0.7010878324508667\n",
            "Epoch 822/1000 | loss : 0.7010639905929565\n",
            "Epoch 823/1000 | loss : 0.7010400295257568\n",
            "Epoch 824/1000 | loss : 0.7010162472724915\n",
            "Epoch 825/1000 | loss : 0.7009924054145813\n",
            "Epoch 826/1000 | loss : 0.7009686231613159\n",
            "Epoch 827/1000 | loss : 0.7009449005126953\n",
            "Epoch 828/1000 | loss : 0.7009211778640747\n",
            "Epoch 829/1000 | loss : 0.7008974552154541\n",
            "Epoch 830/1000 | loss : 0.7008737325668335\n",
            "Epoch 831/1000 | loss : 0.7008501291275024\n",
            "Epoch 832/1000 | loss : 0.7008264660835266\n",
            "Epoch 833/1000 | loss : 0.7008028030395508\n",
            "Epoch 834/1000 | loss : 0.7007793188095093\n",
            "Epoch 835/1000 | loss : 0.7007557153701782\n",
            "Epoch 836/1000 | loss : 0.7007322311401367\n",
            "Epoch 837/1000 | loss : 0.7007086873054504\n",
            "Epoch 838/1000 | loss : 0.7006852626800537\n",
            "Epoch 839/1000 | loss : 0.7006617784500122\n",
            "Epoch 840/1000 | loss : 0.7006382942199707\n",
            "Epoch 841/1000 | loss : 0.7006149291992188\n",
            "Epoch 842/1000 | loss : 0.7005915641784668\n",
            "Epoch 843/1000 | loss : 0.7005681991577148\n",
            "Epoch 844/1000 | loss : 0.7005448937416077\n",
            "Epoch 845/1000 | loss : 0.7005215883255005\n",
            "Epoch 846/1000 | loss : 0.7004983425140381\n",
            "Epoch 847/1000 | loss : 0.7004751563072205\n",
            "Epoch 848/1000 | loss : 0.7004518508911133\n",
            "Epoch 849/1000 | loss : 0.7004286050796509\n",
            "Epoch 850/1000 | loss : 0.700405478477478\n",
            "Epoch 851/1000 | loss : 0.7003823518753052\n",
            "Epoch 852/1000 | loss : 0.7003591656684875\n",
            "Epoch 853/1000 | loss : 0.7003360986709595\n",
            "Epoch 854/1000 | loss : 0.7003130316734314\n",
            "Epoch 855/1000 | loss : 0.7002899646759033\n",
            "Epoch 856/1000 | loss : 0.70026695728302\n",
            "Epoch 857/1000 | loss : 0.7002439498901367\n",
            "Epoch 858/1000 | loss : 0.7002209424972534\n",
            "Epoch 859/1000 | loss : 0.7001979947090149\n",
            "Epoch 860/1000 | loss : 0.7001750469207764\n",
            "Epoch 861/1000 | loss : 0.7001521587371826\n",
            "Epoch 862/1000 | loss : 0.7001292705535889\n",
            "Epoch 863/1000 | loss : 0.7001063823699951\n",
            "Epoch 864/1000 | loss : 0.7000836133956909\n",
            "Epoch 865/1000 | loss : 0.7000607252120972\n",
            "Epoch 866/1000 | loss : 0.700037956237793\n",
            "Epoch 867/1000 | loss : 0.700015127658844\n",
            "Epoch 868/1000 | loss : 0.6999924182891846\n",
            "Epoch 869/1000 | loss : 0.6999697089195251\n",
            "Epoch 870/1000 | loss : 0.6999469995498657\n",
            "Epoch 871/1000 | loss : 0.6999242901802063\n",
            "Epoch 872/1000 | loss : 0.6999017000198364\n",
            "Epoch 873/1000 | loss : 0.6998790502548218\n",
            "Epoch 874/1000 | loss : 0.6998564004898071\n",
            "Epoch 875/1000 | loss : 0.6998338103294373\n",
            "Epoch 876/1000 | loss : 0.6998112797737122\n",
            "Epoch 877/1000 | loss : 0.6997886896133423\n",
            "Epoch 878/1000 | loss : 0.6997661590576172\n",
            "Epoch 879/1000 | loss : 0.6997437477111816\n",
            "Epoch 880/1000 | loss : 0.6997212171554565\n",
            "Epoch 881/1000 | loss : 0.6996987462043762\n",
            "Epoch 882/1000 | loss : 0.6996763348579407\n",
            "Epoch 883/1000 | loss : 0.6996539831161499\n",
            "Epoch 884/1000 | loss : 0.6996315121650696\n",
            "Epoch 885/1000 | loss : 0.6996091604232788\n",
            "Epoch 886/1000 | loss : 0.699586808681488\n",
            "Epoch 887/1000 | loss : 0.6995644569396973\n",
            "Epoch 888/1000 | loss : 0.6995421648025513\n",
            "Epoch 889/1000 | loss : 0.6995198726654053\n",
            "Epoch 890/1000 | loss : 0.6994975805282593\n",
            "Epoch 891/1000 | loss : 0.6994754076004028\n",
            "Epoch 892/1000 | loss : 0.6994531750679016\n",
            "Epoch 893/1000 | loss : 0.6994309425354004\n",
            "Epoch 894/1000 | loss : 0.699408769607544\n",
            "Epoch 895/1000 | loss : 0.6993865966796875\n",
            "Epoch 896/1000 | loss : 0.6993645429611206\n",
            "Epoch 897/1000 | loss : 0.6993423700332642\n",
            "Epoch 898/1000 | loss : 0.6993203163146973\n",
            "Epoch 899/1000 | loss : 0.6992982625961304\n",
            "Epoch 900/1000 | loss : 0.6992761492729187\n",
            "Epoch 901/1000 | loss : 0.6992541551589966\n",
            "Epoch 902/1000 | loss : 0.6992321014404297\n",
            "Epoch 903/1000 | loss : 0.6992101669311523\n",
            "Epoch 904/1000 | loss : 0.699188232421875\n",
            "Epoch 905/1000 | loss : 0.6991661787033081\n",
            "Epoch 906/1000 | loss : 0.6991442441940308\n",
            "Epoch 907/1000 | loss : 0.6991223096847534\n",
            "Epoch 908/1000 | loss : 0.6991004943847656\n",
            "Epoch 909/1000 | loss : 0.6990786194801331\n",
            "Epoch 910/1000 | loss : 0.6990567445755005\n",
            "Epoch 911/1000 | loss : 0.6990349292755127\n",
            "Epoch 912/1000 | loss : 0.6990131139755249\n",
            "Epoch 913/1000 | loss : 0.6989912986755371\n",
            "Epoch 914/1000 | loss : 0.6989695429801941\n",
            "Epoch 915/1000 | loss : 0.6989477872848511\n",
            "Epoch 916/1000 | loss : 0.6989260315895081\n",
            "Epoch 917/1000 | loss : 0.6989043951034546\n",
            "Epoch 918/1000 | loss : 0.6988825798034668\n",
            "Epoch 919/1000 | loss : 0.6988610029220581\n",
            "Epoch 920/1000 | loss : 0.6988393068313599\n",
            "Epoch 921/1000 | loss : 0.6988177299499512\n",
            "Epoch 922/1000 | loss : 0.6987960338592529\n",
            "Epoch 923/1000 | loss : 0.6987744569778442\n",
            "Epoch 924/1000 | loss : 0.6987528800964355\n",
            "Epoch 925/1000 | loss : 0.6987313032150269\n",
            "Epoch 926/1000 | loss : 0.6987097263336182\n",
            "Epoch 927/1000 | loss : 0.6986882090568542\n",
            "Epoch 928/1000 | loss : 0.6986666917800903\n",
            "Epoch 929/1000 | loss : 0.6986452341079712\n",
            "Epoch 930/1000 | loss : 0.698623776435852\n",
            "Epoch 931/1000 | loss : 0.6986023187637329\n",
            "Epoch 932/1000 | loss : 0.6985808610916138\n",
            "Epoch 933/1000 | loss : 0.6985594034194946\n",
            "Epoch 934/1000 | loss : 0.698538064956665\n",
            "Epoch 935/1000 | loss : 0.6985166668891907\n",
            "Epoch 936/1000 | loss : 0.6984953284263611\n",
            "Epoch 937/1000 | loss : 0.6984740495681763\n",
            "Epoch 938/1000 | loss : 0.6984525918960571\n",
            "Epoch 939/1000 | loss : 0.6984313130378723\n",
            "Epoch 940/1000 | loss : 0.6984100341796875\n",
            "Epoch 941/1000 | loss : 0.6983888149261475\n",
            "Epoch 942/1000 | loss : 0.6983675360679626\n",
            "Epoch 943/1000 | loss : 0.6983462572097778\n",
            "Epoch 944/1000 | loss : 0.6983250379562378\n",
            "Epoch 945/1000 | loss : 0.6983038783073425\n",
            "Epoch 946/1000 | loss : 0.6982827186584473\n",
            "Epoch 947/1000 | loss : 0.6982614994049072\n",
            "Epoch 948/1000 | loss : 0.698240339756012\n",
            "Epoch 949/1000 | loss : 0.6982191801071167\n",
            "Epoch 950/1000 | loss : 0.6981980800628662\n",
            "Epoch 951/1000 | loss : 0.6981769800186157\n",
            "Epoch 952/1000 | loss : 0.6981558799743652\n",
            "Epoch 953/1000 | loss : 0.6981347799301147\n",
            "Epoch 954/1000 | loss : 0.6981137990951538\n",
            "Epoch 955/1000 | loss : 0.6980927586555481\n",
            "Epoch 956/1000 | loss : 0.6980717182159424\n",
            "Epoch 957/1000 | loss : 0.6980507373809814\n",
            "Epoch 958/1000 | loss : 0.6980297565460205\n",
            "Epoch 959/1000 | loss : 0.6980087757110596\n",
            "Epoch 960/1000 | loss : 0.6979877948760986\n",
            "Epoch 961/1000 | loss : 0.6979668736457825\n",
            "Epoch 962/1000 | loss : 0.6979459524154663\n",
            "Epoch 963/1000 | loss : 0.6979250907897949\n",
            "Epoch 964/1000 | loss : 0.6979041695594788\n",
            "Epoch 965/1000 | loss : 0.6978832483291626\n",
            "Epoch 966/1000 | loss : 0.697862446308136\n",
            "Epoch 967/1000 | loss : 0.6978415846824646\n",
            "Epoch 968/1000 | loss : 0.697820782661438\n",
            "Epoch 969/1000 | loss : 0.6977999210357666\n",
            "Epoch 970/1000 | loss : 0.6977791786193848\n",
            "Epoch 971/1000 | loss : 0.6977584362030029\n",
            "Epoch 972/1000 | loss : 0.6977376341819763\n",
            "Epoch 973/1000 | loss : 0.6977168321609497\n",
            "Epoch 974/1000 | loss : 0.6976961493492126\n",
            "Epoch 975/1000 | loss : 0.6976754665374756\n",
            "Epoch 976/1000 | loss : 0.6976547241210938\n",
            "Epoch 977/1000 | loss : 0.6976340413093567\n",
            "Epoch 978/1000 | loss : 0.6976134181022644\n",
            "Epoch 979/1000 | loss : 0.6975927352905273\n",
            "Epoch 980/1000 | loss : 0.6975721120834351\n",
            "Epoch 981/1000 | loss : 0.6975514888763428\n",
            "Epoch 982/1000 | loss : 0.6975308656692505\n",
            "Epoch 983/1000 | loss : 0.6975102424621582\n",
            "Epoch 984/1000 | loss : 0.6974896788597107\n",
            "Epoch 985/1000 | loss : 0.6974691152572632\n",
            "Epoch 986/1000 | loss : 0.6974486112594604\n",
            "Epoch 987/1000 | loss : 0.6974280476570129\n",
            "Epoch 988/1000 | loss : 0.6974075436592102\n",
            "Epoch 989/1000 | loss : 0.6973870396614075\n",
            "Epoch 990/1000 | loss : 0.6973665356636047\n",
            "Epoch 991/1000 | loss : 0.6973460912704468\n",
            "Epoch 992/1000 | loss : 0.697325587272644\n",
            "Epoch 993/1000 | loss : 0.6973051428794861\n",
            "Epoch 994/1000 | loss : 0.6972846984863281\n",
            "Epoch 995/1000 | loss : 0.6972643136978149\n",
            "Epoch 996/1000 | loss : 0.6972439289093018\n",
            "Epoch 997/1000 | loss : 0.6972234845161438\n",
            "Epoch 998/1000 | loss : 0.6972030997276306\n",
            "Epoch 999/1000 | loss : 0.6971827149391174\n",
            "Epoch 1000/1000 | loss : 0.697162389755249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07_diabets_logistic.py"
      ],
      "metadata": {
        "id": "N_gPZ2TogL4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "\n",
        "xy = np.loadtxt('/content/drive/MyDrive/google_jax/data/diabetes.csv.gz', delimiter=',', dtype = np.float32)\n",
        "x_data = from_numpy(xy[:, 0: -1])\n",
        "y_data = from_numpy(xy[:, [-1]])\n",
        "\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mATnN4tuc9TC",
        "outputId": "1bcd4435-4cd9-4d6b-f950-639d0daaf298"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(model, self)"
      ],
      "metadata": {
        "id": "wKfT99yfhcjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7AdREOKhiKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "pytorch_to_jax",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}