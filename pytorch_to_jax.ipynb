{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinseriouspark/pytorch_with_jax/blob/main/pytorch_to_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform pytorch to jax\n",
        "\n",
        "- 활용자료 : https://github.com/hunkim/PyTorchZeroToAll"
      ],
      "metadata": {
        "id": "ynp9olWoHBz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hunkim/PyTorchZeroToAll.git"
      ],
      "metadata": {
        "id": "VZeCuLmERFmh",
        "outputId": "111a708b-6fe1-4d7d-fa0b-acdc4d382c46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PyTorchZeroToAll'...\n",
            "remote: Enumerating objects: 598, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 598 (delta 3), reused 4 (delta 0), pack-reused 589\u001b[K\n",
            "Receiving objects: 100% (598/598), 52.77 MiB | 19.15 MiB/s, done.\n",
            "Resolving deltas: 100% (389/389), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CosN6ukkgYbf",
        "outputId": "adc41fbb-b108-43da-ffd4-fb32fe5bd7ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform pytorch to jax"
      ],
      "metadata": {
        "id": "WPcCYoxuG22g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/sample_data/california_housing_train.csv', nrows = 100)\n",
        "feature_col = 'median_income'\n",
        "target_col = 'median_house_value'"
      ],
      "metadata": {
        "id": "FLUkG_f6h6Zs"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01_basic.py"
      ],
      "metadata": {
        "id": "tzo4-YL3qfAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_basic.py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#x_data = data[feature_col].values\n",
        "#y_data = data[target_col].values\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) **2\n",
        "\n",
        "# list of weights/mean square Error (MSE) for each input\n",
        "w_list = []\n",
        "mse_list = []\n",
        "\n",
        "for w in np.arange(0.0, 4.0, 1.0):\n",
        "  l_sum = 0\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    y_pred_val = forward(x_val)\n",
        "    l = loss(x_val, y_val)\n",
        "    l_sum += l\n",
        "\n",
        "    print('\\t', x_val, y_val, y_pred_val, l)\n",
        "  print('MSE=', l_sum/ len(x_data)) # 직접 평균 계산\n",
        "  w_list.append(w)\n",
        "  mse_list.append(l_sum / len(x_data))\n",
        "# Plot it all\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wokSxfHZHOlU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "outputId": "f47169e3-8948-4101-861b-a6aef3022766"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01_basic with jax"
      ],
      "metadata": {
        "id": "O12vMT2Fqhur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_basic with jax\n",
        "## jax.numpy as jnp , from jax import grad, jnp.mean() 등을 사용\n",
        "\n",
        "import datetime\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "# 데이터 정의\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "# forward pass\n",
        "def forward(x, w):\n",
        "  return x * w\n",
        "# loss\n",
        "def loss(w, x, y):\n",
        "  y_pred = forward(x, w)\n",
        "  return jnp.mean((y_pred - y) **2)\n",
        "\n",
        "# grad를 계산하는 함수 생성\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "# 초기 w 값 설정\n",
        "w = 1.0\n",
        "\n",
        "# w업데이트하면서 손실감소\n",
        "num_epochs = 100\n",
        "losses = []\n",
        "lr = 0.01\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  grad_w = grad_loss(w, x_data, y_data)\n",
        "  w -= lr * grad_w\n",
        "  loss_val = loss(w, x_data, y_data)\n",
        "  losses.append(loss_val)\n",
        "  if epoch % 10 == 0:\n",
        "      print(f'Epoch {epoch + 1}, Loss {loss_val}')\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9lydl-tGHAY1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "32c46b21-d01c-4b75-ebf8-8fdd9932ddc0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 3.836207151412964\n",
            "Epoch 11, Loss 0.5405738353729248\n",
            "Epoch 21, Loss 0.0761740654706955\n",
            "Epoch 31, Loss 0.01073399931192398\n",
            "Epoch 41, Loss 0.0015125819481909275\n",
            "Epoch 51, Loss 0.00021314274636097252\n",
            "Epoch 61, Loss 3.003445999638643e-05\n",
            "Epoch 71, Loss 4.233250365359709e-06\n",
            "Epoch 81, Loss 5.96372842665005e-07\n",
            "Epoch 91, Loss 8.396483508477104e-08\n",
            "0:00:02.927140\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02_manual_gradient.py"
      ],
      "metadata": {
        "id": "6bateGiDivo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = 1.0\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y) # 오차제곱\n",
        "\n",
        "# compute gradient\n",
        "def gradient(x, y):\n",
        "  return 2 * x * (x * w - y)\n",
        "\n",
        "# before training\n",
        "print('prediction (before training)', 4, forward(4))\n",
        "\n",
        "# training loop\n",
        "for epoch in range(10):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad = gradient(x_val, y_val)\n",
        "    w = w - 0.01 * grad\n",
        "    print('\\tgrad: ', x_val, y_val, round(grad, 2))\n",
        "    l = loss(x_val, y_val) # 오차 제곱을 데이터셋 별로 더하여 계산\n",
        "  print('progress:', epoch, 'w=', round(w, 2), 'loss=', round(1, 2))\n",
        "\n",
        "#after training\n",
        "print('predicted score (after training)', 4, forward(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi2OAKCRlrco",
        "outputId": "55d128cd-425b-4df4-9ec2-41b8b5b65e6e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.84\n",
            "\tgrad:  3.0 6.0 -16.23\n",
            "progress: 0 w= 1.26 loss= 1\n",
            "\tgrad:  1.0 2.0 -1.48\n",
            "\tgrad:  2.0 4.0 -5.8\n",
            "\tgrad:  3.0 6.0 -12.0\n",
            "progress: 1 w= 1.45 loss= 1\n",
            "\tgrad:  1.0 2.0 -1.09\n",
            "\tgrad:  2.0 4.0 -4.29\n",
            "\tgrad:  3.0 6.0 -8.87\n",
            "progress: 2 w= 1.6 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.81\n",
            "\tgrad:  2.0 4.0 -3.17\n",
            "\tgrad:  3.0 6.0 -6.56\n",
            "progress: 3 w= 1.7 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.6\n",
            "\tgrad:  2.0 4.0 -2.34\n",
            "\tgrad:  3.0 6.0 -4.85\n",
            "progress: 4 w= 1.78 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.44\n",
            "\tgrad:  2.0 4.0 -1.73\n",
            "\tgrad:  3.0 6.0 -3.58\n",
            "progress: 5 w= 1.84 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.33\n",
            "\tgrad:  2.0 4.0 -1.28\n",
            "\tgrad:  3.0 6.0 -2.65\n",
            "progress: 6 w= 1.88 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.24\n",
            "\tgrad:  2.0 4.0 -0.95\n",
            "\tgrad:  3.0 6.0 -1.96\n",
            "progress: 7 w= 1.91 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.18\n",
            "\tgrad:  2.0 4.0 -0.7\n",
            "\tgrad:  3.0 6.0 -1.45\n",
            "progress: 8 w= 1.93 loss= 1\n",
            "\tgrad:  1.0 2.0 -0.13\n",
            "\tgrad:  2.0 4.0 -0.52\n",
            "\tgrad:  3.0 6.0 -1.07\n",
            "progress: 9 w= 1.95 loss= 1\n",
            "predicted score (after training) 4 7.804863933862125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "MVEsUBeOqkZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "w = 1.0\n",
        "\n",
        "def forward(x, w):\n",
        "  return x * w\n",
        "\n",
        "def loss(w, x, y):\n",
        "  y_pred = forward(x, w)\n",
        "  return jnp.mean((y_pred - y) ** 2)\n",
        "\n",
        "# gradient 계산,loss 함수를 입력으로 받아들여 그래디언트 계산하는 함수로 반환\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "print('prediction (before_training)', forward(4, w))\n",
        "\n",
        "# training\n",
        "learning_rate= 0.01\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad_w = grad_loss(w, x_val, y_val)\n",
        "    w -= learning_rate * grad_w\n",
        "    print('\\tgrad:', x_val, y_val, round(grad_w, 2))\n",
        "    loss_val = loss(w, x_val, y_val)\n",
        "  print('progress:', epoch, 'w=', round(w, 2), 'loss=', round(loss_val, 2))\n",
        "\n",
        "# 학습 후 예측\n",
        "print('predicted score (after training):', forward(4, w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXDrwd8rnDUU",
        "outputId": "d5b8da8f-43ac-4ab7-9285-d0747570f06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before_training) 4.0\n",
            "\tgrad: 1.0 2.0 -2.0\n",
            "\tgrad: 2.0 4.0 -7.8399997\n",
            "\tgrad: 3.0 6.0 -16.23\n",
            "progress: 0 w= 1.26 loss= 4.92\n",
            "\tgrad: 1.0 2.0 -1.48\n",
            "\tgrad: 2.0 4.0 -5.7999997\n",
            "\tgrad: 3.0 6.0 -12.0\n",
            "progress: 1 w= 1.4499999 loss= 2.69\n",
            "\tgrad: 1.0 2.0 -1.09\n",
            "\tgrad: 2.0 4.0 -4.29\n",
            "\tgrad: 3.0 6.0 -8.87\n",
            "progress: 2 w= 1.5999999 loss= 1.4699999\n",
            "\tgrad: 1.0 2.0 -0.81\n",
            "\tgrad: 2.0 4.0 -3.1699998\n",
            "\tgrad: 3.0 6.0 -6.56\n",
            "progress: 3 w= 1.6999999 loss= 0.79999995\n",
            "\tgrad: 1.0 2.0 -0.59999996\n",
            "\tgrad: 2.0 4.0 -2.34\n",
            "\tgrad: 3.0 6.0 -4.85\n",
            "progress: 4 w= 1.78 loss= 0.44\n",
            "\tgrad: 1.0 2.0 -0.44\n",
            "\tgrad: 2.0 4.0 -1.73\n",
            "\tgrad: 3.0 6.0 -3.58\n",
            "progress: 5 w= 1.8399999 loss= 0.24\n",
            "\tgrad: 1.0 2.0 -0.32999998\n",
            "\tgrad: 2.0 4.0 -1.28\n",
            "\tgrad: 3.0 6.0 -2.6499999\n",
            "progress: 6 w= 1.88 loss= 0.13\n",
            "\tgrad: 1.0 2.0 -0.24\n",
            "\tgrad: 2.0 4.0 -0.95\n",
            "\tgrad: 3.0 6.0 -1.9599999\n",
            "progress: 7 w= 1.91 loss= 0.07\n",
            "\tgrad: 1.0 2.0 -0.17999999\n",
            "\tgrad: 2.0 4.0 -0.7\n",
            "\tgrad: 3.0 6.0 -1.4499999\n",
            "progress: 8 w= 1.93 loss= 0.04\n",
            "\tgrad: 1.0 2.0 -0.13\n",
            "\tgrad: 2.0 4.0 -0.52\n",
            "\tgrad: 3.0 6.0 -1.0699999\n",
            "progress: 9 w= 1.9499999 loss= 0.02\n",
            "predicted score (after training): 7.8048644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 manual gradient 계산할 때는 loss 계산에서 틀린 것이 없었다."
      ],
      "metadata": {
        "id": "kPHee6J6i1Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_w, loss_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zjYLusNpLwt",
        "outputId": "4e7aaf54-043c-45c0-c742-7b49d6d2e8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(-1.0708666, dtype=float32, weak_type=True),\n",
              " DeviceArray(0.02141885, dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03_auto_gradient.py"
      ],
      "metadata": {
        "id": "UDkcoyF_qV4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "LSZ-pAkqlLHJ",
        "outputId": "1fa36e20-45cd-4592-dcf6-46eb5653b2e0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-99d4765682f1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pdb\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "w = torch.tensor([1.0], requires_grad = True)\n",
        "\n",
        "# forward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# loss function\n",
        "def loss(y_pred, y_val):\n",
        "  return (y_pred - y_val) **2\n",
        "\n",
        "# before training\n",
        "print('prediction (before training)', 4, forward(4).item())\n",
        "\n",
        "# training loop\n",
        "for epoch in range(1):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    y_pred = forward(x_val) # 1) forward pass\n",
        "    l = loss(y_pred, y_val) # 2) compute loss\n",
        "    print(f'{y_pred},{y_val} loss : {l.item()}')\n",
        "    l.backward() # 3) backpropagation to update weights\n",
        "    #print('\\t grad:', x_val, y_val, w.grad.item())\n",
        "    w.data = w.data - 0.01 * w.grad.item()\n",
        "\n",
        "    w.grad.data.zero_()\n",
        "  print(f'Epoch: {epoch} | loss : {l.item()}')\n",
        "\n",
        "# after training\n",
        "print('prediction (after training)', 4, forward(4).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqTUzucjpVbX",
        "outputId": "f4439ef6-5d74-4ce0-f7db-986b9ab34ce3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "tensor([1.], grad_fn=<MulBackward0>),2.0 loss : 1.0\n",
            "tensor([2.0400], grad_fn=<MulBackward0>),4.0 loss : 3.841600179672241\n",
            "tensor([3.2952], grad_fn=<MulBackward0>),6.0 loss : 7.315943717956543\n",
            "Epoch: 0 | loss : 7.315943717956543\n",
            "prediction (after training) 4 5.042752265930176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss(2.04, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnthykkpriNw",
        "outputId": "26879f86-80f5-4a78-e7b6-c3b2db54cc5a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.8415999999999997"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax\n",
        "- torch 구현보다 예측력이 떨어진다!?"
      ],
      "metadata": {
        "id": "kSwVz9EdqnPf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdiTnoPKro1K",
        "outputId": "4f64b494-5849-4060-f00c-d68209c9c66f"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred : 1.0, y_true:2.0\n",
            "torch loss: 1.0, w grad : -2.0\n",
            "jax loss: 1.0, y grad: -2.0\n",
            "jax loss: 1.0, param grad: [-2.]\n",
            "----------\n",
            "y_pred : 2.0, y_true:4.0\n",
            "torch loss: 4.0, w grad : -8.0\n",
            "jax loss: 4.0, y grad: -4.0\n",
            "jax loss: 4.0, param grad: [-8.]\n",
            "----------\n",
            "y_pred : 3.0, y_true:6.0\n",
            "torch loss: 9.0, w grad : -18.0\n",
            "jax loss: 9.0, y grad: -6.0\n",
            "jax loss: 9.0, param grad: [-18.]\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수정 전\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "w = jnp.array([1.0]) # 당연하게 requires_grad = True 는 없다\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "def loss(y_pred, y_val):\n",
        "  return (y_pred - y_val)**2\n",
        "\n",
        "learning_rate = 0.01\n",
        "grad_loss = grad(loss)\n",
        "\n",
        "print('prediction (before training)', 4, forward(4)[0])\n",
        "\n",
        "for epoch in range(20):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    # y_pred = forward(x_val) # 1) forward pass\n",
        "    #l = loss(y_pred, y_val) # 2) compute loss\n",
        "    #l.backward() # 3) backpropagation to update weights\n",
        "    #print('\\t grad:', x_val, y_val, w.grad.item())\n",
        "    y_pred = forward(x_val)\n",
        "    grad_w = grad_loss(y_pred[0],y_val)\n",
        "    w -= learning_rate * grad_w\n",
        "    print('\\tgrad: ', x_val, y_val, grad_w)\n",
        "\n",
        "  print(f'Epoch: {epoch} | Loss: {loss(forward(x_data), y_data)}')\n",
        "\n",
        "print('prediction (after training)', 4, forward(4)[0], '-> 잘못된 grad() 사용으로 인한 결과물 오류 ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTRvGqLtqm7X",
        "outputId": "294d841d-8ea6-41d7-992a-e4106a171a1b"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -3.92\n",
            "\tgrad:  3.0 6.0 -5.6448\n",
            "Epoch: 0 | Loss: [0.7820786 3.1283145 7.038707 ]\n",
            "\tgrad:  1.0 2.0 -1.7687042\n",
            "\tgrad:  2.0 4.0 -3.46666\n",
            "\tgrad:  3.0 6.0 -4.99199\n",
            "Epoch: 1 | Loss: [0.6116468 2.446587  5.5048213]\n",
            "\tgrad:  1.0 2.0 -1.564157\n",
            "\tgrad:  2.0 4.0 -3.0657477\n",
            "\tgrad:  3.0 6.0 -4.4146767\n",
            "Epoch: 2 | Loss: [0.4783557 1.9134228 4.3052006]\n",
            "\tgrad:  1.0 2.0 -1.3832653\n",
            "\tgrad:  2.0 4.0 -2.7111998\n",
            "\tgrad:  3.0 6.0 -3.9041271\n",
            "Epoch: 3 | Loss: [0.37411162 1.4964465  3.3670046 ]\n",
            "\tgrad:  1.0 2.0 -1.2232933\n",
            "\tgrad:  2.0 4.0 -2.397655\n",
            "\tgrad:  3.0 6.0 -3.4526234\n",
            "Epoch: 4 | Loss: [0.29258466 1.1703386  2.6332629 ]\n",
            "\tgrad:  1.0 2.0 -1.0818219\n",
            "\tgrad:  2.0 4.0 -2.1203709\n",
            "\tgrad:  3.0 6.0 -3.0533333\n",
            "Epoch: 5 | Loss: [0.22882412 0.9152965  2.0594177 ]\n",
            "\tgrad:  1.0 2.0 -0.9567113\n",
            "\tgrad:  2.0 4.0 -1.875154\n",
            "\tgrad:  3.0 6.0 -2.700222\n",
            "Epoch: 6 | Loss: [0.17895843 0.7158337  1.6106262 ]\n",
            "\tgrad:  1.0 2.0 -0.8460696\n",
            "\tgrad:  2.0 4.0 -1.6582966\n",
            "\tgrad:  3.0 6.0 -2.387947\n",
            "Epoch: 7 | Loss: [0.13995953 0.5598381  1.2596358 ]\n",
            "\tgrad:  1.0 2.0 -0.7482233\n",
            "\tgrad:  2.0 4.0 -1.4665174\n",
            "\tgrad:  3.0 6.0 -2.111786\n",
            "Epoch: 8 | Loss: [0.10945936 0.43783745 0.9851345 ]\n",
            "\tgrad:  1.0 2.0 -0.66169286\n",
            "\tgrad:  2.0 4.0 -1.2969179\n",
            "\tgrad:  3.0 6.0 -1.8675623\n",
            "Epoch: 9 | Loss: [0.08560585 0.3424234  0.77045244]\n",
            "\tgrad:  1.0 2.0 -0.58516955\n",
            "\tgrad:  2.0 4.0 -1.1469321\n",
            "\tgrad:  3.0 6.0 -1.6515818\n",
            "Epoch: 10 | Loss: [0.06695043 0.26780173 0.60255355]\n",
            "\tgrad:  1.0 2.0 -0.51749563\n",
            "\tgrad:  2.0 4.0 -1.0142913\n",
            "\tgrad:  3.0 6.0 -1.4605789\n",
            "Epoch: 11 | Loss: [0.05236049 0.20944194 0.47124436]\n",
            "\tgrad:  1.0 2.0 -0.45764828\n",
            "\tgrad:  2.0 4.0 -0.8969908\n",
            "\tgrad:  3.0 6.0 -1.291667\n",
            "Epoch: 12 | Loss: [0.04095002 0.16380008 0.36855015]\n",
            "\tgrad:  1.0 2.0 -0.4047222\n",
            "\tgrad:  2.0 4.0 -0.7932553\n",
            "\tgrad:  3.0 6.0 -1.1422882\n",
            "Epoch: 13 | Loss: [0.03202612 0.12810446 0.28823504]\n",
            "\tgrad:  1.0 2.0 -0.35791683\n",
            "\tgrad:  2.0 4.0 -0.7015171\n",
            "\tgrad:  3.0 6.0 -1.0101843\n",
            "Epoch: 14 | Loss: [0.02504694 0.10018776 0.22542247]\n",
            "\tgrad:  1.0 2.0 -0.3165245\n",
            "\tgrad:  2.0 4.0 -0.62038803\n",
            "\tgrad:  3.0 6.0 -0.89335823\n",
            "Epoch: 15 | Loss: [0.01958868 0.07835473 0.17629834]\n",
            "\tgrad:  1.0 2.0 -0.27991915\n",
            "\tgrad:  2.0 4.0 -0.5486417\n",
            "\tgrad:  3.0 6.0 -0.7900448\n",
            "Epoch: 16 | Loss: [0.0153199  0.06127959 0.13787907]\n",
            "\tgrad:  1.0 2.0 -0.24754715\n",
            "\tgrad:  2.0 4.0 -0.4851923\n",
            "\tgrad:  3.0 6.0 -0.69867706\n",
            "Epoch: 17 | Loss: [0.01198136 0.04792544 0.10783225]\n",
            "\tgrad:  1.0 2.0 -0.2189188\n",
            "\tgrad:  2.0 4.0 -0.42908096\n",
            "\tgrad:  3.0 6.0 -0.61787605\n",
            "Epoch: 18 | Loss: [0.00937037 0.03748149 0.08433329]\n",
            "\tgrad:  1.0 2.0 -0.19360137\n",
            "\tgrad:  2.0 4.0 -0.3794589\n",
            "\tgrad:  3.0 6.0 -0.54642105\n",
            "Epoch: 19 | Loss: [0.00732838 0.02931353 0.06595539]\n",
            "prediction (after training) 4 7.657576 -> 잘못된 grad() 사용으로 인한 결과물 오류 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 오류 해결:  torch w.grad.data.zeros_() 와 jax.grad(, argnums = (0,1)) 를 비교"
      ],
      "metadata": {
        "id": "5liyTR9e2FIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch, jax 간 같은 loss 함수를 사용함에도 불구하고 사용법에 따라 문제가 발생하는 이유\n",
        "# grad 를 계산하는 방식이 다름\n",
        "## torch\n",
        "# l = loss(y_pred, y_true)\n",
        "# l.backward()\n",
        "\n",
        "## jax\n",
        "# grad_loss=grad(loss)\n",
        "# grad_loss(y_pred, y_true)\n",
        "\n",
        "\n",
        "w_torch = torch.tensor([1.0], requires_grad = True)\n",
        "w_jax = jnp.array([1.0]) # 당연하게 requires_grad = True 는 없다\n",
        "def forward_torch(x):\n",
        "    return x * w_torch\n",
        "def forward_jax(w_jax, x):\n",
        "    return x * w_jax\n",
        "\n",
        "def loss(y_pred, y_true):\n",
        "    return (y_pred - y_true) **2\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "    return jnp.mean((forward_jax(params, x) - y) **2)\n",
        "\n",
        "\n",
        "x_data_torch = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\n",
        "y_data_torch = torch.tensor([2.0, 4.0, 6.0], requires_grad=False)\n",
        "\n",
        "x_data_jax = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data_jax = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "for x_val_t, y_val_t, x_val_j, y_val_j in zip(x_data_torch, y_data_torch, x_data_jax, y_data_jax):\n",
        "\n",
        "    # torch\n",
        "    y_pred_torch = forward_torch(x_val_t)\n",
        "    l = loss(y_pred_torch, y_val_t)\n",
        "    l.backward()\n",
        "    print(f'y_pred : {y_pred_torch.item()}, y_true:{y_val_t}')\n",
        "    print(f'torch loss: {l.item()}, w grad : {w_torch.grad.item()}') # w에 대한 gradient 였음\n",
        "    w_torch.grad.data.zero_()\n",
        "\n",
        "    # jax\n",
        "    y_pred_jax = forward_jax(w_jax, x_val_j)\n",
        "   #print(f'y_pred : {y_pred_jax[0]}, y_true:{y_val}')\n",
        "    jax_loss = loss(y_pred_jax[0], y_val_j)\n",
        "    grad_ = grad_loss(y_pred_jax[0], y_val_j) # 이런 방식으로 gradient 를 구한다면, y에 대한 gradient 가 출력됨\n",
        "    print(f'jax loss: {jax_loss}, y grad: {grad_}')\n",
        "\n",
        "    grad2 = jax.grad(loss_fn, argnums=(0, 1))(w_jax, x_val_j, y_val_j)\n",
        "    # 위와 같은 방식으로 해야만 제대로 weight 에 대한 gradient 를 계산할 수 있음\n",
        "    print(f'jax loss: {jax_loss}, param grad: {grad2[0]}')\n",
        "    print('-' * 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGg1kVUlml2y",
        "outputId": "ebadc42b-1245-4120-8b0f-c3b5278cd26e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(7.657576, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수정 후\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "x_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_data = jnp.array([2.0, 4.0, 6.0])\n",
        "\n",
        "w = jnp.array([1.0]) # 당연하게 requires_grad = True 는 없다\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "def loss(y_pred, y_val):\n",
        "  return (y_pred - y_val)**2\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "print('prediction (before training)', 4, forward(4)[0])\n",
        "\n",
        "for epoch in range(20):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    # y_pred = forward(x_val) # 1) forward pass\n",
        "    #l = loss(y_pred, y_val) # 2) compute loss\n",
        "    #l.backward() # 3) backpropagation to update weights\n",
        "    #print('\\t grad:', x_val, y_val, w.grad.item())\n",
        "    y_pred = forward(x_val)\n",
        "    grad_w = jax.grad(loss_fn, argnums=(0, 1))(w, x_val, y_val)\n",
        "    w -= learning_rate * grad_w[0]\n",
        "    print('\\tgrad: ', x_val, y_val, grad_w[0])\n",
        "\n",
        "  print(f'Epoch: {epoch} | Loss: {loss(forward(x_data), y_data)}')\n",
        "\n",
        "print('prediction (after training)', 4, forward(4)[0], '->제대로 학습한 결과물 ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKoR3NMTmnRz",
        "outputId": "ef88176d-14bc-49c9-d6a0-332d3b7e3771"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 [-2.]\n",
            "\tgrad:  2.0 4.0 [-7.84]\n",
            "\tgrad:  3.0 6.0 [-16.228802]\n",
            "Epoch: 0 | Loss: [0.54658216 2.1863286  4.919239  ]\n",
            "\tgrad:  1.0 2.0 [-1.4786239]\n",
            "\tgrad:  2.0 4.0 [-5.7962055]\n",
            "\tgrad:  3.0 6.0 [-11.998146]\n",
            "Epoch: 1 | Loss: [0.29875213 1.1950085  2.688769  ]\n",
            "\tgrad:  1.0 2.0 [-1.0931644]\n",
            "\tgrad:  2.0 4.0 [-4.285205]\n",
            "\tgrad:  3.0 6.0 [-8.870373]\n",
            "Epoch: 2 | Loss: [0.16329262 0.65317047 1.4696338 ]\n",
            "\tgrad:  1.0 2.0 [-0.80818963]\n",
            "\tgrad:  2.0 4.0 [-3.1681032]\n",
            "\tgrad:  3.0 6.0 [-6.557974]\n",
            "Epoch: 3 | Loss: [0.0892528  0.3570112  0.80327564]\n",
            "\tgrad:  1.0 2.0 [-0.59750414]\n",
            "\tgrad:  2.0 4.0 [-2.3422165]\n",
            "\tgrad:  3.0 6.0 [-4.8483896]\n",
            "Epoch: 4 | Loss: [0.04878404 0.19513616 0.43905652]\n",
            "\tgrad:  1.0 2.0 [-0.44174218]\n",
            "\tgrad:  2.0 4.0 [-1.7316294]\n",
            "\tgrad:  3.0 6.0 [-3.5844727]\n",
            "Epoch: 5 | Loss: [0.02666449 0.10665795 0.23998016]\n",
            "\tgrad:  1.0 2.0 [-0.3265853]\n",
            "\tgrad:  2.0 4.0 [-1.2802143]\n",
            "\tgrad:  3.0 6.0 [-2.6500454]\n",
            "Epoch: 6 | Loss: [0.01457433 0.05829733 0.13116899]\n",
            "\tgrad:  1.0 2.0 [-0.2414484]\n",
            "\tgrad:  2.0 4.0 [-0.9464779]\n",
            "\tgrad:  3.0 6.0 [-1.9592113]\n",
            "Epoch: 7 | Loss: [0.00796607 0.03186427 0.07169455]\n",
            "\tgrad:  1.0 2.0 [-0.17850566]\n",
            "\tgrad:  2.0 4.0 [-0.6997423]\n",
            "\tgrad:  3.0 6.0 [-1.4484673]\n",
            "Epoch: 8 | Loss: [0.00435411 0.01741644 0.03918699]\n",
            "\tgrad:  1.0 2.0 [-0.13197136]\n",
            "\tgrad:  2.0 4.0 [-0.5173273]\n",
            "\tgrad:  3.0 6.0 [-1.0708666]\n",
            "Epoch: 9 | Loss: [0.00237987 0.00951947 0.02141885]\n",
            "\tgrad:  1.0 2.0 [-0.0975678]\n",
            "\tgrad:  2.0 4.0 [-0.38246536]\n",
            "\tgrad:  3.0 6.0 [-0.7917023]\n",
            "Epoch: 10 | Loss: [0.00130079 0.00520314 0.01170705]\n",
            "\tgrad:  1.0 2.0 [-0.07213283]\n",
            "\tgrad:  2.0 4.0 [-0.28276062]\n",
            "\tgrad:  3.0 6.0 [-0.5853138]\n",
            "Epoch: 11 | Loss: [0.00071098 0.00284393 0.00639884]\n",
            "\tgrad:  1.0 2.0 [-0.05332851]\n",
            "\tgrad:  2.0 4.0 [-0.20904732]\n",
            "\tgrad:  3.0 6.0 [-0.43272972]\n",
            "Epoch: 12 | Loss: [0.00038861 0.00155444 0.00349745]\n",
            "\tgrad:  1.0 2.0 [-0.03942633]\n",
            "\tgrad:  2.0 4.0 [-0.1545515]\n",
            "\tgrad:  3.0 6.0 [-0.3199196]\n",
            "Epoch: 13 | Loss: [0.00021241 0.00084963 0.00191167]\n",
            "\tgrad:  1.0 2.0 [-0.02914834]\n",
            "\tgrad:  2.0 4.0 [-0.11426163]\n",
            "\tgrad:  3.0 6.0 [-0.23652077]\n",
            "Epoch: 14 | Loss: [0.0001161  0.00046439 0.00104489]\n",
            "\tgrad:  1.0 2.0 [-0.0215497]\n",
            "\tgrad:  2.0 4.0 [-0.08447456]\n",
            "\tgrad:  3.0 6.0 [-0.17486286]\n",
            "Epoch: 15 | Loss: [6.3455918e-05 2.5382367e-04 5.7109760e-04]\n",
            "\tgrad:  1.0 2.0 [-0.01593184]\n",
            "\tgrad:  2.0 4.0 [-0.06245327]\n",
            "\tgrad:  3.0 6.0 [-0.12927818]\n",
            "Epoch: 16 | Loss: [3.4683813e-05 1.3873525e-04 3.1215011e-04]\n",
            "\tgrad:  1.0 2.0 [-0.01177859]\n",
            "\tgrad:  2.0 4.0 [-0.04617214]\n",
            "\tgrad:  3.0 6.0 [-0.09557533]\n",
            "Epoch: 17 | Loss: [1.8958355e-05 7.5833421e-05 1.7062831e-04]\n",
            "\tgrad:  1.0 2.0 [-0.00870824]\n",
            "\tgrad:  2.0 4.0 [-0.03413582]\n",
            "\tgrad:  3.0 6.0 [-0.07066154]\n",
            "Epoch: 18 | Loss: [1.0361248e-05 4.1444993e-05 9.3255832e-05]\n",
            "\tgrad:  1.0 2.0 [-0.00643778]\n",
            "\tgrad:  2.0 4.0 [-0.02523613]\n",
            "\tgrad:  3.0 6.0 [-0.05223942]\n",
            "Epoch: 19 | Loss: [5.6633294e-06 2.2653318e-05 5.0968261e-05]\n",
            "prediction (after training) 4 7.990481 ->제대로 학습한 결과물 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05_linear_regression.py"
      ],
      "metadata": {
        "id": "hyGPMymMsn74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1) # one in and one out , 인풋 1개에 대한 아웃풋 1개\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred\n",
        "\n",
        "model = Model()\n",
        "# 가중치 초기화\n",
        "nn.init.constant_(model.linear.weight, 1.0) # 참고 코드엔 없는 내역\n",
        "nn.init.constant_(model.linear.bias, 0.0) # 참고 코드엔 없는 내역\n",
        "\n",
        "# loss function & optimizer\n",
        "criterion = torch.nn.MSELoss(reduction = 'sum') # data point 별로 발생한 loss를 합칠 때 sum, 평균을 낼때는 mean\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "# training loop\n",
        "hour_var = tensor([[4.0]])\n",
        "print('prediction (before training)', 4,  model(hour_var).data[0][0].item())\n",
        "for epoch in range(500):\n",
        "    # 1) forward pass : compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()}')\n",
        "\n",
        "    optimizer.zero_grad() # w.grad.data.zero_() 와 같은 기능\n",
        "    loss.backward() # perform backward pass\n",
        "    optimizer.step() # update the weights\n",
        "\n",
        "# after training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(f'prediction (after training)', 4, model(hour_var).data[0][0].item())"
      ],
      "metadata": {
        "id": "5w1GKtijsTJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2115aaa2-849f-4cc7-c88a-5ef09f1926f0"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction (before training) 4 4.0\n",
            "Epoch: 0 | Loss: 14.0\n",
            "Epoch: 1 | Loss: 6.26400089263916\n",
            "Epoch: 2 | Loss: 2.8196988105773926\n",
            "Epoch: 3 | Loss: 1.2859458923339844\n",
            "Epoch: 4 | Loss: 0.6027218103408813\n",
            "Epoch: 5 | Loss: 0.2981351912021637\n",
            "Epoch: 6 | Loss: 0.16211287677288055\n",
            "Epoch: 7 | Loss: 0.10113726556301117\n",
            "Epoch: 8 | Loss: 0.07357636839151382\n",
            "Epoch: 9 | Loss: 0.06089683622121811\n",
            "Epoch: 10 | Loss: 0.0548475943505764\n",
            "Epoch: 11 | Loss: 0.0517561249434948\n",
            "Epoch: 12 | Loss: 0.04998693987727165\n",
            "Epoch: 13 | Loss: 0.04881209135055542\n",
            "Epoch: 14 | Loss: 0.04790737107396126\n",
            "Epoch: 15 | Loss: 0.047128405421972275\n",
            "Epoch: 16 | Loss: 0.04641082137823105\n",
            "Epoch: 17 | Loss: 0.045725904405117035\n",
            "Epoch: 18 | Loss: 0.045060813426971436\n",
            "Epoch: 19 | Loss: 0.04440966248512268\n",
            "Epoch: 20 | Loss: 0.04376979172229767\n",
            "Epoch: 21 | Loss: 0.04314004257321358\n",
            "Epoch: 22 | Loss: 0.042519811540842056\n",
            "Epoch: 23 | Loss: 0.041908636689186096\n",
            "Epoch: 24 | Loss: 0.041306257247924805\n",
            "Epoch: 25 | Loss: 0.040712546557188034\n",
            "Epoch: 26 | Loss: 0.04012734815478325\n",
            "Epoch: 27 | Loss: 0.03955063596367836\n",
            "Epoch: 28 | Loss: 0.03898235410451889\n",
            "Epoch: 29 | Loss: 0.03842207416892052\n",
            "Epoch: 30 | Loss: 0.03786992281675339\n",
            "Epoch: 31 | Loss: 0.03732563927769661\n",
            "Epoch: 32 | Loss: 0.0367891788482666\n",
            "Epoch: 33 | Loss: 0.03626055270433426\n",
            "Epoch: 34 | Loss: 0.03573933616280556\n",
            "Epoch: 35 | Loss: 0.03522577881813049\n",
            "Epoch: 36 | Loss: 0.03471948951482773\n",
            "Epoch: 37 | Loss: 0.03422058746218681\n",
            "Epoch: 38 | Loss: 0.03372875601053238\n",
            "Epoch: 39 | Loss: 0.03324401378631592\n",
            "Epoch: 40 | Loss: 0.03276621177792549\n",
            "Epoch: 41 | Loss: 0.03229529410600662\n",
            "Epoch: 42 | Loss: 0.031831126660108566\n",
            "Epoch: 43 | Loss: 0.031373728066682816\n",
            "Epoch: 44 | Loss: 0.03092273883521557\n",
            "Epoch: 45 | Loss: 0.030478373169898987\n",
            "Epoch: 46 | Loss: 0.030040375888347626\n",
            "Epoch: 47 | Loss: 0.029608706012368202\n",
            "Epoch: 48 | Loss: 0.02918318472802639\n",
            "Epoch: 49 | Loss: 0.028763674199581146\n",
            "Epoch: 50 | Loss: 0.02835029363632202\n",
            "Epoch: 51 | Loss: 0.027942977845668793\n",
            "Epoch: 52 | Loss: 0.027541309595108032\n",
            "Epoch: 53 | Loss: 0.027145519852638245\n",
            "Epoch: 54 | Loss: 0.026755332946777344\n",
            "Epoch: 55 | Loss: 0.026370886713266373\n",
            "Epoch: 56 | Loss: 0.025991875678300858\n",
            "Epoch: 57 | Loss: 0.02561831846833229\n",
            "Epoch: 58 | Loss: 0.02525017410516739\n",
            "Epoch: 59 | Loss: 0.024887269362807274\n",
            "Epoch: 60 | Loss: 0.0245295949280262\n",
            "Epoch: 61 | Loss: 0.02417709492146969\n",
            "Epoch: 62 | Loss: 0.02382965385913849\n",
            "Epoch: 63 | Loss: 0.023487165570259094\n",
            "Epoch: 64 | Loss: 0.02314964309334755\n",
            "Epoch: 65 | Loss: 0.022816915065050125\n",
            "Epoch: 66 | Loss: 0.022489037364721298\n",
            "Epoch: 67 | Loss: 0.022165769711136818\n",
            "Epoch: 68 | Loss: 0.021847238764166832\n",
            "Epoch: 69 | Loss: 0.02153323031961918\n",
            "Epoch: 70 | Loss: 0.02122379094362259\n",
            "Epoch: 71 | Loss: 0.02091871201992035\n",
            "Epoch: 72 | Loss: 0.02061808481812477\n",
            "Epoch: 73 | Loss: 0.02032187208533287\n",
            "Epoch: 74 | Loss: 0.0200297050178051\n",
            "Epoch: 75 | Loss: 0.019741859287023544\n",
            "Epoch: 76 | Loss: 0.019458161666989326\n",
            "Epoch: 77 | Loss: 0.019178476184606552\n",
            "Epoch: 78 | Loss: 0.018902942538261414\n",
            "Epoch: 79 | Loss: 0.018631260842084885\n",
            "Epoch: 80 | Loss: 0.01836351305246353\n",
            "Epoch: 81 | Loss: 0.018099619075655937\n",
            "Epoch: 82 | Loss: 0.017839454114437103\n",
            "Epoch: 83 | Loss: 0.017583005130290985\n",
            "Epoch: 84 | Loss: 0.017330352216959\n",
            "Epoch: 85 | Loss: 0.01708122156560421\n",
            "Epoch: 86 | Loss: 0.016835832968354225\n",
            "Epoch: 87 | Loss: 0.016593845561146736\n",
            "Epoch: 88 | Loss: 0.016355380415916443\n",
            "Epoch: 89 | Loss: 0.01612027734518051\n",
            "Epoch: 90 | Loss: 0.015888644382357597\n",
            "Epoch: 91 | Loss: 0.01566026546061039\n",
            "Epoch: 92 | Loss: 0.01543521136045456\n",
            "Epoch: 93 | Loss: 0.015213403850793839\n",
            "Epoch: 94 | Loss: 0.014994727447628975\n",
            "Epoch: 95 | Loss: 0.014779238030314445\n",
            "Epoch: 96 | Loss: 0.014566865749657154\n",
            "Epoch: 97 | Loss: 0.014357535168528557\n",
            "Epoch: 98 | Loss: 0.0141511932015419\n",
            "Epoch: 99 | Loss: 0.0139478063210845\n",
            "Epoch: 100 | Loss: 0.013747386634349823\n",
            "Epoch: 101 | Loss: 0.01354972179979086\n",
            "Epoch: 102 | Loss: 0.013355041854083538\n",
            "Epoch: 103 | Loss: 0.013163124211132526\n",
            "Epoch: 104 | Loss: 0.012973926961421967\n",
            "Epoch: 105 | Loss: 0.012787474319338799\n",
            "Epoch: 106 | Loss: 0.012603694573044777\n",
            "Epoch: 107 | Loss: 0.012422554194927216\n",
            "Epoch: 108 | Loss: 0.01224406436085701\n",
            "Epoch: 109 | Loss: 0.01206810399889946\n",
            "Epoch: 110 | Loss: 0.01189466193318367\n",
            "Epoch: 111 | Loss: 0.011723688803613186\n",
            "Epoch: 112 | Loss: 0.011555216275155544\n",
            "Epoch: 113 | Loss: 0.011389117687940598\n",
            "Epoch: 114 | Loss: 0.011225422844290733\n",
            "Epoch: 115 | Loss: 0.01106411125510931\n",
            "Epoch: 116 | Loss: 0.010905126109719276\n",
            "Epoch: 117 | Loss: 0.010748437605798244\n",
            "Epoch: 118 | Loss: 0.010593889281153679\n",
            "Epoch: 119 | Loss: 0.010441676713526249\n",
            "Epoch: 120 | Loss: 0.010291622020304203\n",
            "Epoch: 121 | Loss: 0.010143741965293884\n",
            "Epoch: 122 | Loss: 0.00999793317168951\n",
            "Epoch: 123 | Loss: 0.009854264557361603\n",
            "Epoch: 124 | Loss: 0.00971265509724617\n",
            "Epoch: 125 | Loss: 0.009573010727763176\n",
            "Epoch: 126 | Loss: 0.009435471147298813\n",
            "Epoch: 127 | Loss: 0.009299837052822113\n",
            "Epoch: 128 | Loss: 0.009166201576590538\n",
            "Epoch: 129 | Loss: 0.009034479968249798\n",
            "Epoch: 130 | Loss: 0.008904600515961647\n",
            "Epoch: 131 | Loss: 0.008776657283306122\n",
            "Epoch: 132 | Loss: 0.008650527335703373\n",
            "Epoch: 133 | Loss: 0.00852622278034687\n",
            "Epoch: 134 | Loss: 0.008403655141592026\n",
            "Epoch: 135 | Loss: 0.008282884955406189\n",
            "Epoch: 136 | Loss: 0.008163855411112309\n",
            "Epoch: 137 | Loss: 0.008046495728194714\n",
            "Epoch: 138 | Loss: 0.007930855266749859\n",
            "Epoch: 139 | Loss: 0.007816880010068417\n",
            "Epoch: 140 | Loss: 0.0077045815996825695\n",
            "Epoch: 141 | Loss: 0.007593810558319092\n",
            "Epoch: 142 | Loss: 0.007484696805477142\n",
            "Epoch: 143 | Loss: 0.0073771364986896515\n",
            "Epoch: 144 | Loss: 0.007271073758602142\n",
            "Epoch: 145 | Loss: 0.007166605442762375\n",
            "Epoch: 146 | Loss: 0.007063611876219511\n",
            "Epoch: 147 | Loss: 0.006962105631828308\n",
            "Epoch: 148 | Loss: 0.0068620480597019196\n",
            "Epoch: 149 | Loss: 0.00676342099905014\n",
            "Epoch: 150 | Loss: 0.006666220724582672\n",
            "Epoch: 151 | Loss: 0.006570430938154459\n",
            "Epoch: 152 | Loss: 0.0064759934321045876\n",
            "Epoch: 153 | Loss: 0.0063829366117715836\n",
            "Epoch: 154 | Loss: 0.006291213445365429\n",
            "Epoch: 155 | Loss: 0.006200757808983326\n",
            "Epoch: 156 | Loss: 0.006111684255301952\n",
            "Epoch: 157 | Loss: 0.006023851223289967\n",
            "Epoch: 158 | Loss: 0.005937259178608656\n",
            "Epoch: 159 | Loss: 0.005851933732628822\n",
            "Epoch: 160 | Loss: 0.005767835304141045\n",
            "Epoch: 161 | Loss: 0.00568491593003273\n",
            "Epoch: 162 | Loss: 0.005603249650448561\n",
            "Epoch: 163 | Loss: 0.005522688385099173\n",
            "Epoch: 164 | Loss: 0.005443349480628967\n",
            "Epoch: 165 | Loss: 0.005365116987377405\n",
            "Epoch: 166 | Loss: 0.005287996027618647\n",
            "Epoch: 167 | Loss: 0.005212004296481609\n",
            "Epoch: 168 | Loss: 0.005137084051966667\n",
            "Epoch: 169 | Loss: 0.005063264165073633\n",
            "Epoch: 170 | Loss: 0.004990500397980213\n",
            "Epoch: 171 | Loss: 0.0049188099801540375\n",
            "Epoch: 172 | Loss: 0.004848099313676357\n",
            "Epoch: 173 | Loss: 0.004778431262820959\n",
            "Epoch: 174 | Loss: 0.004709726199507713\n",
            "Epoch: 175 | Loss: 0.004642065614461899\n",
            "Epoch: 176 | Loss: 0.004575323313474655\n",
            "Epoch: 177 | Loss: 0.004509575664997101\n",
            "Epoch: 178 | Loss: 0.004444772377610207\n",
            "Epoch: 179 | Loss: 0.004380896687507629\n",
            "Epoch: 180 | Loss: 0.004317945800721645\n",
            "Epoch: 181 | Loss: 0.004255901090800762\n",
            "Epoch: 182 | Loss: 0.004194723907858133\n",
            "Epoch: 183 | Loss: 0.0041344347409904\n",
            "Epoch: 184 | Loss: 0.004075000993907452\n",
            "Epoch: 185 | Loss: 0.00401643943041563\n",
            "Epoch: 186 | Loss: 0.003958715591579676\n",
            "Epoch: 187 | Loss: 0.003901834599673748\n",
            "Epoch: 188 | Loss: 0.003845750819891691\n",
            "Epoch: 189 | Loss: 0.003790477756410837\n",
            "Epoch: 190 | Loss: 0.0037360202986747026\n",
            "Epoch: 191 | Loss: 0.003682295558974147\n",
            "Epoch: 192 | Loss: 0.003629387589171529\n",
            "Epoch: 193 | Loss: 0.0035772589035332203\n",
            "Epoch: 194 | Loss: 0.0035258315037935972\n",
            "Epoch: 195 | Loss: 0.003475133329629898\n",
            "Epoch: 196 | Loss: 0.0034252172335982323\n",
            "Epoch: 197 | Loss: 0.003375979606062174\n",
            "Epoch: 198 | Loss: 0.003327480982989073\n",
            "Epoch: 199 | Loss: 0.0032796396408230066\n",
            "Epoch: 200 | Loss: 0.0032325314823538065\n",
            "Epoch: 201 | Loss: 0.0031860624440014362\n",
            "Epoch: 202 | Loss: 0.003140262793749571\n",
            "Epoch: 203 | Loss: 0.0030951406806707382\n",
            "Epoch: 204 | Loss: 0.0030506306793540716\n",
            "Epoch: 205 | Loss: 0.0030068163760006428\n",
            "Epoch: 206 | Loss: 0.0029636190738528967\n",
            "Epoch: 207 | Loss: 0.002921030391007662\n",
            "Epoch: 208 | Loss: 0.0028790272772312164\n",
            "Epoch: 209 | Loss: 0.002837659791111946\n",
            "Epoch: 210 | Loss: 0.0027968839276582003\n",
            "Epoch: 211 | Loss: 0.002756674773991108\n",
            "Epoch: 212 | Loss: 0.002717060036957264\n",
            "Epoch: 213 | Loss: 0.0026780173648148775\n",
            "Epoch: 214 | Loss: 0.0026395295280963182\n",
            "Epoch: 215 | Loss: 0.0026016077026724815\n",
            "Epoch: 216 | Loss: 0.0025641960091888905\n",
            "Epoch: 217 | Loss: 0.0025273473002016544\n",
            "Epoch: 218 | Loss: 0.0024910150095820427\n",
            "Epoch: 219 | Loss: 0.002455236855894327\n",
            "Epoch: 220 | Loss: 0.002419928787276149\n",
            "Epoch: 221 | Loss: 0.002385152503848076\n",
            "Epoch: 222 | Loss: 0.0023508728481829166\n",
            "Epoch: 223 | Loss: 0.002317101461812854\n",
            "Epoch: 224 | Loss: 0.002283802255988121\n",
            "Epoch: 225 | Loss: 0.0022509871050715446\n",
            "Epoch: 226 | Loss: 0.002218617359176278\n",
            "Epoch: 227 | Loss: 0.002186740282922983\n",
            "Epoch: 228 | Loss: 0.0021553048864006996\n",
            "Epoch: 229 | Loss: 0.0021243509836494923\n",
            "Epoch: 230 | Loss: 0.0020937970839440823\n",
            "Epoch: 231 | Loss: 0.0020637046545743942\n",
            "Epoch: 232 | Loss: 0.0020340660121291876\n",
            "Epoch: 233 | Loss: 0.002004803391173482\n",
            "Epoch: 234 | Loss: 0.001976011088117957\n",
            "Epoch: 235 | Loss: 0.0019476271700114012\n",
            "Epoch: 236 | Loss: 0.001919639646075666\n",
            "Epoch: 237 | Loss: 0.0018920244183391333\n",
            "Epoch: 238 | Loss: 0.0018648473778739572\n",
            "Epoch: 239 | Loss: 0.0018380341352894902\n",
            "Epoch: 240 | Loss: 0.0018116229912266135\n",
            "Epoch: 241 | Loss: 0.0017855847254395485\n",
            "Epoch: 242 | Loss: 0.0017599179409444332\n",
            "Epoch: 243 | Loss: 0.001734637888148427\n",
            "Epoch: 244 | Loss: 0.0017097042873501778\n",
            "Epoch: 245 | Loss: 0.0016851229593157768\n",
            "Epoch: 246 | Loss: 0.0016609163722023368\n",
            "Epoch: 247 | Loss: 0.001637061359360814\n",
            "Epoch: 248 | Loss: 0.0016135189216583967\n",
            "Epoch: 249 | Loss: 0.0015903394669294357\n",
            "Epoch: 250 | Loss: 0.001567465951666236\n",
            "Epoch: 251 | Loss: 0.0015449431957677007\n",
            "Epoch: 252 | Loss: 0.001522735459730029\n",
            "Epoch: 253 | Loss: 0.0015008596237748861\n",
            "Epoch: 254 | Loss: 0.001479291939176619\n",
            "Epoch: 255 | Loss: 0.0014580246061086655\n",
            "Epoch: 256 | Loss: 0.0014370634453371167\n",
            "Epoch: 257 | Loss: 0.0014164182357490063\n",
            "Epoch: 258 | Loss: 0.0013960676733404398\n",
            "Epoch: 259 | Loss: 0.0013760090805590153\n",
            "Epoch: 260 | Loss: 0.0013562219683080912\n",
            "Epoch: 261 | Loss: 0.0013367226347327232\n",
            "Epoch: 262 | Loss: 0.0013175250496715307\n",
            "Epoch: 263 | Loss: 0.0012985876528546214\n",
            "Epoch: 264 | Loss: 0.0012799435062333941\n",
            "Epoch: 265 | Loss: 0.0012615167070180178\n",
            "Epoch: 266 | Loss: 0.0012433999218046665\n",
            "Epoch: 267 | Loss: 0.001225538202561438\n",
            "Epoch: 268 | Loss: 0.0012079270090907812\n",
            "Epoch: 269 | Loss: 0.001190565642900765\n",
            "Epoch: 270 | Loss: 0.0011734594590961933\n",
            "Epoch: 271 | Loss: 0.0011565957684069872\n",
            "Epoch: 272 | Loss: 0.0011399737559258938\n",
            "Epoch: 273 | Loss: 0.00112357537727803\n",
            "Epoch: 274 | Loss: 0.0011074390495195985\n",
            "Epoch: 275 | Loss: 0.001091515296138823\n",
            "Epoch: 276 | Loss: 0.0010758424177765846\n",
            "Epoch: 277 | Loss: 0.001060377573594451\n",
            "Epoch: 278 | Loss: 0.0010451392736285925\n",
            "Epoch: 279 | Loss: 0.0010301233269274235\n",
            "Epoch: 280 | Loss: 0.0010153104085475206\n",
            "Epoch: 281 | Loss: 0.0010007115779444575\n",
            "Epoch: 282 | Loss: 0.0009863314917311072\n",
            "Epoch: 283 | Loss: 0.0009721643873490393\n",
            "Epoch: 284 | Loss: 0.0009581951308064163\n",
            "Epoch: 285 | Loss: 0.0009444163297303021\n",
            "Epoch: 286 | Loss: 0.0009308597072958946\n",
            "Epoch: 287 | Loss: 0.000917467987164855\n",
            "Epoch: 288 | Loss: 0.0009042765595950186\n",
            "Epoch: 289 | Loss: 0.0008912933408282697\n",
            "Epoch: 290 | Loss: 0.0008784885285422206\n",
            "Epoch: 291 | Loss: 0.000865844776853919\n",
            "Epoch: 292 | Loss: 0.0008534149383194745\n",
            "Epoch: 293 | Loss: 0.0008411458693444729\n",
            "Epoch: 294 | Loss: 0.0008290526457130909\n",
            "Epoch: 295 | Loss: 0.0008171466761268675\n",
            "Epoch: 296 | Loss: 0.0008054028148762882\n",
            "Epoch: 297 | Loss: 0.00079383235424757\n",
            "Epoch: 298 | Loss: 0.0007824025815352798\n",
            "Epoch: 299 | Loss: 0.0007711622165516019\n",
            "Epoch: 300 | Loss: 0.0007600897806696594\n",
            "Epoch: 301 | Loss: 0.000749170605558902\n",
            "Epoch: 302 | Loss: 0.0007383983465842903\n",
            "Epoch: 303 | Loss: 0.0007277841214090586\n",
            "Epoch: 304 | Loss: 0.0007173214107751846\n",
            "Epoch: 305 | Loss: 0.0007070272695273161\n",
            "Epoch: 306 | Loss: 0.000696861301548779\n",
            "Epoch: 307 | Loss: 0.0006868374766781926\n",
            "Epoch: 308 | Loss: 0.0006769770407117903\n",
            "Epoch: 309 | Loss: 0.0006672449526377022\n",
            "Epoch: 310 | Loss: 0.0006576519808731973\n",
            "Epoch: 311 | Loss: 0.0006481935270130634\n",
            "Epoch: 312 | Loss: 0.0006388770998455584\n",
            "Epoch: 313 | Loss: 0.0006297083455137908\n",
            "Epoch: 314 | Loss: 0.0006206493708305061\n",
            "Epoch: 315 | Loss: 0.0006117264274507761\n",
            "Epoch: 316 | Loss: 0.0006029337528161705\n",
            "Epoch: 317 | Loss: 0.0005942700081504881\n",
            "Epoch: 318 | Loss: 0.0005857449141331017\n",
            "Epoch: 319 | Loss: 0.0005773205193690956\n",
            "Epoch: 320 | Loss: 0.0005690320394933224\n",
            "Epoch: 321 | Loss: 0.0005608531064353883\n",
            "Epoch: 322 | Loss: 0.0005527825560420752\n",
            "Epoch: 323 | Loss: 0.000544843846000731\n",
            "Epoch: 324 | Loss: 0.0005370001890696585\n",
            "Epoch: 325 | Loss: 0.0005292856367304921\n",
            "Epoch: 326 | Loss: 0.0005216854042373598\n",
            "Epoch: 327 | Loss: 0.000514182320330292\n",
            "Epoch: 328 | Loss: 0.0005067989695817232\n",
            "Epoch: 329 | Loss: 0.000499506713822484\n",
            "Epoch: 330 | Loss: 0.0004923305823467672\n",
            "Epoch: 331 | Loss: 0.0004852588172070682\n",
            "Epoch: 332 | Loss: 0.00047829674440436065\n",
            "Epoch: 333 | Loss: 0.00047141523100435734\n",
            "Epoch: 334 | Loss: 0.0004646388115361333\n",
            "Epoch: 335 | Loss: 0.00045795616460964084\n",
            "Epoch: 336 | Loss: 0.00045137631241232157\n",
            "Epoch: 337 | Loss: 0.0004448906402103603\n",
            "Epoch: 338 | Loss: 0.0004384909407235682\n",
            "Epoch: 339 | Loss: 0.00043220038060098886\n",
            "Epoch: 340 | Loss: 0.0004259913112036884\n",
            "Epoch: 341 | Loss: 0.00041986769065260887\n",
            "Epoch: 342 | Loss: 0.00041382620111107826\n",
            "Epoch: 343 | Loss: 0.0004078825586475432\n",
            "Epoch: 344 | Loss: 0.0004020249762106687\n",
            "Epoch: 345 | Loss: 0.00039624457713216543\n",
            "Epoch: 346 | Loss: 0.0003905461635440588\n",
            "Epoch: 347 | Loss: 0.0003849358472507447\n",
            "Epoch: 348 | Loss: 0.0003794035001192242\n",
            "Epoch: 349 | Loss: 0.0003739561652764678\n",
            "Epoch: 350 | Loss: 0.00036857501254417\n",
            "Epoch: 351 | Loss: 0.00036328050191514194\n",
            "Epoch: 352 | Loss: 0.00035805729567073286\n",
            "Epoch: 353 | Loss: 0.00035291342646814883\n",
            "Epoch: 354 | Loss: 0.0003478469152469188\n",
            "Epoch: 355 | Loss: 0.00034284821595065296\n",
            "Epoch: 356 | Loss: 0.00033791668829508126\n",
            "Epoch: 357 | Loss: 0.0003330612671561539\n",
            "Epoch: 358 | Loss: 0.00032826728420332074\n",
            "Epoch: 359 | Loss: 0.0003235509211663157\n",
            "Epoch: 360 | Loss: 0.0003189039998687804\n",
            "Epoch: 361 | Loss: 0.0003143175272271037\n",
            "Epoch: 362 | Loss: 0.0003098051529377699\n",
            "Epoch: 363 | Loss: 0.00030534679535776377\n",
            "Epoch: 364 | Loss: 0.0003009681240655482\n",
            "Epoch: 365 | Loss: 0.0002966421307064593\n",
            "Epoch: 366 | Loss: 0.00029236829141154885\n",
            "Epoch: 367 | Loss: 0.00028817186830565333\n",
            "Epoch: 368 | Loss: 0.00028402736643329263\n",
            "Epoch: 369 | Loss: 0.00027994788251817226\n",
            "Epoch: 370 | Loss: 0.0002759288181550801\n",
            "Epoch: 371 | Loss: 0.00027195896836929023\n",
            "Epoch: 372 | Loss: 0.0002680454635992646\n",
            "Epoch: 373 | Loss: 0.00026419060304760933\n",
            "Epoch: 374 | Loss: 0.0002603954926598817\n",
            "Epoch: 375 | Loss: 0.0002566595794633031\n",
            "Epoch: 376 | Loss: 0.00025297095999121666\n",
            "Epoch: 377 | Loss: 0.00024933292297646403\n",
            "Epoch: 378 | Loss: 0.00024575222050771117\n",
            "Epoch: 379 | Loss: 0.00024222186766564846\n",
            "Epoch: 380 | Loss: 0.00023873860482126474\n",
            "Epoch: 381 | Loss: 0.00023530646285507828\n",
            "Epoch: 382 | Loss: 0.00023192126536741853\n",
            "Epoch: 383 | Loss: 0.00022858523880131543\n",
            "Epoch: 384 | Loss: 0.00022530391288455576\n",
            "Epoch: 385 | Loss: 0.00022206884750630707\n",
            "Epoch: 386 | Loss: 0.00021888039191253483\n",
            "Epoch: 387 | Loss: 0.00021573547564912587\n",
            "Epoch: 388 | Loss: 0.00021262775408104062\n",
            "Epoch: 389 | Loss: 0.0002095728850690648\n",
            "Epoch: 390 | Loss: 0.0002065626613330096\n",
            "Epoch: 391 | Loss: 0.0002035941433859989\n",
            "Epoch: 392 | Loss: 0.00020066769502591342\n",
            "Epoch: 393 | Loss: 0.00019778532441705465\n",
            "Epoch: 394 | Loss: 0.00019494249136187136\n",
            "Epoch: 395 | Loss: 0.00019214279018342495\n",
            "Epoch: 396 | Loss: 0.00018938173889182508\n",
            "Epoch: 397 | Loss: 0.0001866605452960357\n",
            "Epoch: 398 | Loss: 0.00018398111569695175\n",
            "Epoch: 399 | Loss: 0.00018133202684111893\n",
            "Epoch: 400 | Loss: 0.00017872467287816107\n",
            "Epoch: 401 | Loss: 0.00017615704564377666\n",
            "Epoch: 402 | Loss: 0.0001736279227770865\n",
            "Epoch: 403 | Loss: 0.0001711284858174622\n",
            "Epoch: 404 | Loss: 0.00016867060912773013\n",
            "Epoch: 405 | Loss: 0.0001662417344050482\n",
            "Epoch: 406 | Loss: 0.00016385427443310618\n",
            "Epoch: 407 | Loss: 0.00016150331066455692\n",
            "Epoch: 408 | Loss: 0.00015918254212010652\n",
            "Epoch: 409 | Loss: 0.0001568989537190646\n",
            "Epoch: 410 | Loss: 0.00015463694580830634\n",
            "Epoch: 411 | Loss: 0.00015241789515130222\n",
            "Epoch: 412 | Loss: 0.00015022701700218022\n",
            "Epoch: 413 | Loss: 0.00014806754188612103\n",
            "Epoch: 414 | Loss: 0.00014594057574868202\n",
            "Epoch: 415 | Loss: 0.00014384224778041244\n",
            "Epoch: 416 | Loss: 0.00014177573029883206\n",
            "Epoch: 417 | Loss: 0.00013973447494208813\n",
            "Epoch: 418 | Loss: 0.0001377277949359268\n",
            "Epoch: 419 | Loss: 0.0001357525761704892\n",
            "Epoch: 420 | Loss: 0.0001338043948635459\n",
            "Epoch: 421 | Loss: 0.0001318776630796492\n",
            "Epoch: 422 | Loss: 0.0001299834402743727\n",
            "Epoch: 423 | Loss: 0.0001281121512874961\n",
            "Epoch: 424 | Loss: 0.00012627532123588026\n",
            "Epoch: 425 | Loss: 0.0001244608429260552\n",
            "Epoch: 426 | Loss: 0.00012266852718312293\n",
            "Epoch: 427 | Loss: 0.00012090325617464259\n",
            "Epoch: 428 | Loss: 0.00011917172378161922\n",
            "Epoch: 429 | Loss: 0.00011745961819542572\n",
            "Epoch: 430 | Loss: 0.00011576741235330701\n",
            "Epoch: 431 | Loss: 0.0001140974200097844\n",
            "Epoch: 432 | Loss: 0.0001124666741816327\n",
            "Epoch: 433 | Loss: 0.00011084462312282994\n",
            "Epoch: 434 | Loss: 0.00010925441165454686\n",
            "Epoch: 435 | Loss: 0.0001076841217582114\n",
            "Epoch: 436 | Loss: 0.00010613360063871369\n",
            "Epoch: 437 | Loss: 0.00010461394413141534\n",
            "Epoch: 438 | Loss: 0.00010310407378710806\n",
            "Epoch: 439 | Loss: 0.00010163035767618567\n",
            "Epoch: 440 | Loss: 0.00010016960004577413\n",
            "Epoch: 441 | Loss: 9.872634109342471e-05\n",
            "Epoch: 442 | Loss: 9.730616875458509e-05\n",
            "Epoch: 443 | Loss: 9.590995614416897e-05\n",
            "Epoch: 444 | Loss: 9.453231905354187e-05\n",
            "Epoch: 445 | Loss: 9.317136573372409e-05\n",
            "Epoch: 446 | Loss: 9.18353398446925e-05\n",
            "Epoch: 447 | Loss: 9.0512286988087e-05\n",
            "Epoch: 448 | Loss: 8.921419794205576e-05\n",
            "Epoch: 449 | Loss: 8.793039160082117e-05\n",
            "Epoch: 450 | Loss: 8.66672708070837e-05\n",
            "Epoch: 451 | Loss: 8.541919669369236e-05\n",
            "Epoch: 452 | Loss: 8.419403457082808e-05\n",
            "Epoch: 453 | Loss: 8.298196189571172e-05\n",
            "Epoch: 454 | Loss: 8.178708958439529e-05\n",
            "Epoch: 455 | Loss: 8.061654807534069e-05\n",
            "Epoch: 456 | Loss: 7.94580701040104e-05\n",
            "Epoch: 457 | Loss: 7.831672701286152e-05\n",
            "Epoch: 458 | Loss: 7.718568667769432e-05\n",
            "Epoch: 459 | Loss: 7.607959560118616e-05\n",
            "Epoch: 460 | Loss: 7.498854392906651e-05\n",
            "Epoch: 461 | Loss: 7.390487735392526e-05\n",
            "Epoch: 462 | Loss: 7.285042374860495e-05\n",
            "Epoch: 463 | Loss: 7.179765088949353e-05\n",
            "Epoch: 464 | Loss: 7.076670590322465e-05\n",
            "Epoch: 465 | Loss: 6.975099677219987e-05\n",
            "Epoch: 466 | Loss: 6.874600512674078e-05\n",
            "Epoch: 467 | Loss: 6.775883957743645e-05\n",
            "Epoch: 468 | Loss: 6.678926001768559e-05\n",
            "Epoch: 469 | Loss: 6.582619971595705e-05\n",
            "Epoch: 470 | Loss: 6.488230428658426e-05\n",
            "Epoch: 471 | Loss: 6.394430238287896e-05\n",
            "Epoch: 472 | Loss: 6.303297413978726e-05\n",
            "Epoch: 473 | Loss: 6.212544394657016e-05\n",
            "Epoch: 474 | Loss: 6.122903869254515e-05\n",
            "Epoch: 475 | Loss: 6.0351343563525006e-05\n",
            "Epoch: 476 | Loss: 5.9483121731318533e-05\n",
            "Epoch: 477 | Loss: 5.862876059836708e-05\n",
            "Epoch: 478 | Loss: 5.778677586931735e-05\n",
            "Epoch: 479 | Loss: 5.6954384490381926e-05\n",
            "Epoch: 480 | Loss: 5.6137610954465345e-05\n",
            "Epoch: 481 | Loss: 5.533279909286648e-05\n",
            "Epoch: 482 | Loss: 5.45368020539172e-05\n",
            "Epoch: 483 | Loss: 5.375083128456026e-05\n",
            "Epoch: 484 | Loss: 5.2979885367676616e-05\n",
            "Epoch: 485 | Loss: 5.2219536883058026e-05\n",
            "Epoch: 486 | Loss: 5.14655293955002e-05\n",
            "Epoch: 487 | Loss: 5.072942076367326e-05\n",
            "Epoch: 488 | Loss: 4.999944212613627e-05\n",
            "Epoch: 489 | Loss: 4.927964982925914e-05\n",
            "Epoch: 490 | Loss: 4.857196472585201e-05\n",
            "Epoch: 491 | Loss: 4.7872221330180764e-05\n",
            "Epoch: 492 | Loss: 4.718633499578573e-05\n",
            "Epoch: 493 | Loss: 4.6507375373039395e-05\n",
            "Epoch: 494 | Loss: 4.583807094604708e-05\n",
            "Epoch: 495 | Loss: 4.518337664194405e-05\n",
            "Epoch: 496 | Loss: 4.4531851017381996e-05\n",
            "Epoch: 497 | Loss: 4.389121022541076e-05\n",
            "Epoch: 498 | Loss: 4.3259424273855984e-05\n",
            "Epoch: 499 | Loss: 4.264171002432704e-05\n",
            "prediction (after training) 4 7.992494106292725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax\n",
        "- jax 는 pytorch와 다르게 함수형 프로그램을 강조하고 있음 (class 가 필요없음)\n",
        "- 모델과 관련된 파라미터와 계산을 함수로 구현하여 사용하는 것이 일반적임\n",
        "\n",
        "\n",
        "### 변경사항\n",
        "  - loss 를 optax 내장함수로 바꾸어줌\n",
        "  - opimizer 가 2줄짜리로 변경 : opt_state, params 업데이트 필요\n",
        "\n",
        "### optax 를 사용한 최적화 코드\n",
        "\n",
        "[참고자료](https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.sgd)\n",
        "\n",
        "```\n",
        "# optax.sgd 를 활용해 optimizer 를 초기화 할때, params 가 jax 배열이 아니기때문에 오류가 발생\n",
        "optimizer = optax.sgd(learning_rate = 0.01)\n",
        "\n",
        "# 아래와 같이 한번 init 해주어야함\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "\n",
        "# optimizer 를 활용해 업데이트를 하면 opt_state 가 업데이트 되며,\n",
        "# 이를 다시 apply_updates 로 처리해서 실제 업데이트도 진행\n",
        "updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "params = optax.apply_updates(params, updates)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Q. jit 을 gradient 계산하는 부분에 적용하기 위해서는 어떻게 해야 하는가?\n",
        "\n",
        "\n",
        "chat gpt 설명\n",
        "```\n",
        "@jit\n",
        "def value_and_grad(params, x, y):\n",
        "    return jax.value_and_grad(loss)(params, x, y)\n",
        "```\n"
      ],
      "metadata": {
        "id": "5t0YGypVuwGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit\n",
        "import optax\n",
        "\n",
        "x_data = jnp.array([[1.0], [2.0], [3.0]])\n",
        "y_data = jnp.array([[2.0], [4.0], [6.0]])\n",
        "\n",
        "def model(params, x):\n",
        "    return jnp.dot(x, params['weight']) + params['bias']\n",
        "\n",
        "def loss(params, x, y):\n",
        "    y_pred = model(params, x)\n",
        "    return jnp.sum((y_pred - y) ** 2)\n",
        "\n",
        "init_params = {'weight' : jnp.array([[1.0]]), 'bias': jnp.array([0.0])}\n",
        "optimizer = optax.sgd(learning_rate = 0.01)\n",
        "opt_state = optimizer.init(init_params)\n",
        "\n",
        "@jit\n",
        "def update(params, opt_state, x, y):\n",
        "    grads = jax.grad(loss)(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates) # optimizer.step()\n",
        "    return new_params, opt_state\n",
        "\n",
        "num_epochs= 500\n",
        "params = init_params\n",
        "\n",
        "#\n",
        "for epoch in range(num_epochs):\n",
        "    loss_val = loss(params, x_data, y_data)\n",
        "    params, opt_state = update(params, opt_state, x_data, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss_val}')\n",
        "\n",
        "# After training\n",
        "hour_var = jnp.array([[4.0]])\n",
        "y_pred = model(params, hour_var)\n",
        "print(\"Prediction (after training):\", 4, y_pred[0][0])\n"
      ],
      "metadata": {
        "id": "dE2U8mH47OPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06_logistic_regression.py"
      ],
      "metadata": {
        "id": "JpunUVTmIUhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# dataset\n",
        "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = tensor([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    instantiate nn.Linear Module\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = nn.Linear(1, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "\n",
        "# out model\n",
        "model = Model()\n",
        "\n",
        "# 가중치 초기화\n",
        "nn.init.constant_(model.linear.weight, 1.0)\n",
        "nn.init.constant_(model.linear.bias, 0.0)\n",
        "\n",
        "# loss function & optimizer contruction\n",
        "# call to model.parameters()\n",
        "criterion = nn.BCELoss(reduction = 'mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "\n",
        "#Traning loop\n",
        "for epoch in range(1000):\n",
        "  # forward pass : compute predicted y by passing x to the model\n",
        "  y_pred = model(x_data)\n",
        "\n",
        "  # compute and print loss\n",
        "  loss = criterion(y_pred, y_data)\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
        "\n",
        "  #zero gradient, perform a backward pass, update the weights\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "#After training\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(tensor([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(tensor([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
      ],
      "metadata": {
        "id": "kyqRLp6dxuwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f98be4a-2c7b-4a0a-d02d-2da47486eb79"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000 | Loss: 0.8767\n",
            "Epoch 101/1000 | Loss: 0.5963\n",
            "Epoch 201/1000 | Loss: 0.5436\n",
            "Epoch 301/1000 | Loss: 0.5231\n",
            "Epoch 401/1000 | Loss: 0.5053\n",
            "Epoch 501/1000 | Loss: 0.4888\n",
            "Epoch 601/1000 | Loss: 0.4733\n",
            "Epoch 701/1000 | Loss: 0.4588\n",
            "Epoch 801/1000 | Loss: 0.4452\n",
            "Epoch 901/1000 | Loss: 0.4325\n",
            "\n",
            "Let's predict the hours need to score above 50%\n",
            "==================================================\n",
            "Prediction after 1 hour of training: 0.3376 | Above 50%: False\n",
            "Prediction after 7 hours of training: 0.9813 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "vMa71RQ-JLhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit\n",
        "import optax\n",
        "\n",
        "x_data = jnp.array([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = jnp.array([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "# 모델 정의하기\n",
        "def model(params, x):\n",
        "    return jax.nn.sigmoid(jnp.dot(x, params['weight']) + params['bias'])\n",
        "\n",
        "# 가중치 초기화\n",
        "# # 랜덤 초기화\n",
        "# key = jax.random.PRNGKey(0)\n",
        "# init_params = {\n",
        "#     'weight': jax.random.normal(key, (1, 1)),\n",
        "#     'bias': jax.random.normal(key, (1, ))\n",
        "# }\n",
        "\n",
        "# 지정값으로 초기화\n",
        "init_params = {'weight': jnp.array([[1.0]]), 'bias': jnp.array([0.0])}\n",
        "\n",
        "# 손실함수 및 최적화기 정의하기\n",
        "def binary_cross_entropy(y_hat, y):\n",
        "    bce = y * jnp.log(y_hat) + (1 - y) * jnp.log(1 - y_hat)\n",
        "    return jnp.mean(-bce)\n",
        "\n",
        "def loss(params, x, y):\n",
        "    y_pred = model(params, x)\n",
        "    #return optax.sigmoid_binary_cross_entropy(y_pred, y).mean()\n",
        "    return binary_cross_entropy(y_pred, y).mean()\n",
        "\n",
        "optimizer = optax.sgd(learning_rate = 0.01)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# def update(params, opt_state, x, y):\n",
        "#     grads = jax.grad(loss)(params, x, y)\n",
        "#     updates, opt_state = optimizer.update(grad, opt_state)\n",
        "#     new_params =\n",
        "\n",
        "num_epochs = 1000\n",
        "params = init_params\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss_val, grads = jax.value_and_grad(loss)(params, x_data, y_data)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params) # optax.sgd~  의 update 는 x, y 데이터셋을 필요로 하지 않음\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch : {epoch + 1} / {num_epochs}| loss: {loss_val}')\n",
        "\n",
        "\n",
        "#After training\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(params, jnp.array([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(params, jnp.array([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf_zIyPlDkLo",
        "outputId": "c86d4b47-731d-4b23-9bbb-ff86c953e2f0"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1 / 1000| loss: 0.8767316341400146\n",
            "Epoch : 101 / 1000| loss: 0.5962704420089722\n",
            "Epoch : 201 / 1000| loss: 0.5435957908630371\n",
            "Epoch : 301 / 1000| loss: 0.5230756998062134\n",
            "Epoch : 401 / 1000| loss: 0.5053068399429321\n",
            "Epoch : 501 / 1000| loss: 0.4887562096118927\n",
            "Epoch : 601 / 1000| loss: 0.4732797145843506\n",
            "Epoch : 701 / 1000| loss: 0.45879119634628296\n",
            "Epoch : 801 / 1000| loss: 0.4452126622200012\n",
            "Epoch : 901 / 1000| loss: 0.4324727952480316\n",
            "\n",
            "Let's predict the hours need to score above 50%\n",
            "==================================================\n",
            "Prediction after 1 hour of training: 0.3376 | Above 50%: False\n",
            "Prediction after 7 hours of training: 0.9813 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 07_diabets_logistic.py"
      ],
      "metadata": {
        "id": "N_gPZ2TogL4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "xy = np.loadtxt('/content/PyTorchZeroToAll/data/diabetes.csv.gz', delimiter=',', dtype = np.float32)\n",
        "x_data = from_numpy(xy[:, 0: -1])\n",
        "y_data = from_numpy(xy[:, [-1]])\n",
        "\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mATnN4tuc9TC",
        "outputId": "93c85b97-0dcb-4320-9728-ed025c98d201"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1= self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = nn.BCELoss(reduction = 'mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
        "\n",
        "for epoch in range(100):\n",
        "    # forward pass\n",
        "    y_pred = model(x_data)\n",
        "    # compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch + 1} / 100 | Loss : {loss.item():.4f}')\n",
        "\n",
        "    # zero gradients, perform a backward pass, update weights\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5eIknoJK324",
        "outputId": "61dc1e13-4e06-450f-c93b-083a1475aa9e"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 100 | Loss : 0.6928\n",
            "Epoch 11 / 100 | Loss : 0.6589\n",
            "Epoch 21 / 100 | Loss : 0.6494\n",
            "Epoch 31 / 100 | Loss : 0.6465\n",
            "Epoch 41 / 100 | Loss : 0.6457\n",
            "Epoch 51 / 100 | Loss : 0.6454\n",
            "Epoch 61 / 100 | Loss : 0.6453\n",
            "Epoch 71 / 100 | Loss : 0.6453\n",
            "Epoch 81 / 100 | Loss : 0.6453\n",
            "Epoch 91 / 100 | Loss : 0.6453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import jax\n",
        "\n",
        "# define model using flax\n",
        "import flax.linen as nn\n",
        "\n",
        "@nn.compact\n",
        "class Model(nn.Module):\n",
        "    def setup(self):\n",
        "        self.l1 = nn.Dense(6)\n",
        "        self.l2 = nn.Dense(4)\n",
        "        self.l3 = nn.Dense(1)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        out1 = nn.sigmoid(self.l1(x))\n",
        "        out2 = nn.sigmoid(self.l2(out1))\n",
        "        y_pred = nn.sigmoid(self.l3(out2))\n",
        "        return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "# Define loss function\n",
        "def loss(params, model,  x, y):\n",
        "    logits = model.apply({\"params\": params}, x)\n",
        "    return jnp.mean(jax.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "\n",
        "# Initialize model parameters\n",
        "rng = jax.random.PRNGKey(0)\n",
        "params = model.init(rng, jnp.ones((1, 8), dtype=jnp.float32))\n",
        "\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optax.sgd(learning_rate=0.1)\n",
        "optimizer_state = optimizer.init(params)\n",
        "\n",
        "\n",
        "# Define training step\n",
        "@jax.jit\n",
        "def tarin_step(optimizer_state, x, y):\n",
        "    params = optimizer_state\n",
        "    grads = jax.grad(loss_fn)(params, model,  x, y)\n",
        "    updates, new_optimizer_state = optimizer.update(grads, optimizer_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_optimizer_state\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer_state, _ = train_step(optimizer_state, x_data, y_data)\n"
      ],
      "metadata": {
        "id": "hO19ys8T867l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "75ebf177-f5a6-43b4-b7b5-f13b0e1da940"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot interpret 'torch.float32' as a data type",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/api_util.py\u001b[0m in \u001b[0;36mshaped_abstractify\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    583\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_shaped_abstractify_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: <class 'torch.Tensor'>",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-211-76e88f1b8f27>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_issubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_issubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_issubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtendedDType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtendedDType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot interpret 'torch.float32' as a data type"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data"
      ],
      "metadata": {
        "id": "LOV22fExbZCq",
        "outputId": "4223d6a4-ca6f-47e4-ee12-91931299952d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2941,  0.4874,  0.1803,  ...,  0.0015, -0.5312, -0.0333],\n",
              "        [-0.8824, -0.1457,  0.0820,  ..., -0.2072, -0.7669, -0.6667],\n",
              "        [-0.0588,  0.8392,  0.0492,  ..., -0.3055, -0.4927, -0.6333],\n",
              "        ...,\n",
              "        [-0.4118,  0.2161,  0.1803,  ..., -0.2191, -0.8574, -0.7000],\n",
              "        [-0.8824,  0.2663, -0.0164,  ..., -0.1028, -0.7686, -0.1333],\n",
              "        [-0.8824, -0.0653,  0.1475,  ..., -0.0939, -0.7976, -0.9333]])"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4lXvOKRHbY4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i\n",
        "\n",
        "\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optax.sgd(learning_rate=0.1)\n",
        "optimizer_state = optimizer.init(params)\n",
        "\n",
        "# Define training step\n",
        "@jax.jit\n",
        "def train_step(optimizer_state, batch):\n",
        "    params = optimizer_state\n",
        "    grads = jax.grad(loss_fn)(params, model, batch)\n",
        "    updates, new_optimizer_state = optimizer.update(grads, optimizer_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_optimizer_state\n",
        "\n",
        "# Define data loader function\n",
        "def data_loader(x_data, y_data, batch_size=32, shuffle=True):\n",
        "    num_samples = len(x_data)\n",
        "    indices = jax.random.permutation(rng, num_samples)\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        batch_indices = indices[i:i+batch_size]\n",
        "        yield x_data[batch_indices], y_data[batch_indices]\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in data_loader(x_data, y_data, batch_size=batch_size):\n",
        "        optimizer_state, _ = train_step(optimizer_state, batch)\n"
      ],
      "metadata": {
        "id": "wKfT99yfhcjf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "4fb7f370-c9e8-42a8-8846-4432107bb856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ApplyScopeInvalidVariablesTypeError",
          "evalue": "The first argument passed to an apply function should be a dictionary of collections. Each collection should be a dictionary with string keys. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ApplyScopeInvalidVariablesTypeError)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApplyScopeInvalidVariablesTypeError\u001b[0m       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-23806569e47b>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-23806569e47b>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(optimizer_state, batch)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_optimizer_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mnew_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-23806569e47b>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(params, model, batch)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/core/scope.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(variables, rngs, mutable, flags)\u001b[0m\n\u001b[1;32m    871\u001b[0m   \"\"\"\n\u001b[1;32m    872\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyScopeInvalidVariablesTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mrngs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_rngs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     raise errors.InvalidRngError(\n",
            "\u001b[0;31mApplyScopeInvalidVariablesTypeError\u001b[0m: The first argument passed to an apply function should be a dictionary of collections. Each collection should be a dictionary with string keys. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ApplyScopeInvalidVariablesTypeError)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 08_1_dataset_loader.py"
      ],
      "metadata": {
        "id": "gKgs1x7JJ0Nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "b4lJ7zY_J2bM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 08_2_dataset_loade_logistic.py"
      ],
      "metadata": {
        "id": "Ih-LpVfMJ4Bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "G8l60JQ-J5xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 09_01_softmax_loss.py"
      ],
      "metadata": {
        "id": "xLav_RkcJ75s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "TvToaJ-MJ9_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 09_2_softmax_mnist.py"
      ],
      "metadata": {
        "id": "gPbAF030J_SG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "Kc6nCi7tKBHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10_1_cnn_mnist.py"
      ],
      "metadata": {
        "id": "R2LGH_duKB-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "FtPt2hf0KDa7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11_1_toy_inception_mnist.py"
      ],
      "metadata": {
        "id": "O4NXyBocKEcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "_s44p2oKKGhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12_1_rnn_basics.py"
      ],
      "metadata": {
        "id": "wZgP5ZwvKI6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "mhaNAZoSKMQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12_2_hello_rnn.py"
      ],
      "metadata": {
        "id": "gJy4ZIXbKNS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "wH6iOSC6KPOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12_3_hello_rnn_seq.py"
      ],
      "metadata": {
        "id": "hBgui7E4KQLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "xTxWr73YKShL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MWY427gCJ2PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12_4_hello_rnn_emb.py"
      ],
      "metadata": {
        "id": "U-gTUJ9qKUnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "ENmCIGWeKd7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13_1_rnn_classification_basics.py"
      ],
      "metadata": {
        "id": "dzXxTIzNKfBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "3FakOCxPKgl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13_2_rnn_classification.py"
      ],
      "metadata": {
        "id": "K7zWVLCtKi6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "KPoBOIxaKj6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13_3_char_rnn.py"
      ],
      "metadata": {
        "id": "XhWuA19xKlQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "FiJofYEzKnMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13_4_pack_pad.py"
      ],
      "metadata": {
        "id": "L7xY_JrmKojE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "hy0sI3wfKqBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14_1_seq2seq.py"
      ],
      "metadata": {
        "id": "dW5jQhhZKsST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "ubtPPVAQKtHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14_2_seq2seq_att.py"
      ],
      "metadata": {
        "id": "hMGApMnOKuBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with jax"
      ],
      "metadata": {
        "id": "koqLEVDlKvUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bBFN1MP_Kk_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZfuoFA4QJ0KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CSdL6wYdJ0II"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uToGsyJZJ0Fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aLaVPY9tJz-a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7AdREOKhiKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "pytorch_to_jax",
      "provenance": [],
      "collapsed_sections": [
        "gKgs1x7JJ0Nj",
        "Ih-LpVfMJ4Bs",
        "xLav_RkcJ75s",
        "gPbAF030J_SG",
        "R2LGH_duKB-a",
        "O4NXyBocKEcf",
        "wZgP5ZwvKI6B",
        "gJy4ZIXbKNS1",
        "hBgui7E4KQLN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}